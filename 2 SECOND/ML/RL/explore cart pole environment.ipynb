{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Cart Pole Environment\n",
    "The goal of the Cart Pole problem is to balance the pole placed upright on cart where a cart moves along a frictionless track. \n",
    "\n",
    "Detail documentation on Cart Pole environment [https://gymnasium.farama.org/environments/classic_control/cart_pole/] "
   ],
   "id": "d997048a9a8e45a6"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-09-19T16:37:13.175423Z",
     "start_time": "2024-09-19T16:37:12.872900Z"
    }
   },
   "source": [
    "# Import gymnasium library\n",
    "import gymnasium as gym\n",
    "\n",
    "SEED = 42"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T16:37:13.183225Z",
     "start_time": "2024-09-19T16:37:13.176857Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize the Frozen Lake Environment\n",
    "env = gym.make('CartPole-v1')"
   ],
   "id": "ee429cb1ed642afb",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T16:37:13.187454Z",
     "start_time": "2024-09-19T16:37:13.184203Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Reset an environment to its initial internal state\n",
    "obs, info = env.reset(seed=SEED)\n",
    "\n",
    "# Print the initial position of agent in the environment\n",
    "print(\"The initial observation is: {}\".format(obs))\n",
    "print(\"The information is : {}\".format(info))"
   ],
   "id": "a145a45f18af7794",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The initial observation is: [ 0.0273956  -0.00611216  0.03585979  0.0197368 ]\n",
      "The information is : {}\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T16:37:13.190714Z",
     "start_time": "2024-09-19T16:37:13.188522Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Print the Observation space (or state space) and action space \n",
    "print(\"The observation space: {}\".format(env.observation_space))"
   ],
   "id": "9ab36939efbb3045",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The observation space: Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32)\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The observation space consists of minimum and maximum value for \n",
    "1. Cart position ( -4.8 to 4.8 ), \n",
    "2. Cart Velocity ( -Inf to Inf ), \n",
    "3. Pole Angle ( ~-0.418 rad to 0.418 rad ), and, \n",
    "4. Pole Angular Velocity ( -Inf to Inf )\n",
    "\n",
    "Box implies that our state space contains continuous values and not discrete values. We can obtain the maximum and minimum values as below:"
   ],
   "id": "b5a75cdf16ecc610"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T16:37:13.195056Z",
     "start_time": "2024-09-19T16:37:13.192656Z"
    }
   },
   "cell_type": "code",
   "source": "print(env.observation_space.high, env.observation_space.low)",
   "id": "48b2b0d5abcab449",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38] [-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38]\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "And the action space in Cart Pole is discrete and contains two discrete values as:\n",
    "1. 0: Push cart to the left\n",
    "2. 1: Push cart to the right"
   ],
   "id": "9bebd97896f8fd05"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T16:37:13.198319Z",
     "start_time": "2024-09-19T16:37:13.196205Z"
    }
   },
   "cell_type": "code",
   "source": "print(\"The action space: {}\".format(env.action_space))\n",
   "id": "670cbe92fbea95db",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The action space: Discrete(2)\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Cart Pole Balancing with Random Policy in multiple episode",
   "id": "80d14cbc97182b65"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T16:37:13.401711Z",
     "start_time": "2024-09-19T16:37:13.199314Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# define action map\n",
    "action_map = {\n",
    "    0: 'left',\n",
    "    1: 'right',\n",
    "}\n",
    "\n",
    "# Number of times the agent moves\n",
    "num_timestep = 50\n",
    "\n",
    "# Number of episodes\n",
    "num_episodes = 100\n",
    "\n",
    "for e in range(num_episodes):\n",
    "    print(\"-----------------------------------\")\n",
    "    print(\"Episode {}/{}\".format(e, num_episodes))\n",
    "    print(\"-----------------------------------\")\n",
    "    \n",
    "    # Initialize the return\n",
    "    RETURN = 0\n",
    "    \n",
    "    # Initialize the state by resetting the environment\n",
    "    state = env.reset(seed=SEED)\n",
    "    \n",
    "    # We take a random step for each episode\n",
    "    for t in range(num_timestep):\n",
    "        print(\"timestep: {} of episode: {}\".format(t+1, e))\n",
    "        print(\"-----------------------------------------------------\")\n",
    "        \n",
    "        # select random action\n",
    "        random_action = env.action_space.sample()\n",
    "        \n",
    "        # Take the action and get the new observation space\n",
    "        next_state, reward, done, info, transition_prob = env.step(random_action)\n",
    "        \n",
    "        print(\"Action: {}\".format(action_map[random_action]))\n",
    "        print(\"Next State: {}\".format(next_state))\n",
    "        print(\"Reward: {}\".format(reward))\n",
    "        print(\"\")\n",
    "        \n",
    "        RETURN = RETURN + reward\n",
    "        \n",
    "        # if the agent moves to hole state, then terminate\n",
    "        if done: \n",
    "            break\n",
    "    \n",
    "    if e % 10 == 0:\n",
    "        print(\"****************************************************\")\n",
    "        print(\"Episode: {}, Return: {}\".format(e, RETURN))\n",
    "        print(\"****************************************************\")\n",
    "    \n",
    "    env.close()\n",
    "    "
   ],
   "id": "7684e0833d100dfe",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "Episode 0/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 0\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02727336 -0.20172954  0.03625453  0.32351476]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 0\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02323877 -0.00714208  0.04272482  0.04248186]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 0\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02309593  0.187342    0.04357446 -0.23642075]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 0\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02684277  0.3818152   0.03884605 -0.51504683]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 0\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03447907  0.57636917  0.02854511 -0.79523975]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 0\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.04600646  0.38086733  0.01264031 -0.49371535]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 0\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.0536238   0.1855694   0.00276601 -0.1970757 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 0\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.05733519  0.38065168 -0.00117551 -0.4888848 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 0\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.06494822  0.5757902  -0.0109532  -0.78193796]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 0\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.07646403  0.3808205  -0.02659196 -0.49272114]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 0\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.08408044  0.18608351 -0.03644639 -0.20853604]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 0\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.08780211  0.38170713 -0.0406171  -0.5124895 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 0\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.09543625  0.18718012 -0.05086689 -0.23287776]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 14 of episode: 0\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.09917986  0.3829906  -0.05552445 -0.54116195]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 15 of episode: 0\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.10683966  0.18869124 -0.06634769 -0.26647788]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 16 of episode: 0\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.11061349 -0.00542414 -0.07167725  0.00456262]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 17 of episode: 0\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.11050501 -0.19944884 -0.071586    0.2737973 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 18 of episode: 0\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.10651603 -0.39348024 -0.06611005  0.5430707 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 19 of episode: 0\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.09864642 -0.19749454 -0.05524864  0.2303121 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 20 of episode: 0\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.09469654 -0.39178526 -0.05064239  0.50506866]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 21 of episode: 0\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.08686083 -0.58615834 -0.04054102  0.78137195]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 22 of episode: 0\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.07513767 -0.78070027 -0.02491358  1.0610293 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 23 of episode: 0\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.05952366 -0.5852574  -0.003693    0.7606321 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 24 of episode: 0\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.04781851 -0.7803283   0.01151965  1.0521507 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 25 of episode: 0\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03221195 -0.9756011   0.03256266  1.3484272 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 26 of episode: 0\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.01269993 -0.7809031   0.0595312   1.0661068 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 27 of episode: 0\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.00291814 -0.97676015  0.08085334  1.3768637 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 28 of episode: 0\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.02245334 -1.1727936   0.10839061  1.6936984 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 29 of episode: 0\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.04590921 -1.3689872   0.14226459  2.0180662 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 30 of episode: 0\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.07328895 -1.56527     0.1826259   2.3512006 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 31 of episode: 0\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.10459436 -1.3721957   0.22964992  2.1197996 ]\n",
      "Reward: 1.0\n",
      "\n",
      "****************************************************\n",
      "Episode: 0, Return: 31.0\n",
      "****************************************************\n",
      "-----------------------------------\n",
      "Episode 1/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 1\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02727336  0.18847767  0.03625453 -0.26141977]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 1\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03104291 -0.00714255  0.03102613  0.04247424]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 1\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03090006 -0.20269535  0.03187562  0.3447825 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 1\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02684616 -0.3982559   0.03877127  0.6473442 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 1\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.01888104 -0.593896    0.05171815  0.95197964]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 1\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.00700312 -0.78967434  0.07075775  1.2604529 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 1\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.00879037 -0.5955252   0.0959668   0.9907433 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 1\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.02070087 -0.7917914   0.11578166  1.3119589 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 1\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.0365367  -0.98817325  0.14202085  1.6385233 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 1\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.05630016 -0.7949733   0.1747913   1.393255  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 1\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.07219963 -0.9917864   0.20265642  1.7351037 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 1\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.09203536 -1.1885614   0.23735848  2.0833998 ]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 2/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 2\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02727336 -0.20172954  0.03625453  0.32351476]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 2\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02323877 -0.00714208  0.04272482  0.04248186]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 2\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02309593 -0.20284982  0.04357446  0.34833285]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 2\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.01903893 -0.00837385  0.05054112  0.06970263]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 2\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.01887146  0.18598844  0.05193517 -0.20661576]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 2\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02259123 -0.00983622  0.04780285  0.10198726]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 2\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.0223945   0.18456922  0.0498426  -0.1752391 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 2\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02608589 -0.01122931  0.04633782  0.13274162]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 2\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.0258613  -0.20698333  0.04899265  0.4396759 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 2\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02172163 -0.01258776  0.05778617  0.16283052]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 2\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02146988 -0.2084873   0.06104278  0.47316903]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 2\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.01730013 -0.40441585  0.07050616  0.7844499 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 2\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.00921181 -0.21032989  0.08619516  0.5147559 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 14 of episode: 2\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.00500522 -0.40655324  0.09649028  0.8333082 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 15 of episode: 2\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.00312585 -0.21287279  0.11315644  0.5724631 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 16 of episode: 2\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.0073833  -0.01950414  0.1246057   0.31746215]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 17 of episode: 2\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.00777339 -0.2161602   0.13095494  0.6466985 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 18 of episode: 2\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.01209659 -0.41284016  0.14388892  0.97758317]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 19 of episode: 2\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.02035339 -0.60956764  0.16344057  1.3117824 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 20 of episode: 2\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.03254475 -0.8063377   0.18967623  1.650842  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 21 of episode: 2\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.0486715  -1.0031021   0.22269306  1.996126  ]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 3/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 3\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02727336  0.18847767  0.03625453 -0.26141977]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 3\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03104291 -0.00714255  0.03102613  0.04247424]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 3\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03090006  0.18752107  0.03187562 -0.24026045]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 3\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03465049 -0.00804138  0.02707041  0.06230407]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 3\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03448966  0.1866822   0.02831649 -0.22171655]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 3\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.0382233  -0.00883281  0.02388216  0.07976232]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 3\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03804665  0.18593879  0.02547741 -0.20529106]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 3\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.04176542  0.3806873   0.02137158 -0.4898295 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 3\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.04937917  0.5755013   0.01157499 -0.77570105]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 3\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.0608892   0.7704622  -0.00393903 -1.0647198 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 3\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.07629844  0.9656361  -0.02523342 -1.3586364 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 3\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.09561116  1.1610652  -0.05240615 -1.6591046 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 3\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.11883246  1.3567574  -0.08558824 -1.9676403 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 14 of episode: 3\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.14596762  1.1626378  -0.12494104 -1.702659  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 15 of episode: 3\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.16922037  0.9691565  -0.15899423 -1.4513365 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 16 of episode: 3\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.18860349  1.1658343  -0.18802096 -1.7891798 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 17 of episode: 3\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.21192019  0.9732543  -0.22380455 -1.5603539 ]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 4/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 4\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02727336  0.18847767  0.03625453 -0.26141977]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 4\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03104291 -0.00714255  0.03102613  0.04247424]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 4\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03090006  0.18752107  0.03187562 -0.24026045]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 4\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03465049 -0.00804138  0.02707041  0.06230407]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 4\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03448966 -0.20354079  0.02831649  0.36340347]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 4\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03041884 -0.00883249  0.03558456  0.07978204]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 4\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03024219 -0.20444602  0.0371802   0.38347623]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 4\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02615327 -0.00987113  0.04484972  0.10274406]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 4\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02595585 -0.20560618  0.0469046   0.4092328 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 4\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02184373 -0.40136066  0.05508926  0.7163265 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 4\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.01381651 -0.20704272  0.06941579  0.4414798 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 4\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.00967566 -0.4030748   0.07824539  0.75521165]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 4\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.00161416 -0.20911366  0.09334962  0.48814073]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 14 of episode: 4\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.00256811 -0.40542015  0.10311244  0.8087234 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 15 of episode: 4\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.01067651 -0.2118509   0.1192869   0.55017304]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 16 of episode: 4\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.01491353 -0.01858863  0.13029036  0.29732692]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 17 of episode: 4\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.0152853   0.17445861  0.1362369   0.04840749]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 18 of episode: 4\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.01179613 -0.02232718  0.13720505  0.38078094]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 19 of episode: 4\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.01224268  0.17060691  0.14482068  0.13431141]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 20 of episode: 4\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.00883054 -0.02626051  0.1475069   0.46894988]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 21 of episode: 4\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.00935575  0.16650338  0.15688589  0.22615393]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 22 of episode: 4\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.00602568 -0.03047216  0.16140898  0.563924  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 23 of episode: 4\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.00663512  0.16206096  0.17268746  0.32612666]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 24 of episode: 4\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.0033939   0.35435766  0.17920999  0.09248995]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 25 of episode: 4\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.00369325  0.54651886  0.1810598  -0.13872947]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 26 of episode: 4\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [0.01462363 0.3493279  0.1782852  0.20516312]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 27 of episode: 4\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02161018  0.54151183  0.18238845 -0.02640478]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 28 of episode: 4\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [0.03244042 0.34430662 0.18186036 0.31782195]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 29 of episode: 4\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [0.03932655 0.14712341 0.1882168  0.6618918 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 30 of episode: 4\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [0.04226902 0.3391973  0.20145464 0.43387878]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 31 of episode: 4\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [0.04905296 0.14187855 0.21013221 0.7826991 ]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 5/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 5\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02727336  0.18847767  0.03625453 -0.26141977]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 5\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03104291  0.38306385  0.03102613 -0.5424507 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 5\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03870419  0.5777363   0.02017712 -0.8251987 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 5\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.05025892  0.3823443   0.00367314 -0.5262386 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 5\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.0579058   0.5774144  -0.00685163 -0.8177619 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 5\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.06945409  0.38238686 -0.02320687 -0.5272419 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 5\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.07710183  0.187599   -0.0337517  -0.24196094]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 5\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.08085381  0.38318643 -0.03859092 -0.5450961 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 5\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.08851754  0.18862738 -0.04949284 -0.2648177 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 5\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.09229009 -0.00575451 -0.0547892   0.01185312]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 5\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.09217499  0.19010864 -0.05455213 -0.29760072]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 5\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.09597716 -0.00419495 -0.06050415 -0.02260928]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 5\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.09589327  0.19174016 -0.06095634 -0.33375153]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 14 of episode: 5\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.09972807 -0.00246364 -0.06763136 -0.06089692]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 15 of episode: 5\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.0996788  -0.196554   -0.0688493   0.20970458]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 16 of episode: 5\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.09574772 -0.0005186  -0.06465521 -0.10387756]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 17 of episode: 5\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.09573735 -0.19465727 -0.06673276  0.16772659]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 18 of episode: 5\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.0918442   0.00135329 -0.06337823 -0.1452397 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 19 of episode: 5\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.09187127 -0.19280648 -0.06628302  0.12679449]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 20 of episode: 5\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.08801514 -0.38691935 -0.06374714  0.39785147]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 21 of episode: 5\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.08027675 -0.19095367 -0.05579011  0.08577103]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 22 of episode: 5\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.07645768  0.00492177 -0.05407469 -0.22397877]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 23 of episode: 5\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.07655612  0.20077322 -0.05855426 -0.5332167 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 24 of episode: 5\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.08057158  0.3966677  -0.0692186  -0.84375995]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 25 of episode: 5\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.08850493  0.5926625  -0.0860938  -1.1573822 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 26 of episode: 5\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.10035818  0.78879464 -0.10924144 -1.4757711 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 27 of episode: 5\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.11613408  0.59516364 -0.13875686 -1.219109  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 28 of episode: 5\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.12803735  0.7917744  -0.16313905 -1.5518544 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 29 of episode: 5\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.14387283  0.9884335  -0.19417614 -1.8906767 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 30 of episode: 5\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.1636415   1.1850622  -0.23198967 -2.2368064 ]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 6/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 6\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02727336 -0.20172954  0.03625453  0.32351476]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 6\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02323877 -0.00714208  0.04272482  0.04248186]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 6\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02309593 -0.20284982  0.04357446  0.34833285]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 6\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.01903893 -0.00837385  0.05054112  0.06970263]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 6\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.01887146 -0.2041826   0.05193517  0.37789345]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 6\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.0147878  -0.00983522  0.05949304  0.10202754]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 6\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.0145911   0.1843859   0.06153359 -0.17130806]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 6\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.01827882  0.37857562  0.05810743 -0.4439619 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 6\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02585033  0.5728293   0.04922819 -0.7177767 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 6\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03730692  0.37706193  0.03487266 -0.41001412]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 6\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.04484816  0.57167256  0.02667237 -0.6915021 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 6\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.05628161  0.76641446  0.01284233 -0.97567034]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 6\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.0716099   0.57112265 -0.00667107 -0.67898124]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 14 of episode: 6\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.08303235  0.376094   -0.0202507  -0.38840604]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 15 of episode: 6\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.09055423  0.18126528 -0.02801882 -0.10217615]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 16 of episode: 6\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.09417953  0.37677732 -0.03006234 -0.4035656 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 17 of episode: 6\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.10171508  0.18209435 -0.03813365 -0.12051006]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 18 of episode: 6\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.10535697  0.3777413  -0.04054386 -0.42497575]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 19 of episode: 6\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.1129118   0.57341343 -0.04904337 -0.73015934]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 20 of episode: 6\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.12438007  0.7691777  -0.06364655 -1.0378656 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 21 of episode: 6\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.13976362  0.96508515 -0.08440387 -1.3498312 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 22 of episode: 6\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.15906532  0.77111894 -0.11140049 -1.084703  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 23 of episode: 6\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.1744877   0.5776288  -0.13309455 -0.8289509 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 24 of episode: 6\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.18604027  0.7742947  -0.14967357 -1.1603551 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 25 of episode: 6\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.20152617  0.97101533 -0.17288068 -1.4959769 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 26 of episode: 6\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.22094648  0.7783652  -0.20280021 -1.2618835 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 27 of episode: 6\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.23651378  0.9754187  -0.22803788 -1.6106216 ]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 7/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 7\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02727336  0.18847767  0.03625453 -0.26141977]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 7\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03104291  0.38306385  0.03102613 -0.5424507 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 7\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03870419  0.1875199   0.02017712 -0.24015574]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 7\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.04245459 -0.0078844   0.015374    0.0588227 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 7\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.0422969   0.18701379  0.01655046 -0.22897024]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 7\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.04603718 -0.00834072  0.01197105  0.06888699]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 7\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.04587036 -0.20363224  0.01334879  0.36532268]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 7\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.04179772 -0.3989413   0.02065525  0.66218466]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 7\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03381889 -0.5943445   0.03389894  0.96129906]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 7\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.021932   -0.39969411  0.05312492  0.67945564]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 7\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.01393812 -0.5955122   0.06671403  0.9883798 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 7\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.00202788 -0.7914609   0.08648163  1.3012483 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 7\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.01380134 -0.59753615  0.11250659  1.0368422 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 14 of episode: 7\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.02575206 -0.40407482  0.13324344  0.7814921 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 15 of episode: 7\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.03383356 -0.600752    0.14887328  1.1129507 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 16 of episode: 7\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.0458486  -0.40786496  0.1711323   0.87042767]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 17 of episode: 7\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.0540059  -0.21543214  0.18854085  0.63606256]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 18 of episode: 7\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.05831454 -0.4126136   0.2012621   0.9816965 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 19 of episode: 7\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.06656682 -0.22067402  0.22089602  0.75837094]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 8/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 8\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02727336 -0.20172954  0.03625453  0.32351476]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 8\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02323877 -0.00714208  0.04272482  0.04248186]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 8\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02309593 -0.20284982  0.04357446  0.34833285]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 8\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.01903893 -0.39856356  0.05054112  0.65443164]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 8\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.01106766 -0.20418042  0.06362975  0.378082  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 8\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.00698405 -0.4001456   0.07119139  0.69012946]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 8\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.00101886 -0.20607999  0.08499398  0.420681  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 8\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.00514046 -0.4022969   0.0934076   0.73890203]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 8\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.0131864  -0.20858039  0.10818564  0.4770159 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 8\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.01735801 -0.01513895  0.11772596  0.22029473]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 8\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.01766078 -0.2117297   0.12213185  0.54767126]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 8\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.02189538 -0.01851612  0.13308528  0.2958273 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 8\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.0222657  -0.21525945  0.13900183  0.6273443 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 14 of episode: 8\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.02657089 -0.02232322  0.15154871  0.38146642]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 15 of episode: 8\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.02701735  0.17035852  0.15917803  0.1401414 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 16 of episode: 8\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.02361018  0.3628851   0.16198087 -0.09839658]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 17 of episode: 8\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.01635248  0.55535966  0.16001293 -0.3359147 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 18 of episode: 8\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.00524529  0.35836533  0.15329464  0.00264526]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 19 of episode: 8\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.00192202  0.55099446  0.15334755 -0.23801778]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 20 of episode: 8\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [0.01294191 0.35405225 0.1485872  0.09883662]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 21 of episode: 8\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02002295  0.5467668   0.15056393 -0.14352597]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 22 of episode: 8\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [0.03095829 0.34984526 0.14769341 0.19261336]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 23 of episode: 8\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [0.03795519 0.15295315 0.15154567 0.5280004 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 24 of episode: 8\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [0.04101426 0.34565455 0.16210568 0.28664523]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 25 of episode: 8\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [0.04792735 0.5381382  0.16783859 0.04915561]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 26 of episode: 8\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.05869011  0.7305057   0.16882169 -0.18622768]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 27 of episode: 8\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [0.07330023 0.5334211  0.16509715 0.15459451]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 28 of episode: 8\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [0.08396865 0.33636752 0.16818903 0.4944741 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 29 of episode: 8\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [0.090696   0.52876806 0.17807852 0.25916037]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 30 of episode: 8\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [0.10127136 0.3316101  0.18326172 0.6022992 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 31 of episode: 8\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [0.10790356 0.13446195 0.1953077  0.94664735]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 32 of episode: 8\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.1105928  -0.06267736  0.21424066  1.2937905 ]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 9/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 9\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02727336 -0.20172954  0.03625453  0.32351476]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 9\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02323877 -0.00714208  0.04272482  0.04248186]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 9\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02309593  0.187342    0.04357446 -0.23642075]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 9\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02684277 -0.00837454  0.03884605  0.06968222]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 9\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02667528  0.18616958  0.04023969 -0.21049595]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 9\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03039867 -0.00950393  0.03602977  0.09460401]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 9\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03020859 -0.20512328  0.03792185  0.39843303]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 9\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02610613 -0.4007621   0.04589051  0.7028267 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 9\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.01809088 -0.2063052   0.05994704  0.42493546]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 9\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.01396478 -0.4022228   0.06844576  0.7358979 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 9\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.00592032 -0.5982201   0.08316371  1.0493127 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 9\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.00604408 -0.40429404  0.10414997  0.78385097]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 9\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.01412996 -0.2107457   0.11982699  0.5256664 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 14 of episode: 9\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.01834487 -0.01749569  0.13034031  0.27301484]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 15 of episode: 9\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.01869479 -0.2142132   0.13580061  0.6037999 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 16 of episode: 9\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.02297905 -0.41094688  0.1478766   0.9359862 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 17 of episode: 9\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.03119799 -0.21809532  0.16659634  0.6931835 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 18 of episode: 9\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.0355599  -0.02562816  0.18046     0.4572329 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 19 of episode: 9\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.03607246 -0.22278103  0.18960465  0.80092764]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 20 of episode: 9\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.04052808 -0.03069489  0.20562321  0.5733724 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 21 of episode: 9\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.04114198  0.16104117  0.21709067  0.35185516]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 10/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 10\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02727336  0.18847767  0.03625453 -0.26141977]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 10\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03104291 -0.00714255  0.03102613  0.04247424]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 10\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03090006  0.18752107  0.03187562 -0.24026045]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 10\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03465049 -0.00804138  0.02707041  0.06230407]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 10\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03448966 -0.20354079  0.02831649  0.36340347]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 10\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03041884 -0.3990535   0.03558456  0.6648789 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 10\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02243777 -0.20444414  0.04888214  0.38360932]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 10\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.01834889 -0.4002248   0.05655432  0.69129515]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 10\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.01034439 -0.20593126  0.07038023  0.4169389 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 10\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.00622577 -0.40197638  0.078719    0.7309533 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 10\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.00181376 -0.59809285  0.09333807  1.0473366 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 10\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.01377562 -0.4043252   0.1142848   0.7853517 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 10\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.02186212 -0.6008163   0.12999183  1.1116924 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 14 of episode: 10\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.03387845 -0.40761897  0.15222569  0.86245143]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 15 of episode: 10\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.04203083 -0.6044493   0.1694747   1.1988643 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 16 of episode: 10\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.05411981 -0.4118759   0.193452    0.9637301 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 17 of episode: 10\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.06235733 -0.21980487  0.2127266   0.73751855]\n",
      "Reward: 1.0\n",
      "\n",
      "****************************************************\n",
      "Episode: 10, Return: 17.0\n",
      "****************************************************\n",
      "-----------------------------------\n",
      "Episode 11/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 11\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02727336 -0.20172954  0.03625453  0.32351476]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 11\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02323877 -0.00714208  0.04272482  0.04248186]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 11\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02309593 -0.20284982  0.04357446  0.34833285]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 11\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.01903893 -0.00837385  0.05054112  0.06970263]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 11\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.01887146 -0.2041826   0.05193517  0.37789345]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 11\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.0147878  -0.40000218  0.05949304  0.68648887]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 11\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.00678776 -0.20575435  0.07322282  0.41311327]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 11\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.00267267 -0.4018337   0.08148509  0.7279525 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 11\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.005364   -0.20792712  0.09604413  0.4619878 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 11\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.00952254 -0.40426594  0.10528389  0.7833323 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 11\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.01760786 -0.6006651   0.12095053  1.1071961 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 11\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.02962116 -0.79715115  0.14309445  1.4352448 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 11\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.04556419 -0.6040542   0.17179935  1.1904861 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 14 of episode: 11\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.05764527 -0.80093443  0.19560908  1.5317199 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 15 of episode: 11\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.07366396 -0.60863346  0.22624347  1.3059124 ]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 12/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 12\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02727336  0.18847767  0.03625453 -0.26141977]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 12\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03104291 -0.00714255  0.03102613  0.04247424]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 12\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03090006  0.18752107  0.03187562 -0.24026045]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 12\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03465049  0.3821735   0.02707041 -0.52272093]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 12\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.04229395  0.18668123  0.01661599 -0.22163221]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 12\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.04602758 -0.00867424  0.01218335  0.0762454 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 12\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.0458541   0.18627095  0.01370825 -0.21256886]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 12\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.04957952 -0.00904428  0.00945688  0.08440655]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 12\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.04939863 -0.20430051  0.01114501  0.38005808]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 12\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.04531262 -0.39957893  0.01874617  0.67623407]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 12\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03732104 -0.5949563   0.03227085  0.9747597 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 12\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02542192 -0.79049593  0.05176605  1.2774024 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 12\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.009612   -0.59607077  0.07731409  1.0013677 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 14 of episode: 12\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.00230942 -0.40206233  0.09734145  0.73393214]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 15 of episode: 12\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.01035067 -0.20841035  0.11202009  0.47340247]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 16 of episode: 12\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.01451887 -0.4049213   0.12148814  0.79918647]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 17 of episode: 12\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.0226173  -0.60148174  0.13747187  1.1274837 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 18 of episode: 12\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.03464693 -0.40840194  0.16002154  0.8808859 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 19 of episode: 12\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.04281497 -0.21577318  0.17763926  0.6424802 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 20 of episode: 12\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.04713044 -0.412868    0.19048886  0.9854218 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 21 of episode: 12\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.05538779 -0.60995984  0.2101973   1.3313776 ]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 13/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 13\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02727336  0.18847767  0.03625453 -0.26141977]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 13\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03104291 -0.00714255  0.03102613  0.04247424]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 13\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03090006 -0.20269535  0.03187562  0.3447825 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 13\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02684616 -0.3982559   0.03877127  0.6473442 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 13\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.01888104 -0.20369498  0.05171815  0.36711797]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 13\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.01480714 -0.00934462  0.05906051  0.09118061]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 13\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.01462025  0.18488325  0.06088412 -0.18229952]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 13\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.01831791 -0.01105466  0.05723813  0.12895164]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 13\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.01809682 -0.20694786  0.05981717  0.43912905]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 13\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.01395786 -0.01272128  0.06859975  0.165886  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 13\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.01370344 -0.20875473  0.07191747  0.4793971 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 13\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.00952834 -0.01471782  0.08150541  0.21021959]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 13\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.00923398  0.17914987  0.0857098  -0.05568051]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 14 of episode: 13\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.01281698  0.37294498  0.08459619 -0.32013825]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 15 of episode: 13\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02027588  0.5667667   0.07819343 -0.5849894 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 16 of episode: 13\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03161122  0.37064153  0.06649364 -0.2687352 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 17 of episode: 13\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [0.03902404 0.17463677 0.06111893 0.04415696]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 18 of episode: 13\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.04251678 -0.02130594  0.06200207  0.35548   ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 19 of episode: 13\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.04209066 -0.21725214  0.06911167  0.66705143]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 20 of episode: 13\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03774562 -0.4132637   0.0824527   0.98066956]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 21 of episode: 13\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02948035 -0.6093882   0.10206609  1.2980705 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 22 of episode: 13\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.01729258 -0.8056472   0.1280275   1.6208823 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 23 of episode: 13\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 1.1796368e-03 -6.1224478e-01  1.6044515e-01  1.3706903e+00]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 24 of episode: 13\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.01106526 -0.80896825  0.18785895  1.7089543 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 25 of episode: 13\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.02724462 -0.6164368   0.22203805  1.4801443 ]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 14/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 14\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02727336 -0.20172954  0.03625453  0.32351476]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 14\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02323877 -0.39734846  0.04272482  0.62740684]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 14\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.0152918  -0.20284806  0.05527296  0.34847975]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 14\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.01123484 -0.00855404  0.06224256  0.07372576]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 14\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.01106376 -0.20451054  0.06371707  0.3853788 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 14\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.00697355 -0.01034828  0.07142465  0.11344658]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 14\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.00676658  0.18368141  0.07369358 -0.1558749 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 14\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.01044021  0.37767512  0.07057608 -0.42442936]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 14\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.01799371  0.57173     0.06208749 -0.69405496]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 14\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02942831  0.7659383   0.04820639 -0.96656406]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 14\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.04474708  0.96038085  0.02887511 -1.2437218 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 14\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.0639547   0.7649005   0.00400068 -0.94213545]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 14\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.0792527   0.56972486 -0.01484203 -0.6481982 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 14 of episode: 14\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.09064721  0.7650504  -0.027806   -0.9455176 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 15 of episode: 14\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.10594822  0.5703138  -0.04671635 -0.66169953]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 16 of episode: 14\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.11735449  0.37587196 -0.05995034 -0.38408455]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 17 of episode: 14\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.12487193  0.5717915  -0.06763203 -0.69505084]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 18 of episode: 14\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.13630776  0.7677831  -0.08153305 -1.0082347 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 19 of episode: 14\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.15166342  0.96389323 -0.10169774 -1.325367  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 20 of episode: 14\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.1709413   0.77019185 -0.12820508 -1.0661637 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 21 of episode: 14\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.18634513  0.96675545 -0.14952835 -1.3961784 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 22 of episode: 14\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.20568024  1.1633874  -0.17745192 -1.7316328 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 23 of episode: 14\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.22894798  1.3600365  -0.21208458 -2.0738719 ]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 15/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 15\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02727336  0.18847767  0.03625453 -0.26141977]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 15\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03104291  0.38306385  0.03102613 -0.5424507 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 15\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03870419  0.1875199   0.02017712 -0.24015574]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 15\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.04245459 -0.0078844   0.015374    0.0588227 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 15\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.0422969   0.18701379  0.01655046 -0.22897024]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 15\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.04603718  0.38189536  0.01197105 -0.516387  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 15\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.05367509  0.1866069   0.00164331 -0.21995586]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 15\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.05740722  0.38170534 -0.0027558  -0.51211995]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 15\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.06504133  0.1866223  -0.0129982  -0.22030675]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 15\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.06877378  0.3819276  -0.01740434 -0.51706135]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 15\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.07641233  0.18705499 -0.02774557 -0.2299133 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 15\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.08015343  0.38256222 -0.03234383 -0.5312174 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 15\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.08780468  0.5781238  -0.04296818 -0.83391386]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 14 of episode: 15\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.09936715  0.38361448 -0.05964646 -0.55504787]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 15 of episode: 15\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.10703944  0.1893785  -0.07074741 -0.28173766]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 16 of episode: 15\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.11082701  0.38543454 -0.07638217 -0.59586847]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 17 of episode: 15\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.1185357   0.19145997 -0.08829954 -0.3281895 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 18 of episode: 15\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.1223649   0.38772076 -0.09486333 -0.6473601 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 19 of episode: 15\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.13011931  0.19403955 -0.10781053 -0.38599253]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 20 of episode: 15\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.13400011  0.00060005 -0.11553038 -0.12915285]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 21 of episode: 15\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.1340121  -0.19269367 -0.11811344  0.12496451]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 22 of episode: 15\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.13015823  0.00390494 -0.11561415 -0.20252344]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 23 of episode: 15\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.13023633  0.20047422 -0.11966461 -0.5293238 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 24 of episode: 15\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.13424581  0.39705837 -0.1302511  -0.8571888 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 25 of episode: 15\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.14218698  0.5936914  -0.14739487 -1.1878256 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 26 of episode: 15\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.15406081  0.4007552  -0.17115138 -0.9447366 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 27 of episode: 15\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.1620759   0.5977177  -0.19004612 -1.285937  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 28 of episode: 15\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.17403027  0.40545338 -0.21576485 -1.0582707 ]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 16/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 16\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02727336  0.18847767  0.03625453 -0.26141977]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 16\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03104291  0.38306385  0.03102613 -0.5424507 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 16\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03870419  0.5777363   0.02017712 -0.8251987 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 16\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.05025892  0.3823443   0.00367314 -0.5262386 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 16\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.0579058   0.5774144  -0.00685163 -0.8177619 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 16\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.06945409  0.38238686 -0.02320687 -0.5272419 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 16\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.07710183  0.187599   -0.0337517  -0.24196094]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 16\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.08085381  0.38318643 -0.03859092 -0.5450961 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 16\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.08851754  0.57882875 -0.04949284 -0.84968406]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 16\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.10009411  0.7745895  -0.06648652 -1.1575105 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 16\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.1155859   0.5803941  -0.08963674 -0.8863936 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 16\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.12719378  0.38659582 -0.10736461 -0.62318116]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 16\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.1349257   0.19312377 -0.11982823 -0.36614874]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 14 of episode: 16\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 1.3878818e-01 -1.0965749e-04 -1.2715121e-01 -1.1352232e-01]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 15 of episode: 16\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.13878599  0.19658321 -0.12942165 -0.44346163]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 16 of episode: 16\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.14271764  0.00350725 -0.13829088 -0.19421364]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 17 of episode: 16\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.1427878  -0.18939345 -0.14217515  0.05184693]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 18 of episode: 16\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.13899992  0.00745058 -0.14113821 -0.28209874]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 19 of episode: 16\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.13914894  0.20427398 -0.1467802  -0.6157552 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 20 of episode: 16\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.14323442  0.01147462 -0.1590953  -0.37266445]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 21 of episode: 16\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.14346391  0.20845671 -0.16654858 -0.71098304]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 22 of episode: 16\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.14763305  0.4054454  -0.18076825 -1.0511166 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 23 of episode: 16\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.15574194  0.2131209  -0.20179057 -0.8201874 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 24 of episode: 16\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.16000436  0.41034764 -0.21819432 -1.1689494 ]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 17/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 17\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02727336  0.18847767  0.03625453 -0.26141977]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 17\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03104291  0.38306385  0.03102613 -0.5424507 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 17\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03870419  0.1875199   0.02017712 -0.24015574]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 17\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.04245459 -0.0078844   0.015374    0.0588227 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 17\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.0422969   0.18701379  0.01655046 -0.22897024]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 17\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.04603718 -0.00834072  0.01197105  0.06888699]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 17\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.04587036 -0.20363224  0.01334879  0.36532268]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 17\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.04179772 -0.3989413   0.02065525  0.66218466]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 17\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03381889 -0.5943445   0.03389894  0.96129906]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 17\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.021932   -0.39969411  0.05312492  0.67945564]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 17\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.01393812 -0.2053488   0.06671403  0.40396032]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 17\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.00983114 -0.40135035  0.07479324  0.716908  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 17\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.00180414 -0.20733884  0.0891314   0.44867304]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 14 of episode: 17\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.00234264 -0.01358326  0.09810486  0.18536331]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 15 of episode: 17\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.00261431  0.18000813  0.10181212 -0.07483087]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 16 of episode: 17\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.00098586  0.3735343   0.10031551 -0.33373582]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 17 of episode: 17\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.00845654  0.17713831  0.09364079 -0.01117956]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 18 of episode: 17\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.01199931 -0.01919314  0.0934172   0.30951756]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 19 of episode: 17\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [0.01161545 0.17448226 0.09960755 0.04769589]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 20 of episode: 17\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.01510509 -0.02191645  0.10056147  0.37006992]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 21 of episode: 17\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.01466676 -0.21831265  0.10796287  0.6926912 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 22 of episode: 17\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.01030051 -0.41475362  0.12181669  1.0173165 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 23 of episode: 17\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.00200544 -0.22144762  0.14216302  0.76523185]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 24 of episode: 17\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.00242351 -0.41821107  0.15746766  1.0990548 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 25 of episode: 17\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.01078774 -0.2254723   0.17944875  0.859628  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 26 of episode: 17\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.01529718 -0.4225249   0.19664131  1.2029358 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 27 of episode: 17\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.02374768 -0.2304117   0.22070003  0.97776026]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 18/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 18\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02727336  0.18847767  0.03625453 -0.26141977]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 18\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03104291  0.38306385  0.03102613 -0.5424507 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 18\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03870419  0.5777363   0.02017712 -0.8251987 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 18\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.05025892  0.7725766   0.00367314 -1.111468  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 18\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.06571045  0.5774066  -0.01855621 -0.817635  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 18\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.07725858  0.77277756 -0.03490891 -1.1160963 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 18\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.09271413  0.5781308  -0.05723084 -0.83456504]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 18\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.10427675  0.773986   -0.07392213 -1.1446835 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 18\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.11975647  0.57990336 -0.09681581 -0.876068  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 18\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.13135454  0.7761985  -0.11433717 -1.1975513 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 18\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.14687851  0.5827267  -0.1382882  -0.9427804 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 18\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.15853304  0.38971168 -0.1571438  -0.69654906]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 18\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.16632727  0.1970776  -0.17107478 -0.45716867]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 14 of episode: 18\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.17026882  0.3941528  -0.18021816 -0.7985172 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 15 of episode: 18\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.17815188  0.20190029 -0.1961885  -0.5675067 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 16 of episode: 18\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.18218988  0.39915454 -0.20753863 -0.91502213]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 17 of episode: 18\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.19017297  0.5963871  -0.22583908 -1.2651017 ]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 19/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 19\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02727336  0.18847767  0.03625453 -0.26141977]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 19\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03104291 -0.00714255  0.03102613  0.04247424]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 19\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03090006  0.18752107  0.03187562 -0.24026045]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 19\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03465049  0.3821735   0.02707041 -0.52272093]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 19\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.04229395  0.57690424  0.01661599 -0.80675226]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 19\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 5.3832039e-02  7.7179450e-01  4.8094502e-04 -1.0941625e+00]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 19\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.06926793  0.9669101  -0.0214023  -1.3866944 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 19\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.08860613  1.1622922  -0.0491362  -1.6859922 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 19\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.11185198  1.357947   -0.08285604 -1.9935604 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 19\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.13901092  1.5538328  -0.12272725 -2.3107128 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 19\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.17008758  1.360028   -0.1689415  -2.0581837 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 19\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.19728813  1.5564259  -0.21010518 -2.3980193 ]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 20/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 20\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02727336 -0.20172954  0.03625453  0.32351476]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 20\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02323877 -0.39734846  0.04272482  0.62740684]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 20\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.0152918  -0.5930399   0.05527296  0.9332334 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 20\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.003431   -0.39870548  0.07393762  0.6584189 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 20\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.00454311 -0.20468625  0.087106    0.38990307]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 20\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.00863683 -0.40092954  0.09490407  0.70872873]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 20\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.01665542 -0.597229    0.10907864  1.0297128 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 20\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.0286    -0.79362    0.1296729  1.354554 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 20\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.0444724  -0.990109    0.15676399  1.6848302 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 20\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.06427458 -0.79711044  0.19046058  1.4447824 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 20\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.08021679 -0.6047733   0.21935622  1.217151  ]\n",
      "Reward: 1.0\n",
      "\n",
      "****************************************************\n",
      "Episode: 20, Return: 11.0\n",
      "****************************************************\n",
      "-----------------------------------\n",
      "Episode 21/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 21\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02727336  0.18847767  0.03625453 -0.26141977]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 21\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03104291  0.38306385  0.03102613 -0.5424507 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 21\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03870419  0.5777363   0.02017712 -0.8251987 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 21\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.05025892  0.7725766   0.00367314 -1.111468  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 21\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.06571045  0.96765006 -0.01855621 -1.4029963 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 21\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.08506345  1.1629975  -0.04661614 -1.7014222 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 21\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.1083234   0.9684425  -0.08064459 -1.4236069 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 21\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.12769225  1.1644636  -0.10911672 -1.7403668 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 21\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.15098153  1.360646   -0.14392406 -2.064907  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 21\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.17819445  1.1672542  -0.1852222  -1.8199863 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 21\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.20153953  0.9746112  -0.22162192 -1.590109  ]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 22/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 22\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02727336 -0.20172954  0.03625453  0.32351476]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 22\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02323877 -0.00714208  0.04272482  0.04248186]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 22\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02309593 -0.20284982  0.04357446  0.34833285]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 22\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.01903893 -0.00837385  0.05054112  0.06970263]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 22\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.01887146 -0.2041826   0.05193517  0.37789345]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 22\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.0147878  -0.40000218  0.05949304  0.68648887]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 22\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.00678776 -0.5958973   0.07322282  0.9972924 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 22\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.00513019 -0.7919179   0.09316867  1.3120437 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 22\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.02096854 -0.5980909   0.11940954  1.049916  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 22\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.03293036 -0.794577    0.14040786  1.3775696 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 22\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.0488219  -0.99114543  0.16795924  1.710665  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 22\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.06864481 -1.1877519   0.20217255  2.0505726 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 22\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.09239985 -0.9951953   0.243184    1.8266554 ]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 23/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 23\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02727336 -0.20172954  0.03625453  0.32351476]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 23\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02323877 -0.00714208  0.04272482  0.04248186]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 23\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02309593 -0.20284982  0.04357446  0.34833285]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 23\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.01903893 -0.39856356  0.05054112  0.65443164]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 23\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.01106766 -0.5943515   0.06362975  0.96259123]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 23\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.00081937 -0.40013957  0.08288158  0.6905575 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 23\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.00882216 -0.5963078   0.09669273  1.008139  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 23\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.02074832 -0.40260035  0.11685551  0.74731845]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 23\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.02880032 -0.59912395  0.13180187  1.0743709 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 23\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.0407828  -0.40596655  0.15328929  0.8257854 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 23\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.04890213 -0.2132359   0.169805    0.58497006]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 23\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.05316685 -0.4102784   0.1815044   0.9259661 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 23\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.06137242 -0.21801011  0.20002373  0.6953709 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 14 of episode: 23\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.06573262 -0.41526127  0.21393114  1.043764  ]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 24/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 24\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02727336  0.18847767  0.03625453 -0.26141977]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 24\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03104291  0.38306385  0.03102613 -0.5424507 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 24\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03870419  0.1875199   0.02017712 -0.24015574]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 24\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.04245459  0.38234788  0.015374   -0.5264066 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 24\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.05010155  0.18701302  0.00484587 -0.22891912]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 24\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.05384181 -0.00817785  0.00026749  0.06528842]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 24\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.05367825 -0.20330364  0.00157326  0.35805574]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 24\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.04961218 -0.00820409  0.00873437  0.0658693 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 24\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.0494481  -0.20345017  0.01005176  0.36129513]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 24\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.04537909 -0.39871353  0.01727766  0.65713054]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 24\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03740482 -0.5940717   0.03042027  0.9552034 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 24\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02552339 -0.39937183  0.04952434  0.67223096]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 24\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.01753595 -0.59514594  0.06296896  0.98008627]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 14 of episode: 24\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.00563303 -0.7910529   0.08257069  1.2918649 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 15 of episode: 24\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.01018802 -0.59707195  0.10840798  1.0261331 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 16 of episode: 24\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.02212946 -0.79345715  0.12893064  1.3507911 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 17 of episode: 24\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.03799861 -0.9899412   0.15594646  1.6808716 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 18 of episode: 24\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.05779743 -1.1864885   0.1895639   2.0177777 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 19 of episode: 24\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.0815272  -1.3830054   0.22991945  2.3626711 ]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 25/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 25\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02727336 -0.20172954  0.03625453  0.32351476]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 25\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02323877 -0.39734846  0.04272482  0.62740684]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 25\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.0152918  -0.20284806  0.05527296  0.34847975]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 25\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.01123484 -0.00855404  0.06224256  0.07372576]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 25\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.01106376 -0.20451054  0.06371707  0.3853788 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 25\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.00697355 -0.01034828  0.07142465  0.11344658]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 25\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.00676658  0.18368141  0.07369358 -0.1558749 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 25\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.01044021 -0.01241407  0.07057608  0.15911628]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 25\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.01019193  0.18163021  0.07375841 -0.1104934 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 25\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.01382453 -0.01446691  0.07154854  0.20451784]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 25\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.0135352  -0.21053524  0.0756389   0.5188852 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 25\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.00932449 -0.40663606  0.0860166   0.834412  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 25\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.00119177 -0.60282123  0.10270484  1.1528594 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 14 of episode: 25\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.01086465 -0.79912186  0.12576203  1.475901  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 15 of episode: 25\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.02684709 -0.6057406   0.15528005  1.2249967 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 16 of episode: 25\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.0389619  -0.80248314  0.17977998  1.5620288 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 17 of episode: 25\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.05501157 -0.9992422   0.21102056  1.9049817 ]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 26/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 26\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02727336 -0.20172954  0.03625453  0.32351476]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 26\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02323877 -0.00714208  0.04272482  0.04248186]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 26\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02309593  0.187342    0.04357446 -0.23642075]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 26\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02684277 -0.00837454  0.03884605  0.06968222]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 26\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02667528  0.18616958  0.04023969 -0.21049595]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 26\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03039867  0.38069376  0.03602977 -0.49021873]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 26\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03801255  0.18508255  0.0262254  -0.18640189]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 26\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.0417142   0.37981966  0.02249736 -0.47069773]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 26\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.04931059  0.18438727  0.0130834  -0.17100966]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 26\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.05299833  0.37931955  0.00966321 -0.4595366 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 26\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 6.0584728e-02  5.7430357e-01  4.7247799e-04 -7.4915808e-01]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 26\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.0720708   0.769419   -0.01451068 -1.0416923 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 26\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.08745918  0.57449275 -0.03534453 -0.7535997 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 14 of episode: 26\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.09894903  0.7700837  -0.05041652 -1.0571921 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 15 of episode: 26\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.11435071  0.96583617 -0.07156037 -1.3652638 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 16 of episode: 26\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.13366744  0.77167964 -0.09886564 -1.0957952 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 17 of episode: 26\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.14910102  0.5779888  -0.12078154 -0.83569676]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 18 of episode: 26\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.1606608   0.77453524 -0.13749547 -1.1637921 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 19 of episode: 26\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.1761515   0.58144474 -0.16077133 -0.91718626]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 20 of episode: 26\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.1877804   0.38881874 -0.17911504 -0.67903686]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 21 of episode: 26\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.19555677  0.19657731 -0.19269578 -0.44766667]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 22 of episode: 26\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.19948833  0.39382812 -0.20164911 -0.79436934]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 23 of episode: 26\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.20736489  0.20196024 -0.21753651 -0.57128286]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 27/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 27\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02727336  0.18847767  0.03625453 -0.26141977]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 27\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03104291  0.38306385  0.03102613 -0.5424507 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 27\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03870419  0.1875199   0.02017712 -0.24015574]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 27\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.04245459  0.38234788  0.015374   -0.5264066 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 27\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.05010155  0.18701302  0.00484587 -0.22891912]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 27\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 5.3841807e-02  3.8206539e-01  2.6748964e-04 -5.2006954e-01]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 27\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.06148311  0.18693967 -0.0101339  -0.22730236]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 27\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.06522191  0.38220495 -0.01467995 -0.5231646 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 27\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.07286601  0.18729267 -0.02514324 -0.23514338]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 27\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.07661186 -0.00746119 -0.02984611  0.04950374]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 27\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.07646263 -0.20214275 -0.02885603  0.33262256]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 27\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.07241978 -0.39684236 -0.02220358  0.6160679 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 27\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.06448293 -0.59164715 -0.00988222  0.90167576]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 14 of episode: 27\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.05264999 -0.39639273  0.00815129  0.6059031 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 15 of episode: 27\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.04472214 -0.5916277   0.02026935  0.9011423 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 16 of episode: 27\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03288958 -0.39678618  0.0382922   0.6148988 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 17 of episode: 27\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02495386 -0.59242165  0.05059018  0.91939205]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 18 of episode: 27\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.01310543 -0.7881896   0.06897802  1.2275354 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 19 of episode: 27\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.00265836 -0.9841282   0.09352873  1.5410079 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 20 of episode: 27\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.02234093 -1.1802422   0.12434888  1.8613504 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 21 of episode: 27\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.04594577 -0.98668385  0.16157588  1.6097193 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 22 of episode: 27\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.06567945 -0.7937976   0.19377027  1.3714553 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 23 of episode: 27\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.0815554  -0.6015538   0.22119938  1.1450989 ]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 28/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 28\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02727336 -0.20172954  0.03625453  0.32351476]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 28\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02323877 -0.00714208  0.04272482  0.04248186]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 28\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02309593 -0.20284982  0.04357446  0.34833285]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 28\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.01903893 -0.39856356  0.05054112  0.65443164]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 28\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.01106766 -0.20418042  0.06362975  0.378082  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 28\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.00698405 -0.01001712  0.07119139  0.10612098]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 28\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.00678371 -0.20608324  0.07331381  0.4203878 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 28\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.00266205 -0.40216327  0.08172157  0.7352527 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 28\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.00538122 -0.59831333  0.09642662  1.0524952 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 28\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.01734748 -0.40459305  0.11747652  0.79157025]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 28\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.02543935 -0.21126309  0.13330793  0.5380328 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 28\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.02966461 -0.01824238  0.14406858  0.2901471 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 28\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.03002946 -0.21509331  0.14987153  0.62457424]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 14 of episode: 28\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.03433132 -0.0223463   0.16236301  0.38259214]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 15 of episode: 28\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.03477825 -0.2193563   0.17001486  0.72174585]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 16 of episode: 28\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.03916537 -0.02694295  0.18444978  0.48703098]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 17 of episode: 28\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.03970423 -0.22412284  0.1941904   0.83170503]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 18 of episode: 28\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.04418669 -0.03210933  0.21082449  0.60583216]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 29/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 29\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02727336  0.18847767  0.03625453 -0.26141977]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 29\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03104291  0.38306385  0.03102613 -0.5424507 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 29\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03870419  0.5777363   0.02017712 -0.8251987 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 29\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.05025892  0.3823443   0.00367314 -0.5262386 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 29\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.0579058   0.5774144  -0.00685163 -0.8177619 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 29\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.06945409  0.38238686 -0.02320687 -0.5272419 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 29\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.07710183  0.5778275  -0.0337517  -0.8271461 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 29\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.08865838  0.38318297 -0.05029463 -0.5452667 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 29\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.09632204  0.5789742  -0.06119996 -0.85336256]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 29\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.10790152  0.38473746 -0.07826721 -0.58053446]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 29\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.11559627  0.5808639  -0.0898779  -0.89681053]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 29\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.12721355  0.3870678  -0.10781411 -0.63367826]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 29\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.1349549   0.5835154  -0.12048768 -0.9582747 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 14 of episode: 29\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.14662522  0.78003323 -0.13965316 -1.286252  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 15 of episode: 29\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.16222587  0.58693725 -0.16537821 -1.0403527 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 16 of episode: 29\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.17396462  0.39435214 -0.18618527 -0.80381626]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 17 of episode: 29\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.18185167  0.591472   -0.2022616  -1.1488088 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 18 of episode: 29\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.1936811   0.7885756  -0.22523777 -1.4974974 ]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 30/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 30\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02727336  0.18847767  0.03625453 -0.26141977]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 30\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03104291 -0.00714255  0.03102613  0.04247424]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 30\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03090006  0.18752107  0.03187562 -0.24026045]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 30\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03465049  0.3821735   0.02707041 -0.52272093]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 30\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.04229395  0.57690424  0.01661599 -0.80675226]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 30\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 5.3832039e-02  7.7179450e-01  4.8094502e-04 -1.0941625e+00]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 30\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.06926793  0.57666624 -0.0214023  -0.80132866]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 30\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.08080126  0.77207506 -0.03742888 -1.1006666 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 30\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.09624276  0.5774651  -0.05944221 -0.8199577 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 30\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.10779206  0.77334803 -0.07584136 -1.1307287 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 30\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.12325902  0.5792966  -0.09845594 -0.8627643 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 30\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.13484494  0.38564304 -0.11571123 -0.6025901 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 30\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.14255781  0.5821771  -0.12776303 -0.9293631 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 14 of episode: 30\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.15420136  0.38898972 -0.1463503  -0.6794041 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 15 of episode: 30\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.16198115  0.58580875 -0.15993837 -1.0143502 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 16 of episode: 30\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.17369732  0.39313933 -0.18022537 -0.77585626]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 17 of episode: 30\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.18156011  0.2008932  -0.1957425  -0.5448575 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 18 of episode: 30\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.18557797  0.00898218 -0.20663965 -0.31966975]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 19 of episode: 30\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.18575762 -0.18268998 -0.21303305 -0.09859863]\n",
      "Reward: 1.0\n",
      "\n",
      "****************************************************\n",
      "Episode: 30, Return: 19.0\n",
      "****************************************************\n",
      "-----------------------------------\n",
      "Episode 31/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 31\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02727336 -0.20172954  0.03625453  0.32351476]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 31\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02323877 -0.39734846  0.04272482  0.62740684]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 31\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.0152918  -0.20284806  0.05527296  0.34847975]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 31\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.01123484 -0.00855404  0.06224256  0.07372576]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 31\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.01106376 -0.20451054  0.06371707  0.3853788 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 31\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.00697355 -0.40047646  0.07142465  0.69745135]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 31\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.00103598 -0.2064138   0.08537367  0.42808053]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 31\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.00516426 -0.01259816  0.09393528  0.16348532]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 31\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.00541622  0.18106231  0.09720499 -0.09814834]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 31\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.00179497  0.37466645  0.09524202 -0.35865033]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 31\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.00569836  0.17832868  0.08806901 -0.03751959]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 31\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.00926493 -0.01793857  0.08731862  0.28159913]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 31\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [0.00890616 0.17583638 0.0929506  0.01768317]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 14 of episode: 31\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.01242289 -0.02048717  0.09330427  0.3381854 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 15 of episode: 31\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.01201314 -0.2168044   0.10006798  0.65877205]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 16 of episode: 31\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.00767705 -0.41316622  0.11324342  0.9812122 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 17 of episode: 31\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-5.8627012e-04 -2.1972910e-01  1.3286766e-01  7.2613746e-01]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 18 of episode: 31\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.00498085 -0.02667008  0.14739041  0.4780496 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 19 of episode: 31\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.00551425  0.1660971   0.15695141  0.23520997]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 20 of episode: 31\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.00219231  0.3586691   0.1616556  -0.00414299]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 21 of episode: 31\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.00498107  0.551148    0.16157274 -0.24177706]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 22 of episode: 31\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [0.01600403 0.35413122 0.15673721 0.097195  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 23 of episode: 31\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02308665  0.54670006  0.15868111 -0.14222518]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 24 of episode: 31\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [0.03402065 0.34970346 0.1558366  0.19601403]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 25 of episode: 31\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [0.04101473 0.15273558 0.15975688 0.5335163 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 26 of episode: 31\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [0.04406944 0.3452928  0.1704272  0.2951275 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 27 of episode: 31\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [0.05097529 0.14820297 0.17632976 0.63634264]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 28 of episode: 31\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [0.05393935 0.34048456 0.1890566  0.4039652 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 29 of episode: 31\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [0.06074904 0.532493   0.19713591 0.17633645]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 30 of episode: 31\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.07139891  0.7243279   0.20066264 -0.04825928]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 31 of episode: 31\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.08588547  0.9160917   0.19969746 -0.27153358]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 32 of episode: 31\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [0.10420729 0.71876377 0.19426678 0.07689755]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 33 of episode: 31\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [0.11858257 0.52146447 0.19580473 0.42403546]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 34 of episode: 31\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [0.12901185 0.32418612 0.20428544 0.77149785]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 35 of episode: 31\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [0.13549559 0.5159993  0.2197154  0.54940385]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 32/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 32\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02727336 -0.20172954  0.03625453  0.32351476]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 32\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02323877 -0.39734846  0.04272482  0.62740684]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 32\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.0152918  -0.5930399   0.05527296  0.9332334 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 32\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.003431   -0.39870548  0.07393762  0.6584189 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 32\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.00454311 -0.20468625  0.087106    0.38990307]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 32\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.00863683 -0.40092954  0.09490407  0.70872873]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 32\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.01665542 -0.20724133  0.10907864  0.44736376]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 32\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.02080025 -0.40372363  0.11802591  0.77234113]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 32\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.02887472 -0.60025465  0.13347274  1.0997059 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 32\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.04087981 -0.79685634  0.15546685  1.4311101 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 32\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.05681694 -0.99351776  0.18408906  1.7680678 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 32\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.0766873  -1.1901802   0.21945041  2.111894  ]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 33/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 33\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02727336 -0.20172954  0.03625453  0.32351476]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 33\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02323877 -0.00714208  0.04272482  0.04248186]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 33\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02309593  0.187342    0.04357446 -0.23642075]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 33\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02684277  0.3818152   0.03884605 -0.51504683]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 33\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03447907  0.57636917  0.02854511 -0.79523975]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 33\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.04600646  0.38086733  0.01264031 -0.49371535]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 33\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.0536238   0.1855694   0.00276601 -0.1970757 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 33\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.05733519  0.38065168 -0.00117551 -0.4888848 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 33\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.06494822  0.5757902  -0.0109532  -0.78193796]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 33\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.07646403  0.3808205  -0.02659196 -0.49272114]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 33\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.08408044  0.57630724 -0.03644639 -0.7936647 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 33\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.09560658  0.38170403 -0.05231968 -0.5126666 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 33\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.10324066  0.18735652 -0.06257301 -0.23691921]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 14 of episode: 33\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.1069878   0.38331404 -0.06731139 -0.5486647 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 15 of episode: 33\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.11465407  0.18919902 -0.07828469 -0.27792615]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 16 of episode: 33\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.11843806 -0.00472391 -0.08384321 -0.01092484]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 17 of episode: 33\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.11834358 -0.1985496  -0.0840617   0.25417137]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 18 of episode: 33\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.11437258 -0.00233426 -0.07897828 -0.06379741]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 19 of episode: 33\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.1143259  -0.19624025 -0.08025423  0.20295943]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 20 of episode: 33\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.11040109 -0.39012823 -0.07619504  0.46928588]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 21 of episode: 33\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.10259853 -0.19401725 -0.06680932  0.15359326]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 22 of episode: 33\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.09871819  0.00199454 -0.06373746 -0.15939583]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 23 of episode: 33\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.09875808 -0.19215974 -0.06692538  0.1125181 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 24 of episode: 33\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.09491488 -0.38626206 -0.06467501  0.3833584 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 25 of episode: 33\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.08718964 -0.19028431 -0.05700784  0.0710052 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 26 of episode: 33\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.08338396 -0.3845446  -0.05558774  0.34517103]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 27 of episode: 33\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.07569306 -0.5788336  -0.04868432  0.61981994]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 28 of episode: 33\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.06411639 -0.77324295 -0.03628792  0.89678097]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 29 of episode: 33\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.04865153 -0.57764834 -0.0183523   0.5929159 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 30 of episode: 33\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03709856 -0.38227436 -0.00649398  0.294509  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 31 of episode: 33\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02945308 -0.5773031  -0.0006038   0.5851368 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 32 of episode: 33\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.01790702 -0.38217273  0.01109893  0.2922637 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 33 of episode: 33\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.01026356 -0.18721077  0.01694421  0.00310179]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 34 of episode: 33\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.00651935  0.00766413  0.01700624 -0.28418723]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 35 of episode: 33\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.00667263  0.20253946  0.0113225  -0.57145834]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 36 of episode: 33\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 1.0723418e-02  7.2605740e-03 -1.0666939e-04 -2.7523008e-01]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 37 of episode: 33\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.01086863 -0.18785985 -0.00561127  0.01741922]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 38 of episode: 33\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.00711143 -0.3829009  -0.00526289  0.30832645]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 39 of episode: 33\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-5.4658577e-04 -5.7794744e-01  9.0364262e-04  5.9934497e-01]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 40 of episode: 33\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.01210553 -0.38283816  0.01289054  0.3069468 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 41 of episode: 33\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.0197623  -0.5781414   0.01902948  0.6036671 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 42 of episode: 33\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.03132512 -0.38329068  0.03110282  0.31703824]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 43 of episode: 33\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.03899094 -0.18862523  0.03744359  0.03432404]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 44 of episode: 33\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.04276345 -0.38426358  0.03813006  0.3385817 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 45 of episode: 33\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.05044872 -0.18970436  0.0449017   0.05816252]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 46 of episode: 33\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.0542428   0.00474598  0.04606495 -0.22002234]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 47 of episode: 33\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.05414788 -0.19100313  0.0416645   0.08682815]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 48 of episode: 33\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.05796795  0.0034976   0.04340107 -0.19242392]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 49 of episode: 33\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.05789799 -0.19221748  0.03955259  0.11362815]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 50 of episode: 33\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.06174234 -0.3878832   0.04182515  0.4185226 ]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 34/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 34\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02727336 -0.20172954  0.03625453  0.32351476]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 34\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02323877 -0.39734846  0.04272482  0.62740684]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 34\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.0152918  -0.5930399   0.05527296  0.9332334 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 34\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.003431   -0.7888622   0.07393762  1.2427603 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 34\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.01234624 -0.9848512   0.09879284  1.5576583 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 34\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.03204326 -0.7910413   0.129946    1.297359  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 34\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.04786409 -0.98755175  0.15589318  1.6277361 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 34\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.06761513 -0.79456854  0.18844791  1.3874189 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 34\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.08350649 -0.6022278   0.21619628  1.1590918 ]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 35/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 35\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02727336 -0.20172954  0.03625453  0.32351476]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 35\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02323877 -0.39734846  0.04272482  0.62740684]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 35\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.0152918  -0.5930399   0.05527296  0.9332334 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 35\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.003431   -0.7888622   0.07393762  1.2427603 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 35\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.01234624 -0.9848512   0.09879284  1.5576583 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 35\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.03204326 -1.1810076   0.129946    1.8794562 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 35\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.05566342 -1.3772845   0.16753513  2.209486  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 35\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.0832091  -1.573571    0.21172485  2.5488186 ]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 36/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 36\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02727336 -0.20172954  0.03625453  0.32351476]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 36\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02323877 -0.00714208  0.04272482  0.04248186]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 36\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02309593 -0.20284982  0.04357446  0.34833285]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 36\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.01903893 -0.00837385  0.05054112  0.06970263]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 36\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.01887146  0.18598844  0.05193517 -0.20661576]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 36\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02259123  0.38033074  0.04780285 -0.4824741 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 36\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03019784  0.18456784  0.03815337 -0.17511648]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 36\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.0338892  -0.01107878  0.03465104  0.12935425]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 36\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03366762  0.18353009  0.03723813 -0.15219845]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 36\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03733822  0.3780996   0.03419416 -0.4329049 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 36\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.04490022  0.5727211   0.02553606 -0.71461546]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 36\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.05635464  0.37725517  0.01124375 -0.41400528]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 36\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.06389974  0.18197566  0.00296365 -0.11779896]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 14 of episode: 36\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.06753925 -0.01318862  0.00060767  0.1758175 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 15 of episode: 36\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.06727548 -0.20831926  0.00412402  0.46869206]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 16 of episode: 36\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.0631091  -0.40349922  0.01349786  0.762672  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 17 of episode: 36\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.05503911 -0.20856577  0.0287513   0.47426668]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 18 of episode: 36\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.0508678  -0.0138614   0.03823663  0.19078255]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 19 of episode: 36\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.05059057  0.18069327  0.04205228 -0.08959732]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 20 of episode: 36\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.05420443 -0.01500542  0.04026034  0.21605092]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 21 of episode: 36\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.05390432 -0.21067911  0.04458135  0.52115697]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 22 of episode: 36\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.04969075 -0.01621213  0.0550045   0.2428489 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 23 of episode: 36\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.0493665   0.17808276  0.05986147 -0.0319895 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 24 of episode: 36\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.05292816  0.37229747  0.05922168 -0.305201  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 25 of episode: 36\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.06037411  0.56652766  0.05311766 -0.5786345 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 26 of episode: 36\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.07170466  0.7608665   0.04154497 -0.8541224 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 27 of episode: 36\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.08692199  0.56520367  0.02446252 -0.5486707 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 28 of episode: 36\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.09822606  0.3697468   0.01348911 -0.24838182]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 29 of episode: 36\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.105621    0.56467354  0.00852147 -0.53677964]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 30 of episode: 36\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.11691447  0.3694328  -0.00221412 -0.24142392]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 31 of episode: 36\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.12430312  0.56458634 -0.0070426  -0.5348044 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 32 of episode: 36\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.13559486  0.7598066  -0.01773869 -0.8296981 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 33 of episode: 36\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.15079099  0.95516646 -0.03433265 -1.1279067 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 34 of episode: 36\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.16989431  0.7605107  -0.05689078 -0.84618694]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 35 of episode: 36\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.18510453  0.95636076 -0.07381452 -1.1562035 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 36 of episode: 36\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.20423174  1.1523633  -0.0969386  -1.4710885 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 37 of episode: 36\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.227279    0.95855135 -0.12636036 -1.2101909 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 38 of episode: 36\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.24645004  1.1550579  -0.15056418 -1.5396518 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 39 of episode: 36\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.2695512   0.96203357 -0.18135722 -1.2974898 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 40 of episode: 36\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.28879187  0.7696177  -0.20730701 -1.0666267 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 41 of episode: 36\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.30418423  0.96678865 -0.22863954 -1.4165632 ]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 37/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 37\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02727336 -0.20172954  0.03625453  0.32351476]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 37\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02323877 -0.39734846  0.04272482  0.62740684]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 37\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.0152918  -0.20284806  0.05527296  0.34847975]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 37\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.01123484 -0.39871082  0.06224256  0.65806717]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 37\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.00326062 -0.5946414   0.0754039   0.9696814 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 37\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.0086322  -0.40060818  0.09479753  0.7016064 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 37\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.01664437 -0.5969074   0.10882965  1.0225619 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 37\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.02858252 -0.40339017  0.1292809   0.76593614]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 37\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.03665032 -0.6000327   0.14459962  1.0963411 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 37\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.04865097 -0.79673177  0.16652644  1.4306749 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 37\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.06458561 -0.99347156  0.19513993  1.770435  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 37\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.08445504 -1.1901875   0.23054864  2.1169162 ]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 38/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 38\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02727336  0.18847767  0.03625453 -0.26141977]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 38\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03104291  0.38306385  0.03102613 -0.5424507 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 38\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03870419  0.1875199   0.02017712 -0.24015574]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 38\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.04245459  0.38234788  0.015374   -0.5264066 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 38\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.05010155  0.5772502   0.00484587 -0.81420565]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 38\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.06164655  0.7723054  -0.01143824 -1.1053604 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 38\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.07709266  0.9675759  -0.03354545 -1.4016098 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 38\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.09644417  0.7728864  -0.06157764 -1.1196003 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 38\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.1119019   0.9687597  -0.08396965 -1.4309459 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 38\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.1312771   0.77476865 -0.11258857 -1.1656426 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 38\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.14677247  0.58127755 -0.13590142 -0.9102747 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 38\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.15839802  0.77795094 -0.15410691 -1.2423968 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 38\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.17395705  0.5851056  -0.17895485 -1.0016853 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 14 of episode: 38\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.18565916  0.39276737 -0.19898856 -0.7701176 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 15 of episode: 38\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.1935145  0.5899901 -0.2143909 -1.1182313]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 39/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 39\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02727336  0.18847767  0.03625453 -0.26141977]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 39\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03104291  0.38306385  0.03102613 -0.5424507 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 39\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03870419  0.1875199   0.02017712 -0.24015574]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 39\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.04245459 -0.0078844   0.015374    0.0588227 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 39\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.0422969   0.18701379  0.01655046 -0.22897024]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 39\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.04603718  0.38189536  0.01197105 -0.516387  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 39\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.05367509  0.5768467   0.00164331 -0.80527365]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 39\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.06521202  0.38170227 -0.01446216 -0.51207423]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 39\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.07284606  0.186787   -0.02470364 -0.22398362]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 39\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.07658181 -0.00797333 -0.02918332  0.0608056 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 39\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.07642234  0.18755463 -0.02796721 -0.24094012]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 39\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.08017343  0.3830647  -0.03278601 -0.54231185]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 39\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.08783472  0.1884185  -0.04363224 -0.2601368 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 14 of episode: 39\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.09160309  0.38413528 -0.04883498 -0.5662564 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 15 of episode: 39\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.0992858   0.18973121 -0.06016011 -0.2893497 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 16 of episode: 39\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.10308042  0.38565713 -0.0659471  -0.60038334]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 17 of episode: 39\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.11079357  0.19151673 -0.07795477 -0.32918012]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 18 of episode: 39\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.1146239   0.38765687 -0.08453837 -0.64539236]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 19 of episode: 39\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.12237704  0.19380838 -0.09744622 -0.3804827 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 20 of episode: 39\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.1262532   0.00019547 -0.10505587 -0.12004501]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 21 of episode: 39\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.12625712 -0.1932767  -0.10745677  0.1377336 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 22 of episode: 39\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.12239158  0.00320715 -0.1047021  -0.18682374]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 23 of episode: 39\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.12245572  0.19965917 -0.10843857 -0.5106142 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 24 of episode: 39\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.1264489   0.00621848 -0.11865086 -0.25397602]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 25 of episode: 39\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.12657328  0.20281711 -0.12373038 -0.5816022 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 26 of episode: 39\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.13062961  0.00962616 -0.13536243 -0.3303151 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 27 of episode: 39\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.13082214 -0.18333536 -0.14196873 -0.08319565]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 28 of episode: 39\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.12715544 -0.376167   -0.14363264  0.16154306]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 29 of episode: 39\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.1196321  -0.5689719  -0.14040178  0.4056893 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 30 of episode: 39\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.10825266 -0.37216717 -0.132288    0.07224452]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 31 of episode: 39\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.10080931 -0.565169   -0.1308431   0.32043839]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 32 of episode: 39\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.08950593 -0.7582084  -0.12443434  0.5691643 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 33 of episode: 39\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.07434177 -0.9513856  -0.11305105  0.82020026]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 34 of episode: 39\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.05531406 -1.144794   -0.09664705  1.0752946 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 35 of episode: 39\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03241818 -1.3385154  -0.07514115  1.3361506 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 36 of episode: 39\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.00564787 -1.1425314  -0.04841814  1.0209335 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 37 of episode: 39\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.01720276 -1.3369759  -0.02799947  1.2980291 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 38 of episode: 39\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.04394228 -1.5317314  -0.00203889  1.581817  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 39 of episode: 39\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.07457691 -1.726829    0.02959746  1.8738635 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 40 of episode: 39\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.10911348 -1.5320424   0.06707472  1.5905118 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 41 of episode: 39\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.13975434 -1.337778    0.09888496  1.3194757 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 42 of episode: 39\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.1665099  -1.1440353   0.12527448  1.0593063 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 43 of episode: 39\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.1893906 -1.3405739  0.1464606  1.3885382]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 44 of episode: 39\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.21620208 -1.5371855   0.17423137  1.7232039 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 45 of episode: 39\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.24694578 -1.7338214   0.20869544  2.0646572 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 46 of episode: 39\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.28162223 -1.9303712   0.24998859  2.413997  ]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 40/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 40\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02727336  0.18847767  0.03625453 -0.26141977]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 40\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03104291  0.38306385  0.03102613 -0.5424507 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 40\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03870419  0.1875199   0.02017712 -0.24015574]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 40\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.04245459  0.38234788  0.015374   -0.5264066 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 40\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.05010155  0.5772502   0.00484587 -0.81420565]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 40\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.06164655  0.7723054  -0.01143824 -1.1053604 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 40\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.07709266  0.9675759  -0.03354545 -1.4016098 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 40\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.09644417  0.7728864  -0.06157764 -1.1196003 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 40\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.1119019   0.57862395 -0.08396965 -0.8468513 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 40\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.12347438  0.7747848  -0.10090668 -1.164714  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 40\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.13897008  0.5811107  -0.12420096 -0.9052968 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 40\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.1505923  0.7776758 -0.1423069 -1.2342945]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 40\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.16614582  0.974311   -0.16699278 -1.5679629 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 14 of episode: 40\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.18563204  1.1709874  -0.19835204 -1.9077415 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 15 of episode: 40\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.20905177  1.3676215  -0.23650688 -2.2548435 ]\n",
      "Reward: 1.0\n",
      "\n",
      "****************************************************\n",
      "Episode: 40, Return: 15.0\n",
      "****************************************************\n",
      "-----------------------------------\n",
      "Episode 41/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 41\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02727336  0.18847767  0.03625453 -0.26141977]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 41\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03104291  0.38306385  0.03102613 -0.5424507 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 41\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03870419  0.1875199   0.02017712 -0.24015574]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 41\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.04245459 -0.0078844   0.015374    0.0588227 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 41\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.0422969  -0.20322338  0.01655046  0.3563163 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 41\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03823243 -0.00834059  0.02367678  0.06889778]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 41\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03806562 -0.20379385  0.02505474  0.36895582]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 41\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03398975 -0.00903668  0.03243386  0.08427709]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 41\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03380901  0.18560569  0.0341194  -0.19799905]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 41\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03752112  0.38022342  0.03015942 -0.47972658]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 41\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.0451256   0.57490695  0.02056488 -0.76275355]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 41\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.05662373  0.7697397   0.00530981 -1.0488952 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 41\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.07201853  0.96479076 -0.01566809 -1.3399067 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 14 of episode: 41\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.09131435  1.1601064  -0.04246622 -1.6374505 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 15 of episode: 41\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.11451647  1.3556999  -0.07521523 -1.9430573 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 16 of episode: 41\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.14163047  1.161456   -0.11407638 -1.6746078 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 17 of episode: 41\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.1648596   0.96782804 -0.14756854 -1.4195194 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 18 of episode: 41\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.18421616  1.1644357  -0.17595892 -1.7544535 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 19 of episode: 41\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.20750487  0.97169197 -0.21104799 -1.5212675 ]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 42/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 42\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02727336  0.18847767  0.03625453 -0.26141977]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 42\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03104291 -0.00714255  0.03102613  0.04247424]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 42\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03090006  0.18752107  0.03187562 -0.24026045]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 42\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03465049  0.3821735   0.02707041 -0.52272093]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 42\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.04229395  0.18668123  0.01661599 -0.22163221]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 42\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.04602758 -0.00867424  0.01218335  0.0762454 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 42\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.0458541   0.18627095  0.01370825 -0.21256886]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 42\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.04957952  0.38119426  0.00945688 -0.5008963 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 42\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.0572034   0.18594028 -0.00056105 -0.2052481 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 42\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.06092221  0.38107026 -0.00466601 -0.49810797]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 42\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.06854361  0.5762577  -0.01462817 -0.7922577 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 42\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.08006877  0.77157736 -0.03047333 -1.0895064 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 42\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.09550031  0.9670875  -0.05226345 -1.3915933 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 14 of episode: 42\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.11484206  1.1628199  -0.08009532 -1.7001494 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 15 of episode: 42\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.13809846  1.3587682  -0.1140983  -2.0166523 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 16 of episode: 42\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.16527383  1.5548745  -0.15443136 -2.3423715 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 17 of episode: 42\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.19637132  1.7510133  -0.20127879 -2.6783009 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 18 of episode: 42\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.23139158  1.5578665  -0.2548448  -2.4532068 ]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 43/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 43\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02727336 -0.20172954  0.03625453  0.32351476]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 43\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02323877 -0.00714208  0.04272482  0.04248186]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 43\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02309593 -0.20284982  0.04357446  0.34833285]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 43\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.01903893 -0.39856356  0.05054112  0.65443164]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 43\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.01106766 -0.20418042  0.06362975  0.378082  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 43\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.00698405 -0.4001456   0.07119139  0.69012946]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 43\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.00101886 -0.20607999  0.08499398  0.420681  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 43\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.00514046 -0.4022969   0.0934076   0.73890203]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 43\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.0131864  -0.59857607  0.10818564  1.0594592 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 43\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.02515792 -0.4050404   0.12937483  0.8025976 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 43\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.03325872 -0.21190739  0.14542678  0.5532493 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 43\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.03749687 -0.40873995  0.15649176  0.8879865 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 43\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.04567167 -0.21604845  0.1742515   0.64830226]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 14 of episode: 43\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.04999264 -0.41311464  0.18721753  0.9903962 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 15 of episode: 43\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.05825493 -0.6101816   0.20702547  1.3355523 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 16 of episode: 43\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.07045857 -0.41818047  0.2337365   1.114132  ]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 44/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 44\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02727336 -0.20172954  0.03625453  0.32351476]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 44\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02323877 -0.00714208  0.04272482  0.04248186]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 44\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02309593 -0.20284982  0.04357446  0.34833285]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 44\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.01903893 -0.00837385  0.05054112  0.06970263]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 44\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.01887146  0.18598844  0.05193517 -0.20661576]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 44\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02259123  0.38033074  0.04780285 -0.4824741 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 44\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03019784  0.57474655  0.03815337 -0.759716  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 44\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.04169277  0.37912026  0.02295905 -0.4552757 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 44\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.04927517  0.18368134  0.01385354 -0.15544522]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 44\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.0529488  -0.01163619  0.01074464  0.14157578]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 44\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.05271608  0.18333024  0.01357615 -0.14769813]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 44\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.05638269 -0.01198347  0.01062219  0.1492367 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 44\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.05614302  0.18298478  0.01360692 -0.14007631]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 14 of episode: 44\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.05980271 -0.01232939  0.0108054   0.15686813]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 15 of episode: 44\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.05955612  0.1826362   0.01394276 -0.13238646]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 16 of episode: 44\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.06320885 -0.01268266  0.01129503  0.1646624 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 17 of episode: 44\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.06295519 -0.20796447  0.01458828  0.4608871 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 18 of episode: 44\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.0587959  -0.01305171  0.02380602  0.17283787]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 19 of episode: 44\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.05853487 -0.20850615  0.02726278  0.47293475]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 20 of episode: 44\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.05436475 -0.4040023   0.03672147  0.7740843 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 21 of episode: 44\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.0462847  -0.5996097   0.05220316  1.0780913 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 22 of episode: 44\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03429251 -0.7953808   0.07376498  1.3866886 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 23 of episode: 44\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.01838489 -0.99134064  0.10149875  1.7014964 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 24 of episode: 44\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-1.4419219e-03 -1.1874747e+00  1.3552868e-01  2.0239727e+00]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 25 of episode: 44\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.02519142 -0.9939912   0.17600814  1.7761323 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 26 of episode: 44\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.04507124 -0.8012352   0.21153079  1.5429449 ]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 45/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 45\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02727336 -0.20172954  0.03625453  0.32351476]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 45\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02323877 -0.39734846  0.04272482  0.62740684]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 45\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.0152918  -0.5930399   0.05527296  0.9332334 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 45\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.003431   -0.39870548  0.07393762  0.6584189 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 45\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.00454311 -0.20468625  0.087106    0.38990307]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 45\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.00863683 -0.40092954  0.09490407  0.70872873]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 45\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.01665542 -0.597229    0.10907864  1.0297128 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 45\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.0286     -0.4037142   0.1296729   0.77317137]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 45\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.03667429 -0.6003591   0.14513633  1.1036793 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 45\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.04868147 -0.40741292  0.16720991  0.85982335]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 45\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.05682972 -0.6043692   0.18440637  1.2000684 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 45\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.06891711 -0.8013341   0.20840774  1.5444151 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 45\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.08494379 -0.60923547  0.23929605  1.3233315 ]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 46/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 46\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02727336 -0.20172954  0.03625453  0.32351476]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 46\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02323877 -0.39734846  0.04272482  0.62740684]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 46\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.0152918  -0.20284806  0.05527296  0.34847975]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 46\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.01123484 -0.39871082  0.06224256  0.65806717]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 46\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.00326062 -0.20450792  0.0754039   0.3856144 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 46\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.00082953 -0.01053289  0.08311619  0.11762639]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 46\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.00104019 -0.20674129  0.08546872  0.43533102]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 46\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.00517502 -0.40296263  0.09417533  0.75368583]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 46\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.01323427 -0.20925653  0.10924905  0.4920609 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 46\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.0174194  -0.01583143  0.11909027  0.23570833]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 46\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.01773603 -0.2124357   0.12380444  0.5634558 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 46\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.02198474 -0.40905747  0.13507356  0.8924366 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 46\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.03016589 -0.6057275   0.15292229  1.2243456 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 14 of episode: 46\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.04228044 -0.4128693   0.1774092   0.9832184 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 15 of episode: 46\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.05053783 -0.6098671   0.19707356  1.3259622 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 16 of episode: 46\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.06273517 -0.80685437  0.2235928   1.673289  ]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 47/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 47\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02727336 -0.20172954  0.03625453  0.32351476]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 47\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02323877 -0.00714208  0.04272482  0.04248186]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 47\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02309593 -0.20284982  0.04357446  0.34833285]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 47\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.01903893 -0.00837385  0.05054112  0.06970263]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 47\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.01887146 -0.2041826   0.05193517  0.37789345]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 47\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.0147878  -0.00983522  0.05949304  0.10202754]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 47\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.0145911  -0.2057571   0.06153359  0.41287106]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 47\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.01047596 -0.0115589   0.06979101  0.14020455]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 47\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.01024478  0.18249768  0.0725951  -0.1296698 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 47\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.01389473  0.37650868  0.07000171 -0.39859557]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 47\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02142491  0.57057136  0.06202979 -0.668413  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 47\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03283633  0.7647784   0.04866153 -0.94093823]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 47\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.0481319   0.95921195  0.02984277 -1.2179425 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 14 of episode: 47\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.06731614  0.7637181   0.00548392 -0.91605985]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 15 of episode: 47\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.08259051  0.56852245 -0.01283727 -0.6216585 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 16 of episode: 47\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.09396096  0.37358212 -0.02527044 -0.3330461 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 17 of episode: 47\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.10143259  0.17882879 -0.03193137 -0.0484381 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 18 of episode: 47\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.10500917  0.37439373 -0.03290013 -0.35102218]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 19 of episode: 47\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.11249705  0.17975472 -0.03992057 -0.06889257]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 20 of episode: 47\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.11609214  0.37542558 -0.04129842 -0.37389857]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 21 of episode: 47\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.12360065  0.5711091  -0.0487764  -0.6793119 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 22 of episode: 47\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.13502283  0.7668735  -0.06236263 -0.9869437 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 23 of episode: 47\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.1503603   0.5726395  -0.08210151 -0.71448195]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 24 of episode: 47\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.1618131   0.7687963  -0.09639114 -1.0318367 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 25 of episode: 47\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.17718902  0.5750795  -0.11702788 -0.7709055 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 26 of episode: 47\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.18869062  0.7716008  -0.13244599 -1.0979989 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 27 of episode: 47\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.20412263  0.57844734 -0.15440597 -0.84963155]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 28 of episode: 47\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.21569157  0.77529967 -0.1713986  -1.1866122 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 29 of episode: 47\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.23119757  0.5827635  -0.19513084 -0.9521847 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 30 of episode: 47\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.24285284  0.7798996  -0.21417454 -1.2992822 ]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 48/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 48\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02727336  0.18847767  0.03625453 -0.26141977]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 48\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03104291  0.38306385  0.03102613 -0.5424507 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 48\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03870419  0.5777363   0.02017712 -0.8251987 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 48\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.05025892  0.7725766   0.00367314 -1.111468  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 48\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.06571045  0.96765006 -0.01855621 -1.4029963 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 48\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.08506345  0.7727635  -0.04661614 -1.116172  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 48\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.10051872  0.57828337 -0.06893958 -0.83846885]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 48\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.11208439  0.7742756  -0.08570895 -1.152011  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 48\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.1275699   0.9704047  -0.10874917 -1.4702923 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 48\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.14697799  1.1666759  -0.13815503 -1.7948693 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 48\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.17031151  1.3630488  -0.1740524  -2.1271105 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 48\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.19757248  1.170032   -0.21659462 -1.8928732 ]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 49/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 49\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02727336  0.18847767  0.03625453 -0.26141977]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 49\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03104291  0.38306385  0.03102613 -0.5424507 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 49\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03870419  0.1875199   0.02017712 -0.24015574]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 49\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.04245459 -0.0078844   0.015374    0.0588227 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 49\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.0422969  -0.20322338  0.01655046  0.3563163 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 49\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03823243 -0.00834059  0.02367678  0.06889778]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 49\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03806562 -0.20379385  0.02505474  0.36895582]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 49\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03398975 -0.39926267  0.03243386  0.66943234]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 49\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02600449 -0.5948202   0.0458225   0.9721483 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 49\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.01410809 -0.79052615  0.06526547  1.278866  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 49\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.00170243 -0.5962939   0.09084279  1.0073125 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 49\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.01362831 -0.7925036   0.11098904  1.3270845 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 49\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.02947838 -0.98883766  0.13753073  1.6523374 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 14 of episode: 49\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.04925514 -0.795564    0.17057748  1.405471  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 15 of episode: 49\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.06516641 -0.60292023  0.1986869   1.1706061 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 16 of episode: 49\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.07722482 -0.79999155  0.22209902  1.5184278 ]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 50/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 50\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02727336  0.18847767  0.03625453 -0.26141977]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 50\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03104291  0.38306385  0.03102613 -0.5424507 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 50\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03870419  0.5777363   0.02017712 -0.8251987 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 50\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.05025892  0.3823443   0.00367314 -0.5262386 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 50\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.0579058   0.18717085 -0.00685163 -0.23240055]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 50\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.06164922 -0.00785253 -0.01149964  0.0581133 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 50\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.06149217  0.1874324  -0.01033737 -0.23817556]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 50\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.06524082  0.3827005  -0.01510088 -0.5341012 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 50\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.07289483  0.57803154 -0.02578291 -0.8315038 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 50\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.08445546  0.38327125 -0.04241299 -0.5470398 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 50\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.09212089  0.5789626  -0.05335378 -0.85277855]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 50\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.10370014  0.384607   -0.07040935 -0.5773386 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 50\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.11139227  0.19053888 -0.08195613 -0.3076409 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 14 of episode: 50\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.11520305 -0.00332545 -0.08810894 -0.04188862]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 15 of episode: 50\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.11513654  0.19294223 -0.08894671 -0.36101866]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 16 of episode: 50\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.11899539 -0.00081023 -0.09616709 -0.09765472]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 17 of episode: 50\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.11897919 -0.19443177 -0.09812018  0.16320609]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 18 of episode: 50\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.11509055 -0.38802215 -0.09485606  0.42339388]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 19 of episode: 50\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.10733011 -0.5816813  -0.08638819  0.6847308 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 20 of episode: 50\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.09569648 -0.7755044  -0.07269356  0.9490147 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 21 of episode: 50\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.08018639 -0.96957636 -0.05371327  1.2180008 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 22 of episode: 50\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.06079486 -1.1639662  -0.02935326  1.4933809 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 23 of episode: 50\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 3.7515540e-02 -1.3587190e+00  5.1436265e-04  1.7767556e+00]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 24 of episode: 50\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.01034116 -1.5538467   0.03604947  2.0695984 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 25 of episode: 50\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.02073577 -1.7493161   0.07744145  2.373208  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 26 of episode: 50\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.0557221  -1.5549604   0.12490561  2.1052933 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 27 of episode: 50\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.0868213  -1.7510936   0.16701147  2.433828  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 28 of episode: 50\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.12184317 -1.5577527   0.21568803  2.1967256 ]\n",
      "Reward: 1.0\n",
      "\n",
      "****************************************************\n",
      "Episode: 50, Return: 28.0\n",
      "****************************************************\n",
      "-----------------------------------\n",
      "Episode 51/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 51\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02727336 -0.20172954  0.03625453  0.32351476]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 51\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02323877 -0.00714208  0.04272482  0.04248186]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 51\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02309593  0.187342    0.04357446 -0.23642075]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 51\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02684277  0.3818152   0.03884605 -0.51504683]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 51\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03447907  0.18616833  0.02854511 -0.21038006]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 51\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03820244 -0.00934989  0.02433751  0.09116892]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 51\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03801544  0.18541493  0.02616089 -0.19373725]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 51\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.04172374  0.38015306  0.02228614 -0.4780541 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 51\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.0493268   0.5749534   0.01272506 -0.76363045]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 51\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.06082587  0.7698978  -0.00254755 -1.0522823 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 51\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.07622383  0.96505344 -0.0235932  -1.3457638 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 51\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.09552489  1.160464   -0.05050847 -1.6457338 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 51\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.11873417  0.9659681  -0.08342315 -1.3692051 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 14 of episode: 51\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.13805354  1.1620289  -0.11080725 -1.6867715 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 15 of episode: 51\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.16129412  0.9683494  -0.14454268 -1.4305447 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 16 of episode: 51\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.1806611   1.1649295  -0.17315358 -1.7646878 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 17 of episode: 51\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.20395969  0.97213686 -0.20844734 -1.5304763 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 18 of episode: 51\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.22340243  1.1690724  -0.23905686 -1.8803258 ]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 52/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 52\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02727336 -0.20172954  0.03625453  0.32351476]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 52\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02323877 -0.39734846  0.04272482  0.62740684]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 52\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.0152918  -0.20284806  0.05527296  0.34847975]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 52\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.01123484 -0.39871082  0.06224256  0.65806717]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 52\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.00326062 -0.20450792  0.0754039   0.3856144 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 52\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.00082953 -0.01053289  0.08311619  0.11762639]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 52\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.00104019 -0.20674129  0.08546872  0.43533102]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 52\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.00517502 -0.40296263  0.09417533  0.75368583]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 52\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.01323427 -0.20925653  0.10924905  0.4920609 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 52\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.0174194  -0.40573618  0.11909027  0.81707865]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 52\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.02553413 -0.21242832  0.13543184  0.5641005 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 52\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.02978269 -0.40916455  0.14671385  0.896198  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 52\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.03796598 -0.21630366  0.16463782  0.65299386]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 14 of episode: 52\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.04229205 -0.41328898  0.17769769  0.9926614 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 15 of episode: 52\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.05055783 -0.220932    0.19755092  0.76063806]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 16 of episode: 52\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.05497647 -0.02900028  0.21276368  0.536043  ]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 53/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 53\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02727336  0.18847767  0.03625453 -0.26141977]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 53\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03104291  0.38306385  0.03102613 -0.5424507 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 53\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03870419  0.5777363   0.02017712 -0.8251987 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 53\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.05025892  0.3823443   0.00367314 -0.5262386 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 53\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.0579058   0.5774144  -0.00685163 -0.8177619 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 53\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.06945409  0.77262944 -0.02320687 -1.112592  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 53\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.08490668  0.9680484  -0.0454587  -1.4124637 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 53\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.10426765  1.1637033  -0.07370798 -1.7190032 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 53\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.12754172  1.3595885  -0.10808804 -2.0336838 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 53\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.15473348  1.5556463  -0.14876172 -2.3577702 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 53\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.18584642  1.3621337  -0.19591711 -2.114282  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 53\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.2130891   1.5586003  -0.23820277 -2.460576  ]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 54/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 54\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02727336 -0.20172954  0.03625453  0.32351476]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 54\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02323877 -0.39734846  0.04272482  0.62740684]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 54\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.0152918  -0.20284806  0.05527296  0.34847975]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 54\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.01123484 -0.00855404  0.06224256  0.07372576]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 54\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.01106376 -0.20451054  0.06371707  0.3853788 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 54\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.00697355 -0.40047646  0.07142465  0.69745135]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 54\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.00103598 -0.2064138   0.08537367  0.42808053]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 54\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.00516426 -0.01259816  0.09393528  0.16348532]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 54\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.00541622  0.18106231  0.09720499 -0.09814834]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 54\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.00179497  0.37466645  0.09524202 -0.35865033]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 54\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.00569836  0.17832868  0.08806901 -0.03751959]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 54\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.00926493  0.37208456  0.08731862 -0.3011682 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 54\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.01670662  0.56586045  0.08129526 -0.56508565]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 14 of episode: 54\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02802383  0.36969772  0.06999355 -0.24793877]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 15 of episode: 54\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [0.03541778 0.17364958 0.06503477 0.06597468]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 16 of episode: 54\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03889078 -0.02234159  0.06635427  0.3784467 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 17 of episode: 54\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03844395 -0.21834005  0.0739232   0.6912913 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 18 of episode: 54\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03407715 -0.02431741  0.08774903  0.42276576]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 19 of episode: 54\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03359079 -0.2205658   0.09620434  0.74177086]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 20 of episode: 54\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02917948 -0.41687486  0.11103976  1.0631133 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 21 of episode: 54\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02084198 -0.61327755  0.13230203  1.3884817 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 22 of episode: 54\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.00857643 -0.80977654  0.16007166  1.7194377 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 23 of episode: 54\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.0076191 -1.00633    0.1944604  2.0573592]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 24 of episode: 54\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.0277457 -0.8136556  0.2356076  1.8306065]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 55/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 55\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02727336 -0.20172954  0.03625453  0.32351476]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 55\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02323877 -0.00714208  0.04272482  0.04248186]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 55\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02309593 -0.20284982  0.04357446  0.34833285]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 55\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.01903893 -0.00837385  0.05054112  0.06970263]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 55\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.01887146 -0.2041826   0.05193517  0.37789345]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 55\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.0147878  -0.40000218  0.05949304  0.68648887]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 55\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.00678776 -0.20575435  0.07322282  0.41311327]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 55\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.00267267 -0.01174257  0.08148509  0.14438367]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 55\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.00243782 -0.2079312   0.08437276  0.4616203 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 55\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.0017208  -0.01409676  0.09360516  0.19667907]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 55\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.00200274  0.17957032  0.09753875 -0.06507005]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 55\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.00158867 -0.01680496  0.09623734  0.2567237 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 55\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.00125257 -0.21315975  0.10137182  0.5781432 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 14 of episode: 55\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.00301062 -0.01959373  0.11293468  0.319037  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 15 of episode: 55\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.0034025  -0.21612777  0.11931542  0.6450923 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 16 of episode: 55\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.00772505 -0.02285283  0.13221727  0.39223665]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 17 of episode: 55\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.00818211  0.17016928  0.140062    0.14398925]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 18 of episode: 55\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.00477873  0.36303648  0.14294179 -0.10143482]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 19 of episode: 55\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [0.002482   0.16618608 0.14091308 0.23271126]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 20 of episode: 55\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.00580573 -0.03063862  0.14556731  0.56631345]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 21 of episode: 55\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [0.00519295 0.16217358 0.15689358 0.32279977]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 22 of episode: 55\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [0.00843642 0.35475406 0.16334958 0.08341485]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 23 of episode: 55\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [0.01553151 0.1577133  0.16501787 0.42285302]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 24 of episode: 55\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [0.01868577 0.35016003 0.17347494 0.18639971]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 25 of episode: 55\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02568897  0.5424304   0.17720293 -0.04693095]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 26 of episode: 55\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [0.03653758 0.34526867 0.17626432 0.296006  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 27 of episode: 55\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [0.04344295 0.14812939 0.18218443 0.63868684]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 28 of episode: 55\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.04640554 -0.04900233  0.19495817  0.98275715]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 29 of episode: 55\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [0.0454255  0.14304909 0.2146133  0.75709265]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 56/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 56\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02727336  0.18847767  0.03625453 -0.26141977]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 56\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03104291 -0.00714255  0.03102613  0.04247424]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 56\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03090006 -0.20269535  0.03187562  0.3447825 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 56\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02684616 -0.3982559   0.03877127  0.6473442 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 56\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.01888104 -0.20369498  0.05171815  0.36711797]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 56\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.01480714 -0.39951223  0.05906051  0.67564946]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 56\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.00681689 -0.595403    0.0725735   0.986327  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 56\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.00509117 -0.40132406  0.09230004  0.7172928 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 56\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.01311765 -0.20759247  0.1066459   0.45503008]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 56\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.0172695  -0.0141273   0.1157465   0.19777551]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 56\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.01755204 -0.21069819  0.11970201  0.5246125 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 56\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.02176601 -0.01744618  0.13019426  0.27191713]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 56\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.02211493  0.17560092  0.1356326   0.0229663 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 14 of episode: 56\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.01860291  0.36854342  0.13609193 -0.22403564]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 15 of episode: 56\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.01123204  0.17176539  0.13161121  0.10828986]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 16 of episode: 56\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.00779674  0.36477983  0.13377701 -0.14014588]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 17 of episode: 56\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.00050114  0.16802081  0.1309741   0.1915689 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 18 of episode: 56\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.00285928  0.3610496   0.13480547 -0.05709798]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 19 of episode: 56\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.01008027  0.55400693  0.1336635  -0.30439526]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 20 of episode: 56\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02116041  0.746996    0.1275756  -0.5521166 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 21 of episode: 56\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03610033  0.9401172   0.11653327 -0.8020386 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 22 of episode: 56\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.05490267  1.1334647   0.1004925  -1.0559095 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 23 of episode: 56\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.07757197  1.3271216   0.07937431 -1.3154343 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 24 of episode: 56\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.1041144   1.1310902   0.05306562 -0.9990014 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 25 of episode: 56\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.12673621  0.9353005   0.0330856  -0.6901364 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 26 of episode: 56\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.14544222  0.7397355   0.01928287 -0.38722393]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 27 of episode: 56\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.16023692  0.5443452   0.01153839 -0.08852417]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 28 of episode: 56\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [0.17112383 0.34905976 0.00976791 0.20777668]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 29 of episode: 56\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.17810503  0.5440407   0.01392344 -0.08180904]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 30 of episode: 56\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.18898584  0.7389603   0.01228726 -0.37006676]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 31 of episode: 56\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.20376503  0.54366595  0.00488592 -0.07353499]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 32 of episode: 56\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [0.21463835 0.3484743  0.00341522 0.22068545]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 33 of episode: 56\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [0.22160785 0.15330368 0.00782893 0.5144437 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 34 of episode: 56\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [0.22467391 0.34831452 0.01811781 0.22423813]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 35 of episode: 56\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [0.2316402  0.15293837 0.02260257 0.5225806 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 36 of episode: 56\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.23469898 -0.0424943   0.03305418  0.8222993 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 37 of episode: 56\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [0.2338491  0.1521602  0.04950017 0.5401932 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 38 of episode: 56\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [0.2368923  0.34655267 0.06030403 0.2635088 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 39 of episode: 56\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [0.24382335 0.15062411 0.06557421 0.57458603]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 40 of episode: 56\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [0.24683583 0.34476843 0.07706593 0.30326045]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 41 of episode: 56\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [0.2537312  0.5387123  0.08313114 0.03584306]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 42 of episode: 56\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.26450545  0.73254985  0.083848   -0.22949672]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 43 of episode: 56\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [0.27915645 0.536336   0.07925807 0.08841246]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 44 of episode: 56\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.28988317  0.7302377   0.08102632 -0.17824954]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 45 of episode: 56\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [0.3044879  0.53405535 0.07746132 0.13885422]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 46 of episode: 56\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.31516904  0.7279873   0.08023841 -0.12842056]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 47 of episode: 56\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [0.32972878 0.5318131  0.07767    0.18845885]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 48 of episode: 56\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [0.34036502 0.33567077 0.08143917 0.5045973 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 49 of episode: 56\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [0.34707844 0.13950123 0.09153112 0.821793  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 50 of episode: 56\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.34986848 -0.05674591  0.10796698  1.1418041 ]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 57/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 57\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02727336 -0.20172954  0.03625453  0.32351476]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 57\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02323877 -0.00714208  0.04272482  0.04248186]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 57\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02309593 -0.20284982  0.04357446  0.34833285]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 57\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.01903893 -0.00837385  0.05054112  0.06970263]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 57\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.01887146  0.18598844  0.05193517 -0.20661576]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 57\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02259123 -0.00983622  0.04780285  0.10198726]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 57\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.0223945  -0.20560949  0.0498426   0.4093604 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 57\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.01828231 -0.01122833  0.05802981  0.13279842]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 57\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.01805774  0.18301645  0.06068578 -0.14102711]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 57\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02171807 -0.01291978  0.05786523  0.17016688]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 57\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02145968 -0.20882018  0.06126857  0.48052853]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 57\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.01728327 -0.01461419  0.07087914  0.20776783]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 57\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.01699099  0.17942642  0.0750345  -0.06174124]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 14 of episode: 57\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02057952  0.3733968   0.07379968 -0.32983872]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 15 of episode: 57\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02804746  0.56739485  0.0672029  -0.5983663 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 16 of episode: 57\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03939535  0.7615153   0.05523557 -0.86914694]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 17 of episode: 57\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.05462566  0.5656872   0.03785263 -0.5596217 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 18 of episode: 57\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.0659394   0.37005496  0.0266602  -0.25525755]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 19 of episode: 57\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [0.07334051 0.17456271 0.02155505 0.0457138 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 20 of episode: 57\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.07683176  0.36936906  0.02246932 -0.24009113]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 21 of episode: 57\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [0.08421914 0.17393345 0.0176675  0.0595937 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 22 of episode: 57\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.0876978   0.3687977   0.01885938 -0.22746307]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 23 of episode: 57\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [0.09507376 0.17341135 0.01431012 0.07110862]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 24 of episode: 57\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.09854199 -0.0219128   0.01573229  0.3682719 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 25 of episode: 57\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.09810373 -0.21725471  0.02309773  0.6658736 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 26 of episode: 57\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.09375864 -0.02246151  0.0364152   0.38055187]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 27 of episode: 57\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.09330941 -0.21808113  0.04402624  0.6844905 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 28 of episode: 57\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.08894778 -0.41378582  0.05771605  0.99070257]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 29 of episode: 57\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.08067207 -0.60963076  0.07753009  1.3009399 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 30 of episode: 57\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.06847946 -0.80564624  0.10354889  1.6168509 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 31 of episode: 57\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.05236653 -1.0018256   0.13588591  1.9399328 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 32 of episode: 57\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03233002 -0.8083909   0.17468457  1.6922832 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 33 of episode: 57\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.0161622  -1.0050472   0.20853023  2.0338748 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 34 of episode: 57\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.00393875 -0.812598    0.24920772  1.8123192 ]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 58/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 58\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02727336 -0.20172954  0.03625453  0.32351476]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 58\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02323877 -0.39734846  0.04272482  0.62740684]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 58\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.0152918  -0.20284806  0.05527296  0.34847975]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 58\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.01123484 -0.00855404  0.06224256  0.07372576]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 58\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.01106376 -0.20451054  0.06371707  0.3853788 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 58\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.00697355 -0.01034828  0.07142465  0.11344658]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 58\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.00676658  0.18368141  0.07369358 -0.1558749 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 58\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.01044021 -0.01241407  0.07057608  0.15911628]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 58\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.01019193 -0.20847175  0.07375841  0.47320282]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 58\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.0060225  -0.40455365  0.08322246  0.7881913 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 58\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.00206858 -0.21066739  0.09898629  0.52280766]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 58\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.00628193 -0.407033    0.10944244  0.8449687 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 58\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.01442259 -0.6034645   0.12634182  1.1699649 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 14 of episode: 58\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.02649187 -0.4101915   0.14974111  0.91941196]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 15 of episode: 58\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.0346957  -0.21737605  0.16812935  0.67728484]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 16 of episode: 58\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.03904323 -0.4143855   0.18167505  1.0178295 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 17 of episode: 58\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.04733093 -0.61140275  0.20203164  1.361611  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 18 of episode: 58\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.05955899 -0.41930306  0.22926386  1.1383162 ]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 59/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 59\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02727336 -0.20172954  0.03625453  0.32351476]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 59\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02323877 -0.00714208  0.04272482  0.04248186]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 59\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02309593  0.187342    0.04357446 -0.23642075]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 59\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02684277  0.3818152   0.03884605 -0.51504683]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 59\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03447907  0.18616833  0.02854511 -0.21038006]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 59\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03820244  0.38087076  0.02433751 -0.4939236 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 59\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.04581986  0.18541417  0.01445904 -0.19367102]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 59\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.04952814 -0.0099116   0.01058562  0.10353782]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 59\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.04932991  0.18505706  0.01265637 -0.18578666]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 59\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.05303105 -0.01024367  0.00894064  0.11086183]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 59\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.05282617  0.18474904  0.01115788 -0.17898703]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 59\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.05652115 -0.01053079  0.00757814  0.11719484]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 59\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.05631054 -0.20576051  0.00992203  0.41225895]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 14 of episode: 59\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.05219533 -0.4010217   0.01816721  0.70805335]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 15 of episode: 59\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.04417489 -0.59639055  0.03232828  1.006399  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 16 of episode: 59\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03224709 -0.7919289   0.05245626  1.3090563 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 17 of episode: 59\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.01640851 -0.9876747   0.07863738  1.6176862 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 18 of episode: 59\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.00334499 -1.1836308   0.11099111  1.9338075 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 19 of episode: 59\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.0270176  -0.9898574   0.14966726  1.6775004 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 20 of episode: 59\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.04681475 -1.1863649   0.18321727  2.0128043 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 21 of episode: 59\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.07054204 -1.3828586   0.22347336  2.3561769 ]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 60/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 60\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02727336  0.18847767  0.03625453 -0.26141977]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 60\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03104291 -0.00714255  0.03102613  0.04247424]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 60\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03090006 -0.20269535  0.03187562  0.3447825 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 60\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02684616 -0.008041    0.03877127  0.06231914]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 60\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02668534 -0.20369677  0.04001765  0.36697814]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 60\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.0226114  -0.00916565  0.04735721  0.08717711]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 60\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02242809  0.18524662  0.04910076 -0.19019653]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 60\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02613302 -0.01054214  0.04529683  0.11756248]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 60\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02592218  0.18390252  0.04764808 -0.16049261]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 60\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02960023  0.37831107  0.04443822 -0.43777126]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 60\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03716645  0.5727768   0.0356828  -0.7161213 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 60\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.04862199  0.76738715  0.02136037 -0.99736255]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 60\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.06396973  0.9622171   0.00141312 -1.2832613 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 14 of episode: 60\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.08321407  1.157321   -0.02425211 -1.5755014 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 15 of episode: 60\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.10636049  1.3527235  -0.05576213 -1.8756484 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 16 of episode: 60\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.13341495  1.5484078  -0.0932751  -2.1851041 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 17 of episode: 60\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.16438311  1.7443014  -0.13697718 -2.5050504 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 18 of episode: 60\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.19926915  1.5505478  -0.1870782  -2.2572877 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 19 of episode: 60\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.2302801   1.3576105  -0.23222394 -2.027612  ]\n",
      "Reward: 1.0\n",
      "\n",
      "****************************************************\n",
      "Episode: 60, Return: 19.0\n",
      "****************************************************\n",
      "-----------------------------------\n",
      "Episode 61/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 61\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02727336  0.18847767  0.03625453 -0.26141977]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 61\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03104291  0.38306385  0.03102613 -0.5424507 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 61\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03870419  0.5777363   0.02017712 -0.8251987 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 61\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.05025892  0.3823443   0.00367314 -0.5262386 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 61\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.0579058   0.18717085 -0.00685163 -0.23240055]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 61\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.06164922  0.38239002 -0.01149964 -0.5272368 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 61\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.06929702  0.5776719  -0.02204438 -0.823521  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 61\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.08085046  0.3828583  -0.0385148  -0.5378522 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 61\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.08850762  0.5785     -0.04927184 -0.84241766]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 61\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.10007763  0.38408393 -0.06612019 -0.5656275 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 61\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.1077593   0.18994887 -0.07743274 -0.29448643]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 61\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.11155828 -0.00398865 -0.08332247 -0.02719431]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 61\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.11147851 -0.19782297 -0.08386636  0.23808   ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 14 of episode: 61\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.10752205 -0.3916529  -0.07910476  0.5031752 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 15 of episode: 61\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.09968899 -0.585576   -0.06904125  0.76991767]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 16 of episode: 61\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.08797747 -0.3895752  -0.0536429   0.45633486]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 17 of episode: 61\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.08018596 -0.5838993  -0.0445162   0.7316383 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 18 of episode: 61\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.06850798 -0.7783788  -0.02988344  1.009985  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 19 of episode: 61\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.05294041 -0.582871   -0.00968374  0.7080699 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 20 of episode: 61\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.04128299 -0.7778575   0.00447766  0.99768895]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 21 of episode: 61\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02572584 -0.5827957   0.02443144  0.7064156 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 22 of episode: 61\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.01406992 -0.7782474   0.03855975  1.0066879 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 23 of episode: 61\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.00149503 -0.583661    0.05869351  0.7263589 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 24 of episode: 61\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.01316825 -0.38939756  0.07322069  0.45271155]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 25 of episode: 61\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.0209562  -0.58547443  0.08227492  0.7675465 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 26 of episode: 61\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.03266569 -0.39157572  0.09762584  0.50184375]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 27 of episode: 61\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.0404972  -0.19795564  0.10766272  0.24145298]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 28 of episode: 61\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.04445631 -0.00452314  0.11249178 -0.01542408]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 29 of episode: 61\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.04454678  0.18882085  0.1121833  -0.27060413]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 30 of episode: 61\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.04077036 -0.00770829  0.10677122  0.05524928]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 31 of episode: 61\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.04092453 -0.2041863   0.1078762   0.37961912]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 32 of episode: 61\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.04500825 -0.4006616   0.11546858  0.704273  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 33 of episode: 61\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.05302148 -0.5971782   0.12955405  1.0309575 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 34 of episode: 61\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.06496505 -0.7937634   0.15017319  1.3613464 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 35 of episode: 61\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.08084032 -0.60080844  0.17740013  1.1191566 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 36 of episode: 61\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.09285649 -0.40840042  0.19978325  0.88695663]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 37 of episode: 61\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.10102449 -0.21646883  0.21752238  0.66313195]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 62/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 62\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02727336 -0.20172954  0.03625453  0.32351476]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 62\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02323877 -0.39734846  0.04272482  0.62740684]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 62\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.0152918  -0.20284806  0.05527296  0.34847975]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 62\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.01123484 -0.39871082  0.06224256  0.65806717]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 62\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.00326062 -0.20450792  0.0754039   0.3856144 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 62\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.00082953 -0.4006148   0.08311619  0.70108664]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 62\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.00884183 -0.59678453  0.09713792  1.0187335 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 62\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.02077752 -0.40308207  0.11751259  0.75806314]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 62\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.02883916 -0.20975865  0.13267384  0.5045472 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 62\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.03303434 -0.40647635  0.14276479  0.8359223 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 62\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.04116386 -0.2135629   0.15948324  0.5913265 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 62\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.04543512 -0.41051587  0.17130977  0.9296963 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 62\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.05364544 -0.21806844  0.18990369  0.6953697 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 14 of episode: 62\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.05800681 -0.41524574  0.2038111   1.0413152 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 15 of episode: 62\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.06631172 -0.61240524  0.22463739  1.3904399 ]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 63/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 63\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02727336  0.18847767  0.03625453 -0.26141977]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 63\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03104291 -0.00714255  0.03102613  0.04247424]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 63\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03090006 -0.20269535  0.03187562  0.3447825 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 63\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02684616 -0.008041    0.03877127  0.06231914]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 63\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02668534 -0.20369677  0.04001765  0.36697814]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 63\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.0226114  -0.00916565  0.04735721  0.08717711]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 63\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02242809  0.18524662  0.04910076 -0.19019653]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 63\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02613302  0.37963298  0.04529683 -0.46699485]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 63\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03372568  0.57408667  0.03595693 -0.7450635 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 63\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.04520741  0.76869446  0.02105566 -1.0262175 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 63\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 6.0581304e-02  9.6352983e-01  5.3131033e-04 -1.3122158e+00]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 63\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.0798519   1.158645   -0.02571301 -1.6047324 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 63\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.1030248   0.96383655 -0.05780765 -1.3201749 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 14 of episode: 63\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.12230153  1.1596396  -0.08421116 -1.630375  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 15 of episode: 63\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.14549433  1.355644   -0.11681865 -1.9480685 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 16 of episode: 63\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.1726072   1.5517995  -0.15578002 -2.274563  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 17 of episode: 63\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.20364319  1.3584332  -0.20127128 -2.0336401 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 18 of episode: 63\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.23081186  1.554983   -0.24194409 -2.3812883 ]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 64/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 64\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02727336  0.18847767  0.03625453 -0.26141977]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 64\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03104291 -0.00714255  0.03102613  0.04247424]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 64\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03090006 -0.20269535  0.03187562  0.3447825 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 64\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02684616 -0.008041    0.03877127  0.06231914]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 64\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02668534 -0.20369677  0.04001765  0.36697814]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 64\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.0226114  -0.39936385  0.04735721  0.67200583]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 64\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.01462412 -0.595111    0.06079733  0.9792152 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 64\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.0027219  -0.40085447  0.08038163  0.70623213]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 64\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.00529519 -0.20693277  0.09450628  0.43989557]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 64\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.00943384 -0.40325627  0.10330418  0.76081026]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 64\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.01749897 -0.5996382   0.11852039  1.0841302 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 64\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.02949173 -0.4062624   0.140203    0.8308648 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 64\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.03761698 -0.2133065   0.1568203   0.58535576]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 14 of episode: 64\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.04188311 -0.02068846  0.16852741  0.34589058]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 15 of episode: 64\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.04229688  0.17168562  0.17544521  0.11073039]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 16 of episode: 64\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.03886316 -0.02545957  0.17765982  0.45322528]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 17 of episode: 64\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.03937235 -0.22259024  0.18672433  0.79622465]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 18 of episode: 64\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.04382416 -0.41971636  0.20264882  1.1413525 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 19 of episode: 64\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.05221849 -0.22773501  0.22547588  0.91844517]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 65/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 65\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02727336 -0.20172954  0.03625453  0.32351476]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 65\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02323877 -0.00714208  0.04272482  0.04248186]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 65\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02309593 -0.20284982  0.04357446  0.34833285]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 65\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.01903893 -0.00837385  0.05054112  0.06970263]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 65\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.01887146  0.18598844  0.05193517 -0.20661576]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 65\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02259123  0.38033074  0.04780285 -0.4824741 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 65\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03019784  0.18456784  0.03815337 -0.17511648]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 65\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.0338892  -0.01107878  0.03465104  0.12935425]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 65\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03366762  0.18353009  0.03723813 -0.15219845]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 65\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03733822 -0.01210473  0.03419416  0.15199582]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 65\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03709613 -0.2076992   0.03723408  0.45526713]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 65\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03294215 -0.4033273   0.04633942  0.7594502 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 65\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.0248756  -0.20887342  0.06152842  0.4817014 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 14 of episode: 65\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02069813 -0.40480742  0.07116245  0.7931242 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 15 of episode: 65\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.01260198 -0.21073076  0.08702493  0.5236501 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 16 of episode: 65\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.00838737 -0.01693446  0.09749794  0.2596088 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 17 of episode: 65\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.00804868 -0.21330327  0.10269012  0.58138216]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 18 of episode: 65\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.00378261 -0.01975873  0.11431776  0.32273257]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 19 of episode: 65\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [0.00338744 0.17356552 0.12077241 0.06817526]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 20 of episode: 65\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.00685875  0.36676756  0.12213591 -0.18409601]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 21 of episode: 65\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [0.0141941  0.17012896 0.11845399 0.1444834 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 22 of episode: 65\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.01759668  0.3633729   0.12134366 -0.10860717]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 23 of episode: 65\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [0.02486414 0.16673993 0.11917152 0.21976101]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 24 of episode: 65\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02819894  0.35997468  0.12356674 -0.03308176]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 25 of episode: 65\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [0.03539843 0.1633173  0.12290511 0.29589137]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 26 of episode: 65\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [0.03866477 0.35649234 0.12882292 0.04435779]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 27 of episode: 65\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.04579462  0.5495543   0.12971008 -0.20506619]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 28 of episode: 65\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [0.05678571 0.352839   0.12560876 0.12555584]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 29 of episode: 65\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.06384248  0.5459586   0.12811987 -0.12500936]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 30 of episode: 65\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.07476166  0.7390345   0.1256197  -0.3746852 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 31 of episode: 65\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.08954234  0.5423729   0.11812599 -0.04518216]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 32 of episode: 65\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [0.1003898  0.34577256 0.11722235 0.28231153]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 33 of episode: 65\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [0.10730525 0.14919068 0.12286858 0.60954523]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 34 of episode: 65\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.11028907 -0.0474152   0.13505948  0.9382633 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 35 of episode: 65\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [0.10934076 0.14565279 0.15382475 0.69088554]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 36 of episode: 65\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.11225382 -0.05123105  0.16764246  1.0277705 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 37 of episode: 65\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [0.1112292  0.14131117 0.18819787 0.79206246]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 38 of episode: 65\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [0.11405542 0.33341986 0.20403911 0.5639916 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 39 of episode: 65\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [0.12072382 0.5251829  0.21531895 0.341886  ]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 66/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 66\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02727336  0.18847767  0.03625453 -0.26141977]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 66\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03104291 -0.00714255  0.03102613  0.04247424]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 66\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03090006 -0.20269535  0.03187562  0.3447825 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 66\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02684616 -0.3982559   0.03877127  0.6473442 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 66\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.01888104 -0.593896    0.05171815  0.95197964]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 66\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.00700312 -0.39950672  0.07075775  0.67598397]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 66\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-9.870153e-04 -5.955368e-01  8.427742e-02  9.900788e-01]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 66\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.01289775 -0.79167956  0.104079    1.3079969 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 66\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.02873134 -0.598019    0.13023894  1.049622  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 66\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.04069172 -0.40484264  0.15123138  0.8004936 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 66\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.04878858 -0.6016795   0.16724125  1.1366717 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 66\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.06082217 -0.4090922   0.18997468  0.9007613 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 66\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.06900401 -0.21698144  0.2079899   0.67329675]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 14 of episode: 66\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.07334364 -0.41429406  0.22145584  1.023596  ]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 67/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 67\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02727336  0.18847767  0.03625453 -0.26141977]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 67\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03104291 -0.00714255  0.03102613  0.04247424]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 67\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03090006 -0.20269535  0.03187562  0.3447825 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 67\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02684616 -0.3982559   0.03877127  0.6473442 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 67\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.01888104 -0.20369498  0.05171815  0.36711797]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 67\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.01480714 -0.39951223  0.05906051  0.67564946]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 67\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.00681689 -0.595403    0.0725735   0.986327  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 67\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.00509117 -0.7914179   0.09230004  1.3008933 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 67\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.02091952 -0.9875819   0.1183179   1.6209846 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 67\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.04067116 -1.1838822   0.1507376   1.9480807 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 67\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.06434881 -1.3802533   0.18969922  2.2834466 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 67\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.09195387 -1.1873299   0.23536815  2.0546904 ]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 68/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 68\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02727336 -0.20172954  0.03625453  0.32351476]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 68\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02323877 -0.39734846  0.04272482  0.62740684]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 68\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.0152918  -0.5930399   0.05527296  0.9332334 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 68\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.003431   -0.39870548  0.07393762  0.6584189 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 68\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.00454311 -0.20468625  0.087106    0.38990307]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 68\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.00863683 -0.01090164  0.09490407  0.12590498]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 68\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.00885486  0.18274164  0.09742217 -0.13539292]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 68\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.00520003 -0.01363098  0.09471431  0.18636611]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 68\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.00547265  0.1800172   0.09844163 -0.07499988]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 68\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.00187231 -0.01636809  0.09694164  0.247047  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 68\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.00219967 -0.21273127  0.10188257  0.568665  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 68\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.00645429 -0.01917478  0.11325587  0.30973756]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 68\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.00683779  0.17416665  0.11945062  0.05480947]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 14 of episode: 68\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.00335446 -0.0224475   0.12054681  0.3826642 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 15 of episode: 68\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.00380341  0.17077506  0.1282001   0.13028866]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 16 of episode: 68\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.0003879   0.3638497   0.13080586 -0.11935897]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 17 of episode: 68\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.00688909  0.55687857  0.12841868 -0.36808136]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 18 of episode: 68\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.01802666  0.36018804  0.12105706 -0.03782361]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 19 of episode: 68\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [0.02523042 0.16355686 0.12030059 0.2904685 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 20 of episode: 68\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02850156 -0.0330567   0.12610996  0.61854047]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 21 of episode: 68\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [0.02784042 0.16009913 0.13848077 0.36808574]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 22 of episode: 68\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [0.03104241 0.35300964 0.1458425  0.12207346]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 23 of episode: 68\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.0381026   0.54577374  0.14828396 -0.12127723]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 24 of episode: 68\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.04901808  0.73849446  0.1458584  -0.36375004]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 25 of episode: 68\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.06378797  0.54163325  0.1385834  -0.02886332]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 26 of episode: 68\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.07462063  0.73452383  0.13800615 -0.274812  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 27 of episode: 68\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.08931111  0.9274346   0.1325099  -0.52098185]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 28 of episode: 68\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.1078598   1.1204668   0.12209027 -0.7691477 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 29 of episode: 68\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.13026914  0.9238949   0.10670731 -0.44067922]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 30 of episode: 68\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.14874703  0.7274374   0.09789373 -0.11635672]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 31 of episode: 68\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [0.16329578 0.531059   0.09556659 0.20553535]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 32 of episode: 68\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [0.17391697 0.33470958 0.0996773  0.5267694 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 33 of episode: 68\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [0.18061115 0.52829814 0.11021269 0.26708457]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 34 of episode: 68\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [0.19117711 0.7216887  0.11555438 0.01109572]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 35 of episode: 68\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [0.20561089 0.5251155  0.11577629 0.3378866 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 36 of episode: 68\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [0.2161132  0.7184159  0.12253403 0.08383923]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 37 of episode: 68\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.23048152  0.9115878   0.12421081 -0.16781104]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 38 of episode: 68\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.24871327  1.1047331   0.12085459 -0.41887283]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 39 of episode: 68\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.27080795  0.9081245   0.11247713 -0.09066619]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 40 of episode: 68\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [0.28897044 0.7115851  0.11066381 0.23527864]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 41 of episode: 68\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.30320212  0.9049663   0.11536939 -0.02054997]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 42 of episode: 68\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.32130146  1.098261    0.11495838 -0.27472112]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 43 of episode: 68\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [0.34326667 0.90170246 0.10946396 0.05189389]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 44 of episode: 68\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.36130074  1.0950985   0.11050184 -0.20434566]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 45 of episode: 68\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [0.3832027  0.898584   0.10641493 0.12104969]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 46 of episode: 68\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.40117437  1.0920331   0.10883592 -0.1362556 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 47 of episode: 68\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [0.42301503 0.8955342  0.10611081 0.18868344]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 48 of episode: 68\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [0.44092572 0.6990668  0.10988448 0.51286507]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 49 of episode: 68\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [0.45490706 0.8924835  0.12014178 0.25673097]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 50 of episode: 68\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [0.4727567  0.6958694  0.1252764  0.58476293]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 69/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 69\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02727336 -0.20172954  0.03625453  0.32351476]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 69\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02323877 -0.00714208  0.04272482  0.04248186]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 69\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02309593  0.187342    0.04357446 -0.23642075]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 69\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02684277 -0.00837454  0.03884605  0.06968222]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 69\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02667528 -0.20403126  0.04023969  0.37436375]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 69\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02259465 -0.399701    0.04772697  0.6794581 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 69\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.01460063 -0.20527339  0.06131613  0.40217513]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 69\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.01049517 -0.401209    0.06935963  0.71354187]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 69\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.00247099 -0.20711237  0.08363046  0.44347236]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 69\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.00167126 -0.01326716  0.09249991  0.17827947]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 69\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.00193661  0.18041772  0.09606551 -0.0838496 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 69\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.00167175 -0.01594065  0.09438851  0.23752975]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 69\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.00135294 -0.21227545  0.09913911  0.5584301 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 14 of episode: 69\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.00289257 -0.01867466  0.11030771  0.298554  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 15 of episode: 69\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.00326607  0.1747162   0.11627879  0.04259551]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 16 of episode: 69\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 2.2825784e-04 -2.1864552e-02  1.1713070e-01  3.6958441e-01]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 17 of episode: 69\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-2.0903321e-04 -2.1843900e-01  1.2452239e-01  6.9678342e-01]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 18 of episode: 69\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.00457781 -0.02524371  0.13845806  0.4457494 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 19 of episode: 69\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.00508269 -0.22202505  0.14737305  0.77867335]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 20 of episode: 69\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.00952319 -0.02920352  0.1629465   0.5357473 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 21 of episode: 69\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.01010726 -0.22619645  0.17366146  0.87501705]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 22 of episode: 69\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.01463119 -0.42319974  0.1911618   1.2168775 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 23 of episode: 69\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.02309518 -0.62020206  0.21549934  1.562858  ]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 70/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 70\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02727336 -0.20172954  0.03625453  0.32351476]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 70\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02323877 -0.00714208  0.04272482  0.04248186]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 70\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02309593  0.187342    0.04357446 -0.23642075]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 70\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02684277 -0.00837454  0.03884605  0.06968222]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 70\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02667528  0.18616958  0.04023969 -0.21049595]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 70\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03039867  0.38069376  0.03602977 -0.49021873]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 70\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03801255  0.18508255  0.0262254  -0.18640189]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 70\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.0417142  -0.01040461  0.02249736  0.1144374 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 70\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.0415061   0.18438788  0.02478611 -0.17106372]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 70\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.04519386 -0.0110799   0.02136483  0.12933426]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 70\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.04497226  0.18372959  0.02395152 -0.1565325 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 70\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.04864686 -0.01172696  0.02082087  0.1436093 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 70\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.04841232 -0.2071408   0.02369305  0.44278744]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 14 of episode: 70\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.0442695  -0.01236199  0.0325488   0.15766631]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 15 of episode: 70\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.04402226 -0.20793447  0.03570213  0.4604373 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 16 of episode: 70\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03986357 -0.01333486  0.04491087  0.1792181 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 17 of episode: 70\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03959687  0.1811166   0.04849524 -0.09896562]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 18 of episode: 70\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.0432192  -0.01466563  0.04651592  0.20861447]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 19 of episode: 70\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.04292589 -0.21042079  0.05068821  0.5156003 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 20 of episode: 70\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03871748 -0.0160479   0.06100022  0.23931143]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 21 of episode: 70\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03839652 -0.21198583  0.06578645  0.5505946 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 22 of episode: 70\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.0341568  -0.40796712  0.07679834  0.86325794]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 23 of episode: 70\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02599746 -0.21397002  0.0940635   0.59567654]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 24 of episode: 70\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02171806 -0.02028164  0.10597703  0.33404222]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 25 of episode: 70\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02131243 -0.21673974  0.11265787  0.65817505]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 26 of episode: 70\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.01697763 -0.02335116  0.12582137  0.40298244]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 27 of episode: 70\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.01651061 -0.2200121   0.13388102  0.7325359 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 28 of episode: 70\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.01211037 -0.02696913  0.14853173  0.4848062 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 29 of episode: 70\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.01157099 -0.22384064  0.15822786  0.8203699 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 30 of episode: 70\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.00709417 -0.03119674  0.17463526  0.58133894]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 31 of episode: 70\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [0.00647024 0.16110381 0.18626204 0.34835765]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 32 of episode: 70\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.00969231 -0.03611125  0.1932292   0.69350845]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 33 of episode: 70\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.00897009 -0.23331374  0.20709936  1.0402635 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 34 of episode: 70\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.00430381 -0.4304945   0.22790463  1.3901674 ]\n",
      "Reward: 1.0\n",
      "\n",
      "****************************************************\n",
      "Episode: 70, Return: 34.0\n",
      "****************************************************\n",
      "-----------------------------------\n",
      "Episode 71/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 71\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02727336 -0.20172954  0.03625453  0.32351476]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 71\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02323877 -0.00714208  0.04272482  0.04248186]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 71\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02309593  0.187342    0.04357446 -0.23642075]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 71\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02684277 -0.00837454  0.03884605  0.06968222]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 71\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02667528 -0.20403126  0.04023969  0.37436375]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 71\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02259465 -0.399701    0.04772697  0.6794581 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 71\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.01460063 -0.20527339  0.06131613  0.40217513]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 71\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.01049517 -0.01107229  0.06935963  0.12943655]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 71\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.01027372  0.18299106  0.07194836 -0.14058317]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 71\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.01393354 -0.01308365  0.06913669  0.17390256]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 71\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.01367187 -0.20912348  0.07261474  0.48756978]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 71\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.0094894  -0.40519083  0.08236615  0.80222577]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 71\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.00138558 -0.6013399   0.09841066  1.1196401 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 14 of episode: 71\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.01064122 -0.40763685  0.12080346  0.8593775 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 15 of episode: 71\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.01879395 -0.6041788   0.13799101  1.1874717 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 16 of episode: 71\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.03087753 -0.8007933   0.16174045  1.5200307 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 17 of episode: 71\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.0468934  -0.6079536   0.19214106  1.2818911 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 18 of episode: 71\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.05905247 -0.8049321   0.21777889  1.6280642 ]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 72/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 72\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02727336 -0.20172954  0.03625453  0.32351476]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 72\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02323877 -0.00714208  0.04272482  0.04248186]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 72\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02309593  0.187342    0.04357446 -0.23642075]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 72\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02684277 -0.00837454  0.03884605  0.06968222]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 72\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02667528  0.18616958  0.04023969 -0.21049595]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 72\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03039867  0.38069376  0.03602977 -0.49021873]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 72\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03801255  0.5752894   0.0262254  -0.7713323 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 72\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.04951833  0.37981656  0.01079875 -0.4705145 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 72\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.05711466  0.57478434  0.00138846 -0.7597743 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 72\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.06861036  0.3796433  -0.01380703 -0.4666548 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 72\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.07620322  0.1847191  -0.02314012 -0.17835556]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 72\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.0798976   0.3801644  -0.02670723 -0.47824764]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 72\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.08750089  0.5756531  -0.03627219 -0.779227  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 14 of episode: 72\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.09901395  0.7712544  -0.05185673 -1.0830978 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 15 of episode: 72\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.11443904  0.9670209  -0.07351869 -1.3915919 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 16 of episode: 72\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.13377945  1.1629773  -0.10135052 -1.7063274 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 17 of episode: 72\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.157039    1.3591086  -0.13547707 -2.0287607 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 18 of episode: 72\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.18422118  1.5553455  -0.17605229 -2.3601274 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 19 of episode: 72\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.21532808  1.3621783  -0.22325483 -2.1263478 ]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 73/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 73\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02727336  0.18847767  0.03625453 -0.26141977]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 73\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03104291 -0.00714255  0.03102613  0.04247424]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 73\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03090006 -0.20269535  0.03187562  0.3447825 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 73\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02684616 -0.008041    0.03877127  0.06231914]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 73\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02668534  0.18650423  0.04001765 -0.21788351]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 73\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03041542  0.38103196  0.03565998 -0.49767944]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 73\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03803606  0.18542582  0.02570639 -0.19397497]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 73\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.04174458 -0.01005424  0.02182689  0.1067051 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 73\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.04154349  0.18474823  0.02396099 -0.17901242]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 73\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.04523846  0.37951925  0.02038075 -0.4640412 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 73\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.05282884  0.5743474   0.01109992 -0.75023115]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 73\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.06431579  0.37907407 -0.0039047  -0.454076  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 73\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.07189727  0.57425106 -0.01298622 -0.74798715]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 14 of episode: 73\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.08338229  0.3793106  -0.02794596 -0.459419  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 15 of episode: 73\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.0909685   0.5748162  -0.03713435 -0.76077795]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 16 of episode: 73\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.10246482  0.380225   -0.0523499  -0.48000735]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 17 of episode: 73\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.11006933  0.57604533 -0.06195005 -0.78871936]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 18 of episode: 73\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.12159023  0.771961   -0.07772444 -1.1002307 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 19 of episode: 73\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.13702945  0.96801496 -0.09972905 -1.4162518 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 20 of episode: 73\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.15638976  0.7742595  -0.12805408 -1.1563345 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 21 of episode: 73\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.17187494  0.9707968  -0.15118077 -1.4862717 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 22 of episode: 73\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.19129089  1.1674031  -0.1809062  -1.8220955 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 23 of episode: 73\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.21463895  0.9746931  -0.21734813 -1.5906445 ]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 74/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 74\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02727336 -0.20172954  0.03625453  0.32351476]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 74\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02323877 -0.00714208  0.04272482  0.04248186]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 74\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02309593 -0.20284982  0.04357446  0.34833285]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 74\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.01903893 -0.00837385  0.05054112  0.06970263]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 74\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.01887146 -0.2041826   0.05193517  0.37789345]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 74\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.0147878  -0.40000218  0.05949304  0.68648887]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 74\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.00678776 -0.20575435  0.07322282  0.41311327]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 74\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.00267267 -0.4018337   0.08148509  0.7279525 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 74\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.005364   -0.5979819   0.09604413  1.0451287 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 74\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.01732364 -0.40425712  0.1169467   0.78407425]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 74\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.02540878 -0.60077524  0.13262819  1.111142  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 74\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.03742428 -0.7973662   0.15485103  1.4423171 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 74\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.05337161 -0.6044525   0.18369737  1.2017535 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 14 of episode: 74\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.06546066 -0.41211838  0.20773244  0.9718101 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 15 of episode: 74\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.07370303 -0.22029762  0.22716865  0.7509    ]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 75/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 75\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02727336 -0.20172954  0.03625453  0.32351476]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 75\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02323877 -0.00714208  0.04272482  0.04248186]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 75\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02309593 -0.20284982  0.04357446  0.34833285]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 75\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.01903893 -0.00837385  0.05054112  0.06970263]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 75\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.01887146  0.18598844  0.05193517 -0.20661576]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 75\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02259123 -0.00983622  0.04780285  0.10198726]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 75\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.0223945   0.18456922  0.0498426  -0.1752391 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 75\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02608589 -0.01122931  0.04633782  0.13274162]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 75\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.0258613  -0.20698333  0.04899265  0.4396759 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 75\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02172163 -0.01258776  0.05778617  0.16283052]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 75\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02146988  0.18166138  0.06104278 -0.11107717]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 75\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02510311  0.37585792  0.05882123 -0.383894  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 75\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03262026  0.17995232  0.05114336 -0.07326034]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 14 of episode: 75\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03621931 -0.01586411  0.04967815  0.23510984]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 15 of episode: 75\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03590203  0.17851415  0.05438035 -0.04149849]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 16 of episode: 75\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03947231 -0.01734372  0.05355038  0.26783398]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 17 of episode: 75\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03912544  0.17697471  0.05890705 -0.00748956]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 18 of episode: 75\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.04266493  0.37120456  0.05875726 -0.2810203 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 19 of episode: 75\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.05008902  0.5654413   0.05313686 -0.5546079 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 20 of episode: 75\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.06139785  0.36961508  0.0420447  -0.2456683 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 21 of episode: 75\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [0.06879015 0.17391862 0.03713133 0.05997445]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 22 of episode: 75\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.07226852 -0.0217155   0.03833082  0.36413747]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 23 of episode: 75\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [0.07183421 0.17284133 0.04561357 0.0837831 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 24 of episode: 75\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.07529104  0.36728075  0.04728923 -0.19416693]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 25 of episode: 75\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [0.08263665 0.17151536 0.04340589 0.11305072]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 26 of episode: 75\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.08606696  0.36598933  0.04566691 -0.16562815]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 27 of episode: 75\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.09338675  0.56042886  0.04235435 -0.44356194]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 28 of episode: 75\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.10459533  0.364734    0.03348311 -0.13783446]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 29 of episode: 75\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [0.11189    0.16914886 0.03072642 0.16522099]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 30 of episode: 75\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.11527298 -0.02639916  0.03403084  0.4674367 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 31 of episode: 75\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [0.114745   0.16822588 0.04337957 0.1856713 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 32 of episode: 75\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.11810952  0.36270118  0.047093   -0.09301763]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 33 of episode: 75\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.12536354  0.55711764  0.04523265 -0.37047875]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 34 of episode: 75\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.13650589  0.3613832   0.03782307 -0.06388354]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 35 of episode: 75\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.14373356  0.555943    0.0365454  -0.34439722]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 36 of episode: 75\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.15485242  0.36032075  0.02965746 -0.04041779]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 37 of episode: 75\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.16205883  0.55500513  0.0288491  -0.32359794]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 38 of episode: 75\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.17315893  0.35948452  0.02237714 -0.02195861]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 39 of episode: 75\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [0.18034862 0.16404894 0.02193797 0.2776997 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 40 of episode: 75\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.1836296  -0.031379    0.02749196  0.5772203 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 41 of episode: 75\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.18300202 -0.22687528  0.03903637  0.87843555]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 42 of episode: 75\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.17846452 -0.42250532  0.05660508  1.1831309 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 43 of episode: 75\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.17001441 -0.6183142   0.0802677   1.4930068 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 44 of episode: 75\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.15764813 -0.4242554   0.11012784  1.2264291 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 45 of episode: 75\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.14916302 -0.2307099   0.13465641  0.9701818 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 46 of episode: 75\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.14454882 -0.42735714  0.15406005  1.3019519 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 47 of episode: 75\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.13600168 -0.62406105  0.18009908  1.6386279 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 48 of episode: 75\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.12352046 -0.43144947  0.21287166  1.4070468 ]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 76/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 76\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02727336 -0.20172954  0.03625453  0.32351476]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 76\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02323877 -0.00714208  0.04272482  0.04248186]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 76\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02309593  0.187342    0.04357446 -0.23642075]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 76\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02684277 -0.00837454  0.03884605  0.06968222]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 76\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02667528 -0.20403126  0.04023969  0.37436375]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 76\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02259465 -0.399701    0.04772697  0.6794581 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 76\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.01460063 -0.5954523   0.06131613  0.98677707]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 76\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.00269159 -0.7913394   0.08105167  1.2980711 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 76\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.0131352  -0.5973347   0.10701309  1.0318226 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 76\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.0250819  -0.7937048   0.12764955  1.3560946 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 76\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.04095599 -0.60039455  0.15477143  1.1059155 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 76\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.05296388 -0.7971748   0.17688975  1.442879  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 76\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.06890738 -0.9939777   0.20574732  1.7852117 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 14 of episode: 76\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.08878693 -1.1907316   0.24145156  2.1341817 ]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 77/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 77\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02727336 -0.20172954  0.03625453  0.32351476]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 77\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02323877 -0.00714208  0.04272482  0.04248186]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 77\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02309593  0.187342    0.04357446 -0.23642075]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 77\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02684277  0.3818152   0.03884605 -0.51504683]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 77\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03447907  0.18616833  0.02854511 -0.21038006]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 77\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03820244 -0.00934989  0.02433751  0.09116892]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 77\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03801544 -0.20481206  0.02616089  0.3914299 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 77\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.0339192  -0.01007097  0.03398949  0.10710862]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 77\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03371778  0.18454783  0.03613166 -0.17465998]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 77\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03740874 -0.0110721   0.03263846  0.12919879]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 77\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.0371873   0.18356746  0.03522243 -0.15301107]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 77\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.04085864  0.37816784  0.03216221 -0.4343773 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 77\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.048422    0.18260567  0.02347466 -0.13173172]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 14 of episode: 77\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.05207412  0.37738362  0.02084003 -0.41691723]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 15 of episode: 77\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.05962179  0.5722041   0.01250168 -0.702958  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 16 of episode: 77\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.07106587  0.37691116 -0.00155747 -0.40636605]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 17 of episode: 77\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.07860409  0.57205516 -0.0096848  -0.6995396 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 18 of episode: 77\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.0900452   0.76731    -0.02367559 -0.9952555 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 19 of episode: 77\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.1053914   0.96274054 -0.0435807  -1.295279  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 20 of episode: 77\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.12464621  0.7681985  -0.06948628 -1.0165517 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 21 of episode: 77\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.14001018  0.9641746  -0.08981731 -1.330219  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 22 of episode: 77\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.15929367  0.7702932  -0.11642169 -1.06694   ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 23 of episode: 77\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.17469953  0.9667468  -0.13776049 -1.3937763 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 24 of episode: 77\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.19403447  0.7735819  -0.16563602 -1.1471475 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 25 of episode: 77\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.20950611  0.5809637  -0.18857896 -0.91064924]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 26 of episode: 77\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.22112538  0.77806795 -0.20679195 -1.2561784 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 27 of episode: 77\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.23668674  0.9751478  -0.23191552 -1.6058642 ]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 78/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 78\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02727336  0.18847767  0.03625453 -0.26141977]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 78\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03104291 -0.00714255  0.03102613  0.04247424]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 78\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03090006 -0.20269535  0.03187562  0.3447825 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 78\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02684616 -0.3982559   0.03877127  0.6473442 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 78\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.01888104 -0.20369498  0.05171815  0.36711797]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 78\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.01480714 -0.39951223  0.05906051  0.67564946]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 78\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.00681689 -0.20525856  0.0725735   0.40213072]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 78\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.00271172 -0.01123697  0.08061612  0.1331823 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 78\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.00248698 -0.2074156   0.08327976  0.45017   ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 78\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.00166133 -0.4036106   0.09228316  0.76789856]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 78\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.00973354 -0.20987199  0.10764113  0.50561994]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 78\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.01393098 -0.01641846  0.11775353  0.24870452]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 78\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.01425935  0.17684221  0.12272762 -0.00463942]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 14 of episode: 78\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.0107225  -0.01980656  0.12263484  0.32410648]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 15 of episode: 78\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.01111864  0.17337522  0.12911697  0.07247441]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 16 of episode: 78\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.00765113 -0.02333865  0.13056645  0.40294406]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 17 of episode: 78\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.0081179   0.16971317  0.13862532  0.15410867]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 18 of episode: 78\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.00472364 -0.02709333  0.14170751  0.4871119 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 19 of episode: 78\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.00526551  0.16577463  0.15144974  0.24223255]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 20 of episode: 78\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.00195002  0.35844538  0.15629439  0.00089078]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 21 of episode: 78\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.00521889  0.55102074  0.15631221 -0.23868759]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 22 of episode: 78\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.01623931  0.7436044   0.15153846 -0.4782723 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 23 of episode: 78\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03111139  0.9362986   0.14197302 -0.71961915]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 24 of episode: 78\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.04983737  1.1292006   0.12758063 -0.9644609 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 25 of episode: 78\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.07242138  0.93261707  0.10829141 -0.6345751 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 26 of episode: 78\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.09107372  0.7361645   0.0955999  -0.30984703]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 27 of episode: 78\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.10579701  0.9298036   0.08940297 -0.5709158 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 28 of episode: 78\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.12439308  0.7335492   0.07798465 -0.25146037]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 29 of episode: 78\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [0.13906406 0.53740525 0.07295544 0.06476557]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 30 of episode: 78\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.14981218  0.7314095   0.07425076 -0.20403683]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 31 of episode: 78\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [0.16444036 0.5353086  0.07017002 0.11111376]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 32 of episode: 78\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.17514654  0.72935855  0.07239229 -0.15863182]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 33 of episode: 78\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.1897337   0.9233734   0.06921966 -0.42762712]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 34 of episode: 78\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.20820117  1.1174502   0.06066712 -0.6977109 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 35 of episode: 78\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.23055017  1.3116808   0.0467129  -0.97069556]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 36 of episode: 78\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.25678378  1.115964    0.02729899 -0.6637121 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 37 of episode: 78\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.27910307  1.3106958   0.01402474 -0.947676  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 38 of episode: 78\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.30531698  1.5056261  -0.00492878 -1.2359196 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 39 of episode: 78\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.33542952  1.700811   -0.02964717 -1.5301424 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 40 of episode: 78\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.36944574  1.8962777  -0.06025002 -1.8319285 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 41 of episode: 78\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.40737128  1.7018721  -0.09688859 -1.5585521 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 42 of episode: 78\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.44140872  1.5080345  -0.12805963 -1.2976    ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 43 of episode: 78\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.47156942  1.7045287  -0.15401164 -1.6274747 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 44 of episode: 78\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.50566     1.9010898  -0.18656112 -1.963927  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 45 of episode: 78\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.5436818   2.097633   -0.22583966 -2.3081574 ]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 79/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 79\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02727336  0.18847767  0.03625453 -0.26141977]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 79\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03104291 -0.00714255  0.03102613  0.04247424]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 79\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03090006 -0.20269535  0.03187562  0.3447825 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 79\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02684616 -0.008041    0.03877127  0.06231914]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 79\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02668534 -0.20369677  0.04001765  0.36697814]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 79\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.0226114  -0.00916565  0.04735721  0.08717711]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 79\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02242809 -0.20493332  0.04910076  0.3944172 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 79\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.01832942 -0.40071633  0.0569891   0.70216763]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 79\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.0103151  -0.59657997  0.07103245  1.0122318 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 79\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.0016165  -0.40247387  0.09127709  0.7426729 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 79\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.00966598 -0.20872246  0.10613055  0.4800538 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 79\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.01384043 -0.40516993  0.11573162  0.8042109 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 79\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.02194383 -0.6016723   0.13181584  1.1309419 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 14 of episode: 79\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.03397727 -0.7982504   0.15443468  1.4618927 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 15 of episode: 79\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.04994228 -0.9948909   0.18367253  1.7985667 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 16 of episode: 79\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.0698401  -0.8022381   0.21964386  1.5681448 ]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 80/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 80\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02727336  0.18847767  0.03625453 -0.26141977]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 80\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03104291 -0.00714255  0.03102613  0.04247424]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 80\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03090006  0.18752107  0.03187562 -0.24026045]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 80\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03465049  0.3821735   0.02707041 -0.52272093]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 80\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.04229395  0.18668123  0.01661599 -0.22163221]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 80\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.04602758 -0.00867424  0.01218335  0.0762454 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 80\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.0458541   0.18627095  0.01370825 -0.21256886]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 80\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.04957952 -0.00904428  0.00945688  0.08440655]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 80\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.04939863 -0.20430051  0.01114501  0.38005808]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 80\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.04531262 -0.39957893  0.01874617  0.67623407]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 80\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03732104 -0.20472242  0.03227085  0.3895117 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 80\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03322659 -0.40028718  0.04006109  0.69219214]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 80\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02522085 -0.59594136  0.05390493  0.9972127 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 14 of episode: 80\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.01330202 -0.791741    0.07384918  1.306326  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 15 of episode: 80\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.0025328  -0.59762883  0.09997571  1.0376432 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 16 of episode: 80\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.01448538 -0.79392713  0.12072857  1.3599643 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 17 of episode: 80\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.03036392 -0.99033797  0.14792785  1.6878442 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 18 of episode: 80\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.05017068 -1.1868286   0.18168473  2.0226936 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 19 of episode: 80\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.07390725 -1.3833086   0.2221386   2.3656845 ]\n",
      "Reward: 1.0\n",
      "\n",
      "****************************************************\n",
      "Episode: 80, Return: 19.0\n",
      "****************************************************\n",
      "-----------------------------------\n",
      "Episode 81/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 81\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02727336  0.18847767  0.03625453 -0.26141977]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 81\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03104291  0.38306385  0.03102613 -0.5424507 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 81\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03870419  0.1875199   0.02017712 -0.24015574]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 81\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.04245459 -0.0078844   0.015374    0.0588227 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 81\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.0422969   0.18701379  0.01655046 -0.22897024]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 81\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.04603718  0.38189536  0.01197105 -0.516387  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 81\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.05367509  0.5768467   0.00164331 -0.80527365]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 81\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.06521202  0.7719461  -0.01446216 -1.0974392 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 81\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.08065094  0.57701755 -0.03641094 -0.8093286 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 81\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.09219129  0.77261895 -0.05259752 -1.1132388 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 81\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.10764367  0.5782257  -0.07486229 -0.8375087 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 81\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.11920819  0.77428585 -0.09161247 -1.1527641 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 81\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.1346939   0.5804705  -0.11466775 -0.8901567 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 14 of episode: 81\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.14630331  0.38707548 -0.13247088 -0.6356077 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 15 of episode: 81\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.15404482  0.58377194 -0.14518304 -0.96690005]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 16 of episode: 81\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.16572025  0.78051364 -0.16452104 -1.3014418 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 17 of episode: 81\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.18133053  0.9772959  -0.19054987 -1.6407807 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 18 of episode: 81\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.20087644  1.174071   -0.22336549 -1.9862843 ]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 82/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 82\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02727336  0.18847767  0.03625453 -0.26141977]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 82\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03104291  0.38306385  0.03102613 -0.5424507 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 82\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03870419  0.5777363   0.02017712 -0.8251987 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 82\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.05025892  0.3823443   0.00367314 -0.5262386 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 82\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.0579058   0.5774144  -0.00685163 -0.8177619 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 82\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.06945409  0.38238686 -0.02320687 -0.5272419 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 82\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.07710183  0.5778275  -0.0337517  -0.8271461 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 82\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.08865838  0.38318297 -0.05029463 -0.5452667 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 82\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.09632204  0.5789742  -0.06119996 -0.85336256]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 82\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.10790152  0.77487457 -0.07826721 -1.1646446 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 82\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.12339901  0.97092324 -0.1015601  -1.4808044 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 82\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.14281748  0.7771767  -0.13117619 -1.2214893 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 82\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.15836102  0.9737219  -0.15560597 -1.5522296 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 14 of episode: 82\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.17783545  1.1703298  -0.18665057 -1.8891422 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 15 of episode: 82\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.20124204  1.3669254  -0.2244334  -2.2334707 ]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 83/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 83\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02727336 -0.20172954  0.03625453  0.32351476]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 83\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02323877 -0.39734846  0.04272482  0.62740684]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 83\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.0152918  -0.20284806  0.05527296  0.34847975]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 83\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.01123484 -0.00855404  0.06224256  0.07372576]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 83\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.01106376 -0.20451054  0.06371707  0.3853788 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 83\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.00697355 -0.01034828  0.07142465  0.11344658]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 83\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.00676658  0.18368141  0.07369358 -0.1558749 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 83\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.01044021 -0.01241407  0.07057608  0.15911628]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 83\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.01019193 -0.20847175  0.07375841  0.47320282]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 83\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.0060225  -0.01446476  0.08322246  0.2046489 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 83\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.0057332   0.17937456  0.08731544 -0.06066459]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 83\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.00932069 -0.01688375  0.08610215  0.2582395 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 83\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.00898302 -0.21312265  0.09126694  0.57679015]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 14 of episode: 83\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.00472056 -0.01939052  0.10280274  0.31419668]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 15 of episode: 83\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.00433275 -0.2158152   0.10908668  0.63744897]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 16 of episode: 83\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 1.6448317e-05 -4.1227570e-01  1.2183566e-01  9.6239591e-01]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 17 of episode: 83\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.00822907 -0.6088055   0.14108357  1.2907364 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 18 of episode: 83\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.02040518 -0.41573092  0.1668983   1.0453433 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 19 of episode: 83\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.02871979 -0.22317007  0.18780516  0.80935615]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 20 of episode: 83\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.03318319 -0.42030042  0.20399229  1.154743  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 21 of episode: 83\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.0415892  -0.22833642  0.22708715  0.932326  ]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 84/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 84\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02727336  0.18847767  0.03625453 -0.26141977]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 84\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03104291  0.38306385  0.03102613 -0.5424507 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 84\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03870419  0.1875199   0.02017712 -0.24015574]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 84\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.04245459 -0.0078844   0.015374    0.0588227 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 84\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.0422969  -0.20322338  0.01655046  0.3563163 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 84\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03823243 -0.00834059  0.02367678  0.06889778]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 84\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03806562 -0.20379385  0.02505474  0.36895582]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 84\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03398975 -0.00903668  0.03243386  0.08427709]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 84\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03380901 -0.20460819  0.0341194   0.38701394]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 84\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02971685 -0.00998676  0.04185968  0.10528087]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 84\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02951711 -0.20568281  0.04396529  0.41087094]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 84\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02540346 -0.01121083  0.05218271  0.13236648]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 84\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02517924 -0.20703992  0.05483004  0.44104505]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 14 of episode: 84\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02103844 -0.40289322  0.06365094  0.75049543]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 15 of episode: 84\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.01298058 -0.59883255  0.07866085  1.06251   ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 16 of episode: 84\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 1.0039262e-03 -7.9490286e-01  9.9911049e-02  1.3788085e+00]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 17 of episode: 84\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.01489413 -0.99112034  0.12748723  1.7009919 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 18 of episode: 84\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.03471654 -0.79767686  0.16150706  1.4505614 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 19 of episode: 84\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.05067008 -0.9943727   0.19051829  1.7890424 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 20 of episode: 84\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.07055753 -1.191054    0.22629914  2.1344004 ]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 85/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 85\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02727336 -0.20172954  0.03625453  0.32351476]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 85\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02323877 -0.00714208  0.04272482  0.04248186]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 85\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02309593 -0.20284982  0.04357446  0.34833285]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 85\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.01903893 -0.00837385  0.05054112  0.06970263]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 85\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.01887146 -0.2041826   0.05193517  0.37789345]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 85\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.0147878  -0.40000218  0.05949304  0.68648887]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 85\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.00678776 -0.5958973   0.07322282  0.9972924 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 85\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.00513019 -0.40182677  0.09316867  0.72847486]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 85\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.01316672 -0.20810778  0.10773816  0.46650863]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 85\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.01732888 -0.01465987  0.11706834  0.20963298]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 85\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.01762207 -0.21124421  0.12126099  0.53683066]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 85\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.02184696 -0.4078437   0.1319976   0.8651279 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 85\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.03000383 -0.21474154  0.14930016  0.616689  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 14 of episode: 85\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.03429866 -0.02198597  0.16163395  0.37450352]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 15 of episode: 85\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.03473838  0.1705153   0.16912402  0.13682905]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 16 of episode: 85\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.03132808 -0.0265745   0.17186059  0.4777316 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 17 of episode: 85\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.03185957 -0.22365312  0.18141523  0.8192732 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 18 of episode: 85\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.03633263 -0.03141595  0.1978007   0.58869356]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 19 of episode: 85\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.03696095  0.16046691  0.20957457  0.36425644]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 86/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 86\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02727336  0.18847767  0.03625453 -0.26141977]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 86\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03104291 -0.00714255  0.03102613  0.04247424]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 86\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03090006 -0.20269535  0.03187562  0.3447825 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 86\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02684616 -0.008041    0.03877127  0.06231914]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 86\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02668534  0.18650423  0.04001765 -0.21788351]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 86\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03041542 -0.00916626  0.03565998  0.08714928]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 86\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.0302321   0.18542688  0.03740297 -0.19407304]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 86\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03394064 -0.01020959  0.03352151  0.11017033]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 86\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03373644 -0.20579547  0.03572491  0.4132378 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 86\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02962053 -0.01119764  0.04398967  0.13202819]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 86\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02939658 -0.2069212   0.04663023  0.43825835]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 86\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02525816 -0.01248921  0.0553954   0.16063169]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 86\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02500837  0.18179773  0.05860803 -0.11407378]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 14 of episode: 86\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02864433 -0.0141129   0.05632656  0.19650851]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 15 of episode: 86\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02836207 -0.2099934   0.06025673  0.50641453]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 16 of episode: 86\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.0241622  -0.01577     0.07038502  0.23331293]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 17 of episode: 86\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.0238468  -0.21182336  0.07505128  0.5473409 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 18 of episode: 86\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.01961033 -0.0178316   0.0859981   0.27921683]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 19 of episode: 86\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.0192537  -0.21406835  0.09158243  0.5977364 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 20 of episode: 86\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.01497233 -0.02033918  0.10353716  0.33524808]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 21 of episode: 86\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [0.01456555 0.17316857 0.11024212 0.07692642]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 22 of episode: 86\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.01802892  0.36655164  0.11178065 -0.1790417 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 23 of episode: 86\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [0.02535995 0.17002232 0.10819982 0.1467076 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 24 of episode: 86\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.0287604  -0.02646936  0.11113397  0.47147024]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 25 of episode: 86\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02823101 -0.22297114  0.12056337  0.7970107 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 26 of episode: 86\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02377159 -0.02969158  0.13650359  0.5445557 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 27 of episode: 86\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02317776 -0.226441    0.1473947   0.8769421 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 28 of episode: 86\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.01864894 -0.42322558  0.16493355  1.2120957 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 29 of episode: 86\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.01018443 -0.23057103  0.18917546  0.97530645]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 30 of episode: 86\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.00557301 -0.03842054  0.20868158  0.74750924]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 31 of episode: 86\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.0048046  -0.23571661  0.22363177  1.0979409 ]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 87/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 87\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02727336 -0.20172954  0.03625453  0.32351476]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 87\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02323877 -0.00714208  0.04272482  0.04248186]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 87\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02309593  0.187342    0.04357446 -0.23642075]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 87\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02684277  0.3818152   0.03884605 -0.51504683]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 87\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03447907  0.18616833  0.02854511 -0.21038006]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 87\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03820244 -0.00934989  0.02433751  0.09116892]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 87\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03801544  0.18541493  0.02616089 -0.19373725]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 87\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.04172374  0.38015306  0.02228614 -0.4780541 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 87\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.0493268   0.18472368  0.01272506 -0.17843123]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 87\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.05302127  0.37966123  0.00915643 -0.4670728 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 87\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 6.0614500e-02  5.7465261e-01 -1.8502203e-04 -7.5685573e-01]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 87\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.07210755  0.37953323 -0.01532214 -0.464231  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 87\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.07969822  0.5748683  -0.02460676 -0.7617038 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 14 of episode: 87\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.09119558  0.7703204  -0.03984083 -1.0620269 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 15 of episode: 87\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.10660199  0.575748   -0.06108137 -0.7821099 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 16 of episode: 87\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.11811695  0.3815164  -0.07672357 -0.5092526 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 17 of episode: 87\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.12574728  0.18755446 -0.08690862 -0.24170019]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 18 of episode: 87\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.12949838  0.3838034  -0.09174262 -0.56048155]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 19 of episode: 87\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.13717444  0.1900807  -0.10295226 -0.29805404]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 20 of episode: 87\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.14097606 -0.00343453 -0.10891333 -0.03953269]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 21 of episode: 87\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.14090736 -0.1968398  -0.10970399  0.21689901]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 22 of episode: 87\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.13697056 -0.3902364  -0.10536601  0.47306174]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 23 of episode: 87\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.12916584 -0.5837248  -0.09590477  0.7307644 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 24 of episode: 87\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.11749134 -0.77739966 -0.08128949  0.9917889 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 25 of episode: 87\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.10194335 -0.5812896  -0.06145371  0.6747224 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 26 of episode: 87\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.09031755 -0.38536987 -0.04795926  0.3633415 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 27 of episode: 87\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.08261016 -0.57977855 -0.04069243  0.6405246 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 28 of episode: 87\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.07101459 -0.38411364 -0.02788194  0.33530992]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 29 of episode: 87\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.06333231 -0.1886062  -0.02117574  0.03396653]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 30 of episode: 87\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.05956019  0.00681292 -0.02049641 -0.2653216 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 31 of episode: 87\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.05969645  0.20222132 -0.02580284 -0.56439817]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 32 of episode: 87\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.06374088  0.00747073 -0.0370908  -0.27995473]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 33 of episode: 87\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.06389029  0.2031016  -0.0426899  -0.58410144]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 34 of episode: 87\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.06795232  0.39879477 -0.05437193 -0.8899207 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 35 of episode: 87\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.07592822  0.59461063 -0.07217034 -1.199188  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 36 of episode: 87\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.08782043  0.7905883  -0.0961541  -1.5135889 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 37 of episode: 87\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.1036322   0.9867339  -0.12642588 -1.834674  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 38 of episode: 87\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.12336688  1.1830074  -0.16311936 -2.1638048 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 39 of episode: 87\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.14702702  0.98981446 -0.20639545 -1.9256068 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 40 of episode: 87\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.16682331  0.7974191  -0.24490759 -1.7033893 ]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 88/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 88\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02727336  0.18847767  0.03625453 -0.26141977]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 88\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03104291  0.38306385  0.03102613 -0.5424507 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 88\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03870419  0.1875199   0.02017712 -0.24015574]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 88\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.04245459 -0.0078844   0.015374    0.0588227 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 88\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.0422969  -0.20322338  0.01655046  0.3563163 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 88\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03823243 -0.39857668  0.02367678  0.65417176]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 88\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.0302609  -0.20379226  0.03676022  0.36903733]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 88\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02618506 -0.00921137  0.04414096  0.08816826]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 88\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02600083 -0.20493732  0.04590433  0.39444444]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 88\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02190208 -0.40067956  0.05379322  0.70123965]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 88\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.01388849 -0.5965043   0.06781801  1.0103594 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 88\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.0019584 -0.4023497  0.0880252  0.7397202]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 88\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.00608859 -0.5985697   0.10281961  1.0587566 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 14 of episode: 88\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.01805998 -0.79489225  0.12399474  1.381861  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 15 of episode: 88\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.03395783 -0.6015164   0.15163195  1.1303853 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 16 of episode: 88\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.04598816 -0.40866956  0.17423967  0.8888433 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 17 of episode: 88\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.05416155 -0.60567325  0.19201653  1.2308421 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 18 of episode: 88\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.06627502 -0.8026748   0.21663336  1.57702   ]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 89/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 89\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02727336 -0.20172954  0.03625453  0.32351476]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 89\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02323877 -0.00714208  0.04272482  0.04248186]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 89\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02309593 -0.20284982  0.04357446  0.34833285]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 89\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.01903893 -0.39856356  0.05054112  0.65443164]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 89\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.01106766 -0.20418042  0.06362975  0.378082  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 89\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.00698405 -0.01001712  0.07119139  0.10612098]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 89\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.00678371 -0.20608324  0.07331381  0.4203878 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 89\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.00266205 -0.01207249  0.08172157  0.15168837]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 89\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.0024206   0.18178996  0.08475533 -0.11413544]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 89\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.0060564  -0.01443771  0.08247262  0.20403777]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 89\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.00576764  0.1794139   0.08655338 -0.06153185]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 89\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.00935592 -0.01683553  0.08532275  0.25715527]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 89\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.00901921  0.17697121  0.09046585 -0.00744283]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 14 of episode: 89\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.01255863 -0.01932386  0.090317    0.3123564 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 15 of episode: 89\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.01217216 -0.21560863  0.09656412  0.63210064]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 16 of episode: 89\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.00785998 -0.41193575  0.10920613  0.9535651 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 17 of episode: 89\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-3.7873178e-04 -6.0834408e-01  1.2827744e-01  1.2784654e+00]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 18 of episode: 89\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.01254561 -0.4150691   0.15384674  1.0285453 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 19 of episode: 89\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.02084699 -0.6118665   0.17441764  1.3653075 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 20 of episode: 89\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.03308433 -0.80869013  0.2017238   1.7070827 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 21 of episode: 89\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.04925813 -1.0054796   0.23586546  2.0551867 ]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 90/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 90\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02727336 -0.20172954  0.03625453  0.32351476]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 90\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02323877 -0.39734846  0.04272482  0.62740684]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 90\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.0152918  -0.5930399   0.05527296  0.9332334 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 90\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.003431   -0.7888622   0.07393762  1.2427603 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 90\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.01234624 -0.594763    0.09879284  0.9741247 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 90\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.0242415  -0.40109524  0.11827533  0.7140375 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 90\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.03226341 -0.207792    0.13255608  0.46080026]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 90\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.03641925 -0.01476824  0.14177208  0.2126621 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 90\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.03671461  0.17807215  0.14602533 -0.03215487]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 90\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.03315317 -0.01880922  0.14538223  0.3028032 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 90\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.03352935  0.1739739   0.1514383   0.05927109]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 90\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.03004987  0.36663672  0.15262371 -0.18206279]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 90\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.02271714  0.5592828   0.14898245 -0.4229754 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 14 of episode: 90\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.01153148  0.75201476  0.14052294 -0.66523194]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 15 of episode: 90\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.00350881  0.9449314   0.1272183  -0.9105766 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 16 of episode: 90\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02240744  1.1381235   0.10900678 -1.1607215 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 17 of episode: 90\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.04516991  1.3316699   0.08579235 -1.4173334 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 18 of episode: 90\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.07180331  1.5256313   0.05744568 -1.6820133 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 19 of episode: 90\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.10231593  1.7200427   0.02380541 -1.9562697 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 20 of episode: 90\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.1367168   1.9149042  -0.01531998 -2.241481  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 21 of episode: 90\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.17501487  2.1101675  -0.06014961 -2.5388453 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 22 of episode: 90\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.21721822  1.9155794  -0.11092651 -2.2651646 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 23 of episode: 90\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.25552982  2.1115494  -0.15622981 -2.5898583 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 24 of episode: 90\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.2977608   2.3075104  -0.20802698 -2.9259648 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 25 of episode: 90\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.343911    2.5031946  -0.26654625 -3.2738826 ]\n",
      "Reward: 1.0\n",
      "\n",
      "****************************************************\n",
      "Episode: 90, Return: 25.0\n",
      "****************************************************\n",
      "-----------------------------------\n",
      "Episode 91/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 91\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02727336  0.18847767  0.03625453 -0.26141977]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 91\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03104291 -0.00714255  0.03102613  0.04247424]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 91\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03090006  0.18752107  0.03187562 -0.24026045]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 91\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03465049  0.3821735   0.02707041 -0.52272093]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 91\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.04229395  0.57690424  0.01661599 -0.80675226]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 91\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 5.3832039e-02  7.7179450e-01  4.8094502e-04 -1.0941625e+00]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 91\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.06926793  0.57666624 -0.0214023  -0.80132866]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 91\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.08080126  0.77207506 -0.03742888 -1.1006666 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 91\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.09624276  0.5774651  -0.05944221 -0.8199577 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 91\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.10779206  0.38320485 -0.07584136 -0.5465476 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 91\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.11545616  0.18922581 -0.08677232 -0.27869144]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 91\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.11924067 -0.00455801 -0.09234615 -0.01458841]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 91\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.11914951 -0.1982426  -0.09263791  0.2475894 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 14 of episode: 91\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.11518466 -0.00192813 -0.08768612 -0.07281629]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 15 of episode: 91\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.1151461  -0.19569069 -0.08914245  0.1909642 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 16 of episode: 91\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.11123228 -0.38943177 -0.08532317  0.4542488 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 17 of episode: 91\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.10344364 -0.19321354 -0.07623819  0.13593753]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 18 of episode: 91\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.09957938 -0.3871654  -0.07351944  0.40362793]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 19 of episode: 91\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.09183607 -0.19108194 -0.06544688  0.08870204]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 20 of episode: 91\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.08801443 -0.3852077  -0.06367284  0.36003965]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 21 of episode: 91\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.08031028 -0.18924117 -0.05647205  0.04797833]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 22 of episode: 91\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.07652545  0.00664316 -0.05551248 -0.26197374]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 23 of episode: 91\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.07665832  0.20251177 -0.06075196 -0.5716364 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 24 of episode: 91\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.08070855  0.39843065 -0.07218468 -0.88282263]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 25 of episode: 91\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.08867717  0.20435937 -0.08984113 -0.6136777 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 26 of episode: 91\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.09276436  0.01060011 -0.10211469 -0.35058868]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 27 of episode: 91\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.09297635  0.20701481 -0.10912646 -0.67364556]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 28 of episode: 91\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.09711665  0.40347067 -0.12259937 -0.998596  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 29 of episode: 91\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.10518607  0.21018195 -0.1425713  -0.74679315]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 30 of episode: 91\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.1093897   0.01728474 -0.15750715 -0.50215715]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 31 of episode: 91\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.1097354  -0.17530736 -0.1675503  -0.26296088]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 32 of episode: 91\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.10622925  0.0217605  -0.17280951 -0.6034528 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 33 of episode: 91\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.10666446 -0.17057714 -0.18487857 -0.369797  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 34 of episode: 91\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.10325292  0.02662411 -0.19227451 -0.7146032 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 35 of episode: 91\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.1037854  -0.16538996 -0.20656657 -0.48807073]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 36 of episode: 91\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.1004776  -0.35709062 -0.216328   -0.26693243]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 92/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 92\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02727336  0.18847767  0.03625453 -0.26141977]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 92\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03104291  0.38306385  0.03102613 -0.5424507 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 92\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03870419  0.1875199   0.02017712 -0.24015574]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 92\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.04245459  0.38234788  0.015374   -0.5264066 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 92\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.05010155  0.5772502   0.00484587 -0.81420565]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 92\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.06164655  0.3820622  -0.01143824 -0.5200025 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 92\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.06928779  0.18710311 -0.02183829 -0.23094575]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 92\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.07302985 -0.00770008 -0.02645721  0.05476942]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 92\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.07287586 -0.20243289 -0.02536182  0.33898887]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 92\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.0688272  -0.39718494 -0.01858204  0.62356746]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 92\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.0608835  -0.20180854 -0.00611069  0.32509065]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 92\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 5.6847326e-02 -3.9684296e-01  3.9112219e-04  6.1584026e-01]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 92\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.04891047 -0.20172647  0.01270793  0.32328054]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 14 of episode: 92\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.04487594 -0.00678776  0.01917354  0.03463213]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 15 of episode: 92\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.04474019  0.18805407  0.01986618 -0.25194022]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 16 of episode: 92\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.04850126  0.3828868   0.01482738 -0.53829134]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 17 of episode: 92\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.056159    0.5777972   0.00406155 -0.8262657 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 18 of episode: 92\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.06771494  0.7728633  -0.01246376 -1.1176685 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 19 of episode: 92\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.08317221  0.9681466  -0.03481713 -1.4142349 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 20 of episode: 92\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.10253514  0.7734729  -0.06310183 -1.1326355 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 21 of episode: 92\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.11800461  0.57923114 -0.08575454 -0.86039233]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 22 of episode: 92\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.12958923  0.38537523 -0.10296239 -0.5958579 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 23 of episode: 92\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.13729674  0.19183348 -0.11487955 -0.33730027]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 24 of episode: 92\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.1411334  -0.00148225 -0.12162555 -0.08293835]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 25 of episode: 92\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.14110376 -0.1946698  -0.12328432  0.16903247]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 26 of episode: 92\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.13721035 -0.38783124 -0.11990367  0.42042166]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 27 of episode: 92\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.12945373 -0.19123258 -0.11149523  0.09247372]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 28 of episode: 92\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.12562908 -0.3845945  -0.10964576  0.34800398]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 29 of episode: 92\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.11793719 -0.18809775 -0.10268568  0.02285749]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 30 of episode: 92\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.11417524  0.00833546 -0.10222853 -0.3003768 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 31 of episode: 92\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.11434194 -0.18519208 -0.10823607 -0.0416039 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 32 of episode: 92\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.1106381  -0.3786089  -0.10906815  0.21506426]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 33 of episode: 92\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.10306593 -0.57201624 -0.10476686  0.47144896]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 34 of episode: 92\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.0916256  -0.37558264 -0.09533788  0.14766894]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 35 of episode: 92\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.08411395 -0.17923388 -0.0923845  -0.1735036 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 36 of episode: 92\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.08052927 -0.3729205  -0.09585457  0.08866501]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 37 of episode: 92\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.07307086 -0.56654716 -0.09408128  0.34963354]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 38 of episode: 92\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.06173992 -0.7602139  -0.08708861  0.6112299 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 39 of episode: 92\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.04653564 -0.95401764 -0.07486401  0.875262  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 40 of episode: 92\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02745529 -0.75796217 -0.05735876  0.5600131 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 41 of episode: 92\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.01229604 -0.5620841  -0.0461585   0.24982493]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 42 of episode: 92\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.00105436 -0.75651747 -0.041162    0.5275987 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 43 of episode: 92\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.01407599 -0.9510369  -0.03061003  0.80703235]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 44 of episode: 92\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.03309672 -1.1457262  -0.01446938  1.0899316 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 45 of episode: 92\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.05601125 -1.3406545   0.00732925  1.3780395 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 46 of episode: 92\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.08282433 -1.1456248   0.03489004  1.0876577 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 47 of episode: 92\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.10573683 -1.341189    0.05664319  1.3910811 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 48 of episode: 92\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.13256061 -1.1468164   0.08446482  1.1166339 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 49 of episode: 92\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.15549694 -0.9528984   0.10679749  0.8515971 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 50 of episode: 92\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.17455491 -0.75938195  0.12382944  0.594315  ]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 93/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 93\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02727336  0.18847767  0.03625453 -0.26141977]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 93\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03104291  0.38306385  0.03102613 -0.5424507 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 93\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03870419  0.1875199   0.02017712 -0.24015574]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 93\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.04245459  0.38234788  0.015374   -0.5264066 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 93\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.05010155  0.5772502   0.00484587 -0.81420565]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 93\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.06164655  0.3820622  -0.01143824 -0.5200025 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 93\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.06928779  0.18710311 -0.02183829 -0.23094575]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 93\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.07302985  0.3825302  -0.02645721 -0.53043646]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 93\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.08068046  0.18779022 -0.03706593 -0.24620621]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 93\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.08443627 -0.00678328 -0.04199006  0.03455869]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 93\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.0843006  -0.2012787  -0.04129888  0.31370324]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 93\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.08027503 -0.39578876 -0.03502482  0.59308106]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 93\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.07235925 -0.20019446 -0.0231632   0.2895744 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 14 of episode: 93\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.06835536 -0.00475001 -0.01737171 -0.01032301]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 15 of episode: 93\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.06826036  0.19061671 -0.01757817 -0.3084359 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 16 of episode: 93\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.07207269 -0.00425042 -0.02374689 -0.02134807]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 17 of episode: 93\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.07198769 -0.1990239  -0.02417385  0.26374885]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 18 of episode: 93\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.06800721 -0.0035654  -0.01889887 -0.03645966]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 19 of episode: 93\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.0679359   0.1918224  -0.01962807 -0.33504495]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 20 of episode: 93\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.07177235  0.38721812 -0.02632897 -0.63385236]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 21 of episode: 93\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.07951671  0.19247314 -0.03900601 -0.34957597]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 22 of episode: 93\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.08336617  0.3881275  -0.04599753 -0.65429914]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 23 of episode: 93\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.09112872  0.5838587  -0.05908351 -0.9611039 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 24 of episode: 93\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.1028059   0.3895785  -0.07830559 -0.6875525 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 25 of episode: 93\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.11059747  0.19562574 -0.09205664 -0.4205132 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 26 of episode: 93\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.11450998  0.00192048 -0.10046691 -0.15821207]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 27 of episode: 93\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.11454839  0.19832657 -0.10363115 -0.48082325]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 28 of episode: 93\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.11851493  0.394747   -0.11324761 -0.80428624]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 29 of episode: 93\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.12640986  0.20134474 -0.12933333 -0.54926485]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 30 of episode: 93\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.13043676  0.39802355 -0.14031863 -0.8797372 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 31 of episode: 93\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.13839723  0.20505825 -0.15791339 -0.63425255]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 32 of episode: 93\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.14249839  0.40198937 -0.17059843 -0.9722076 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 33 of episode: 93\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.15053818  0.20951623 -0.19004259 -0.737602  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 34 of episode: 93\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.1547285   0.01745582 -0.20479462 -0.5102349 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 35 of episode: 93\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.15507762 -0.17428194 -0.21499932 -0.28842807]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 94/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 94\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02727336 -0.20172954  0.03625453  0.32351476]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 94\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02323877 -0.00714208  0.04272482  0.04248186]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 94\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02309593  0.187342    0.04357446 -0.23642075]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 94\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02684277  0.3818152   0.03884605 -0.51504683]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 94\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03447907  0.57636917  0.02854511 -0.79523975]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 94\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.04600646  0.77108794  0.01264031 -1.0788078 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 94\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.06142822  0.96604073 -0.00893584 -1.3674974 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 94\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.08074903  0.77103174 -0.03628579 -1.0776228 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 94\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.09616967  0.9666137  -0.05783825 -1.3814683 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 94\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.11550194  0.7722593  -0.08546761 -1.1074191 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 94\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.13094713  0.57835835 -0.107616   -0.8427263 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 94\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.14251429  0.38485682 -0.12447052 -0.58573115]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 94\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.15021142  0.19167788 -0.13618514 -0.33470443]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 14 of episode: 94\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.15404499 -0.00126964 -0.14287923 -0.08787764]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 15 of episode: 94\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.1540196   0.19558051 -0.14463678 -0.42200777]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 16 of episode: 94\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.15793121  0.00277209 -0.15307695 -0.17819011]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 17 of episode: 94\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.15798664  0.1997155  -0.15664074 -0.51497984]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 18 of episode: 94\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.16198096  0.39665627 -0.16694035 -0.85263854]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 19 of episode: 94\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.16991408  0.5936124  -0.18399312 -1.1928184 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 20 of episode: 94\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.18178633  0.4012871  -0.20784947 -0.9629891 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 21 of episode: 94\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.18981208  0.20947187 -0.22710927 -0.74212766]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 95/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 95\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02727336  0.18847767  0.03625453 -0.26141977]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 95\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03104291  0.38306385  0.03102613 -0.5424507 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 95\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03870419  0.1875199   0.02017712 -0.24015574]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 95\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.04245459  0.38234788  0.015374   -0.5264066 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 95\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.05010155  0.18701302  0.00484587 -0.22891912]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 95\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 5.3841807e-02  3.8206539e-01  2.6748964e-04 -5.2006954e-01]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 95\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.06148311  0.57718354 -0.0101339  -0.8126682 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 95\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.07302678  0.7724428  -0.02638726 -1.1085213 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 95\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.08847564  0.96790147 -0.04855769 -1.4093642 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 95\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.10783368  0.77341425 -0.07674497 -1.1322476 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 95\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.12330196  0.5793762  -0.09938993 -0.86458814]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 95\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.13488948  0.7757004  -0.11668169 -1.1867936 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 95\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.15040348  0.58226854 -0.14041756 -0.93284535]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 14 of episode: 95\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.16204886  0.77897716 -0.15907447 -1.2661514 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 15 of episode: 95\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.1776284   0.97573316 -0.1843975  -1.60413   ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 16 of episode: 95\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.19714306  0.7832095  -0.2164801  -1.3741466 ]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 96/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 96\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02727336 -0.20172954  0.03625453  0.32351476]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 96\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02323877 -0.39734846  0.04272482  0.62740684]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 96\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.0152918  -0.5930399   0.05527296  0.9332334 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 96\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.003431   -0.39870548  0.07393762  0.6584189 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 96\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.00454311 -0.20468625  0.087106    0.38990307]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 96\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.00863683 -0.40092954  0.09490407  0.70872873]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 96\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.01665542 -0.20724133  0.10907864  0.44736376]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 96\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.02080025 -0.01381782  0.11802591  0.19095838]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 96\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.0210766   0.17943527  0.12184508 -0.06228545]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 96\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.0174879  -0.01720386  0.12059937  0.26622033]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 96\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.01783198 -0.21382211  0.12592378  0.5943759 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 96\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.02210842 -0.02066671  0.1378113   0.3438607 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 96\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.02252175  0.1722532   0.14468852  0.09761281]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 14 of episode: 96\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.01907669  0.36503705  0.14664076 -0.14615114]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 15 of episode: 96\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.01177595  0.5577879   0.14371775 -0.38921633]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 16 of episode: 96\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-6.2019052e-04  7.5060874e-01  1.3593341e-01 -6.3335806e-01]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 17 of episode: 96\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.01439198  0.5538786   0.12326626 -0.30114356]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 18 of episode: 96\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [0.02546956 0.3572351  0.11724339 0.02773218]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 19 of episode: 96\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [0.03261426 0.160644   0.11779803 0.35498506]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 20 of episode: 96\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [0.03582714 0.35391125 0.12489773 0.1016458 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 21 of episode: 96\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [0.04290536 0.15724124 0.12693065 0.4309774 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 22 of episode: 96\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [0.04605019 0.3503588  0.1355502  0.18084896]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 23 of episode: 96\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.05305737  0.543307    0.13916717 -0.06618865]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 24 of episode: 96\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [0.06392351 0.34649256 0.1378434  0.26696196]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 25 of episode: 96\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [0.07085335 0.5394056  0.14318264 0.02073489]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 26 of episode: 96\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [0.08164147 0.3425515  0.14359733 0.3549464 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 27 of episode: 96\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [0.0884925  0.53537095 0.15069626 0.11076683]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 28 of episode: 96\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [0.09919991 0.3384469  0.1529116  0.44694242]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 29 of episode: 96\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [0.10596886 0.14152995 0.16185045 0.78365237]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 30 of episode: 96\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [0.10879945 0.3341017  0.1775235  0.5459464 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 31 of episode: 96\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [0.11548149 0.13698809 0.18844242 0.8888883 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 32 of episode: 96\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.11822125 -0.06012236  0.2062202   1.2343946 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 33 of episode: 96\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [0.1170188  0.1318407  0.23090808 1.012751  ]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 97/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 97\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02727336 -0.20172954  0.03625453  0.32351476]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 97\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02323877 -0.00714208  0.04272482  0.04248186]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 97\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02309593  0.187342    0.04357446 -0.23642075]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 97\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02684277  0.3818152   0.03884605 -0.51504683]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 97\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03447907  0.57636917  0.02854511 -0.79523975]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 97\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.04600646  0.77108794  0.01264031 -1.0788078 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 97\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.06142822  0.96604073 -0.00893584 -1.3674974 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 97\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.08074903  0.77103174 -0.03628579 -1.0776228 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 97\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.09616967  0.5764074  -0.05783825 -0.7965441 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 97\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.10769781  0.38212478 -0.07376913 -0.5226025 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 97\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.11534031  0.18811445 -0.08422118 -0.2540469 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 97\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.1191026  -0.00571029 -0.08930212  0.01092794]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 97\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.11898839 -0.19944558 -0.08908356  0.27415293]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 14 of episode: 97\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.11499948 -0.00317298 -0.0836005  -0.04524447]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 15 of episode: 97\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.11493602  0.19304207 -0.08450539 -0.36308905]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 16 of episode: 97\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.11879686  0.38925704 -0.09176717 -0.6811763 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 17 of episode: 97\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.126582    0.58552563 -0.1053907  -1.0012821 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 18 of episode: 97\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.13829252  0.781886   -0.12541634 -1.3251159 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 19 of episode: 97\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.15393023  0.97834855 -0.15191865 -1.654271  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 20 of episode: 97\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.1734972   0.78529143 -0.18500407 -1.4125131 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 21 of episode: 97\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.18920304  0.5928809  -0.21325433 -1.1829039 ]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 98/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 98\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02727336 -0.20172954  0.03625453  0.32351476]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 98\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02323877 -0.39734846  0.04272482  0.62740684]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 98\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.0152918  -0.20284806  0.05527296  0.34847975]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 98\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.01123484 -0.39871082  0.06224256  0.65806717]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 98\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.00326062 -0.5946414   0.0754039   0.9696814 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 98\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.0086322  -0.40060818  0.09479753  0.7016064 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 98\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.01664437 -0.5969074   0.10882965  1.0225619 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 98\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.02858252 -0.40339017  0.1292809   0.76593614]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 98\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.03665032 -0.21026279  0.14459962  0.5165654 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 98\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.04085558 -0.40709317  0.15493092  0.85109395]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 98\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.04899744 -0.21438448  0.1719528   0.61086094]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 98\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.05328513 -0.02203004  0.18417002  0.3768898 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 98\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.05372573  0.17006396  0.19170782  0.14746207]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 14 of episode: 98\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [-0.05032445 -0.02721241  0.19465706  0.49397305]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 15 of episode: 98\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.0508687   0.1647085   0.20453651  0.26839706]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 16 of episode: 98\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [-0.04757453  0.3564138   0.20990446  0.04654849]\n",
      "Reward: 1.0\n",
      "\n",
      "-----------------------------------\n",
      "Episode 99/100\n",
      "-----------------------------------\n",
      "timestep: 1 of episode: 99\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02727336  0.18847767  0.03625453 -0.26141977]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 2 of episode: 99\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03104291 -0.00714255  0.03102613  0.04247424]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 3 of episode: 99\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03090006  0.18752107  0.03187562 -0.24026045]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 4 of episode: 99\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03465049 -0.00804138  0.02707041  0.06230407]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 5 of episode: 99\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03448966 -0.20354079  0.02831649  0.36340347]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 6 of episode: 99\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03041884 -0.00883249  0.03558456  0.07978204]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 7 of episode: 99\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.03024219 -0.20444602  0.0371802   0.38347623]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 8 of episode: 99\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02615327 -0.00987113  0.04484972  0.10274406]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 9 of episode: 99\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.02595585 -0.20560618  0.0469046   0.4092328 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 10 of episode: 99\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02184373 -0.01117953  0.05508926  0.13169847]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 11 of episode: 99\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02162014  0.18311179  0.05772323 -0.14310834]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 12 of episode: 99\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.02528237  0.3773616   0.05486107 -0.41703653]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 13 of episode: 99\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.0328296   0.18150681  0.04652033 -0.10757531]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 14 of episode: 99\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.03645974  0.3759323   0.04436883 -0.385226  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 15 of episode: 99\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.04397839  0.18020949  0.03666431 -0.07889052]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 16 of episode: 99\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.04758257  0.37478718  0.0350865  -0.359784  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 17 of episode: 99\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.05507832  0.1791845   0.02789082 -0.05624726]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 18 of episode: 99\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.05866201 -0.01632603  0.02676587  0.24510331]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 19 of episode: 99\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.05833549  0.1784036   0.03166794 -0.03901829]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 20 of episode: 99\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.06190356  0.37305745  0.03088757 -0.32154387]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 21 of episode: 99\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.06936471  0.17750958  0.02445669 -0.01928245]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 22 of episode: 99\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.0729149   0.3722724   0.02407105 -0.30414978]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 23 of episode: 99\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.08036035  0.5670432   0.01798805 -0.5891451 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 24 of episode: 99\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.09170121  0.37167403  0.00620515 -0.2908506 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 25 of episode: 99\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 9.9134691e-02  5.6670696e-01  3.8813584e-04 -5.8157003e-01]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 26 of episode: 99\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.11046883  0.7618235  -0.01124327 -0.87413067]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 27 of episode: 99\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.1257053   0.5668562  -0.02872588 -0.5850037 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 28 of episode: 99\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.13704243  0.37214813 -0.04042595 -0.30150634]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 29 of episode: 99\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.14448538  0.5678223  -0.04645608 -0.6066597 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 30 of episode: 99\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.15584183  0.3733796  -0.05858927 -0.32896358]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 31 of episode: 99\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.16330943  0.17913854 -0.06516854 -0.05531731]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 32 of episode: 99\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.1668922  -0.0149914  -0.06627489  0.21611348]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 33 of episode: 99\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.16659237 -0.20910637 -0.06195262  0.48717615]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 34 of episode: 99\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.16241024 -0.0131675  -0.0522091   0.17562926]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 35 of episode: 99\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.1621469   0.18266125 -0.04869651 -0.13305612]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 36 of episode: 99\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.16580011  0.37844568 -0.05135763 -0.44069576]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 37 of episode: 99\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.17336904  0.5742554  -0.06017155 -0.7491156 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 38 of episode: 99\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.18485413  0.77015346 -0.07515386 -1.0601107 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 39 of episode: 99\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.20025721  0.57610303 -0.09635608 -0.79193103]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 40 of episode: 99\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.21177927  0.3824267  -0.11219469 -0.531049  ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 41 of episode: 99\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.21942781  0.5789332  -0.12281568 -0.8568716 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 42 of episode: 99\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.23100647  0.77549523 -0.1399531  -1.1855109 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 43 of episode: 99\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.24651638  0.58243793 -0.16366333 -0.9397684 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 44 of episode: 99\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: [ 0.25816512  0.38985515 -0.1824587  -0.7026569 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 45 of episode: 99\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.26596224  0.58697385 -0.19651183 -1.0467726 ]\n",
      "Reward: 1.0\n",
      "\n",
      "timestep: 46 of episode: 99\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: [ 0.2777017   0.78408355 -0.21744728 -1.39415   ]\n",
      "Reward: 1.0\n",
      "\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "print(\"****************************************************\")\n",
    "print(\"Final reward after episode: {}, Return: {}\".format(num_episodes, RETURN))\n",
    "print(\"****************************************************\")"
   ],
   "id": "b65a5a460ce10c3b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************************\n",
      "Final reward after episode: 100, Return: 46.0\n",
      "****************************************************\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "55e33b8fc5ac58f9",
   "outputs": [],
   "execution_count": 8
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
