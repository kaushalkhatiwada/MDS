{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a0aff65a5901a05",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Explore Frozen Lake Environment\n",
    "\n",
    "Detail documentation on Frozen Lake environment [https://gymnasium.farama.org/environments/toy_text/frozen_lake/] "
   ]
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-02T14:56:20.211396Z",
     "start_time": "2024-06-02T14:56:19.959713Z"
    }
   },
   "source": [
    "# Import necessary library\n",
    "import gymnasium as gym\n",
    "\n",
    "# define seed\n",
    "SEED = 42"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "4cf6566aafba6c14",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-02T14:56:20.235880Z",
     "start_time": "2024-06-02T14:56:20.224088Z"
    }
   },
   "source": [
    "# Initialize the Frozen Lake Environment\n",
    "env = gym.make('FrozenLake-v1', map_name=\"4x4\", is_slippery=False, render_mode='ansi')"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "cb957ae0c430ed2e",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-02T14:56:20.241459Z",
     "start_time": "2024-06-02T14:56:20.238279Z"
    }
   },
   "source": [
    "# Reset an environment to its initial internal state\n",
    "obs, info = env.reset(seed=SEED)\n",
    "\n",
    "# Print the initial position of agent in the environment\n",
    "print(\"The initial observation is: {}\".format(obs))\n",
    "print(\"The information is : {}\".format(info))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The initial observation is: 0\n",
      "The information is : {'prob': 1}\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "a06d4d37cb26cdca",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-02T14:56:20.244758Z",
     "start_time": "2024-06-02T14:56:20.242351Z"
    }
   },
   "source": [
    "# Let's render the environment and observe the complete frozen lake environment\n",
    "print(env.render())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001B[41mS\u001B[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "id": "7b99bff5315b92ab",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Here, \n",
    "\n",
    "**S** refers to the starting state,\n",
    "**F** refers to the frozen state,\n",
    "**H** refers to the hole state, and,\n",
    "**G** refers to the goal state\n",
    "\n",
    "Our goal, here is to reach goal state **G** from the starting state **S** without visiting any hole state **H**. By any chance, if agent visits the hole state **H**, the agent will fall into the hole and die."
   ]
  },
  {
   "cell_type": "code",
   "id": "263fe55e78940356",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-02T14:56:20.249212Z",
     "start_time": "2024-06-02T14:56:20.246873Z"
    }
   },
   "source": [
    "# Print the Observation space (or state space) and action space \n",
    "print(\"The observation space: {}\".format(env.observation_space))\n",
    "print(\"The action space: {}\".format(env.action_space))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The observation space: Discrete(16)\n",
      "The action space: Discrete(4)\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "id": "da71a85188931744",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The observation space is also called **State Space**. And there are 16 discrete states in our environment starting from 0 (S) to 15 (G) (from left to right and top to bottom). And, in each state, an agent can take four action as defined by action space. They are:\n",
    "\n",
    "    0 => Left\n",
    "\n",
    "    1 => Down\n",
    "\n",
    "    2 => Right\n",
    "\n",
    "    3 => Up"
   ]
  },
  {
   "cell_type": "code",
   "id": "4b9d2b53f7d581f3",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-02T14:56:20.253916Z",
     "start_time": "2024-06-02T14:56:20.250203Z"
    }
   },
   "source": [
    "# map numbers to action\n",
    "action_map = {\n",
    "    0: \"left\",\n",
    "    1: \"down\",\n",
    "    2: \"right\",\n",
    "    3: \"up\"\n",
    "}\n",
    "\n",
    "# Let's take a random action now from the action space\n",
    "random_action = env.action_space.sample()\n",
    "\n",
    "# # Take the action and get the new observation space\n",
    "next_state, reward, done, info, transition_prob = env.step(random_action)\n",
    "print(\"Action: {}\".format(action_map[random_action]))\n",
    "print(\"Next State: {}\".format(next_state))\n",
    "print(\"Reward: {}\".format(reward))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: left\n",
      "Next State: 0\n",
      "Reward: 0.0\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Frozen Lake is the stochastic environment i.e. agent reach the next states with some probability known as **Transition probability**. The transition probability is obtained using `env.P[state][action]`.\n",
    "\n",
    "**Stochastic Environment**: In the stochastic environment, not always the agent will reach the next state s' exactly by taking someing action a. This is because there is some randomness associated with the stochastic environment and due to which the agent reaches the enxt state s' with some probability. "
   ],
   "id": "be94a91f2a2e7c5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-02T14:56:20.259255Z",
     "start_time": "2024-06-02T14:56:20.254703Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Transition probability in state 1 while taking 'right' action\n",
    "print(env.P[3][1])"
   ],
   "id": "f0fc9e01967e946f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1.0, 7, 0.0, True)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rajanadhikari/Documents/Projects/Reinforcement-Learning/venv/lib/python3.9/site-packages/gymnasium/core.py:311: UserWarning: \u001B[33mWARN: env.P to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.P` for environment variables or `env.get_wrapper_attr('P')` that will search the reminding wrappers.\u001B[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "63eecd55d971e214",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-02T14:56:20.262189Z",
     "start_time": "2024-06-02T14:56:20.260215Z"
    }
   },
   "source": [
    "# Let's render the environment and confirm the same in the environment\n",
    "print(env.render())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Left)\n",
      "\u001B[41mS\u001B[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "id": "fe74f3f50860d50a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## One Episode Random Walk\n",
    "Episode refers to the transition of agent from initial state to the final state. This contains the path of the transition from initial to final state for agent. Hence, also known as trajectory."
   ]
  },
  {
   "cell_type": "code",
   "id": "8885ce86ef59cbd7",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-02T14:56:20.266711Z",
     "start_time": "2024-06-02T14:56:20.262919Z"
    }
   },
   "source": [
    "# Number of times the agent moves\n",
    "num_timestep = 20\n",
    "\n",
    "# Reset the environment\n",
    "env.reset(seed=SEED)\n",
    "\n",
    "for i in range(num_timestep):\n",
    "    print(\"------- Step: {} --------\".format(i+1))\n",
    "        # Let's take a random action now from the action space\n",
    "    # Random action means we are taking random policy at the moment.\n",
    "    random_action = env.action_space.sample()\n",
    "    \n",
    "    # # Take the action and get the new observation space\n",
    "    next_state, reward, done, info, transition_prob = env.step(random_action)\n",
    "    \n",
    "    print(\"Action: {}\".format(action_map[random_action]))\n",
    "    print(\"Next State: {}\".format(next_state))\n",
    "    print(\"Reward: {}\".format(reward))\n",
    "    \n",
    "    print(env.render())\n",
    "    \n",
    "    # if the agent moves to hole state, then terminate\n",
    "    if done: \n",
    "        break"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Step: 1 --------\n",
      "Action: right\n",
      "Next State: 1\n",
      "Reward: 0.0\n",
      "  (Right)\n",
      "S\u001B[41mF\u001B[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "------- Step: 2 --------\n",
      "Action: up\n",
      "Next State: 1\n",
      "Reward: 0.0\n",
      "  (Up)\n",
      "S\u001B[41mF\u001B[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "------- Step: 3 --------\n",
      "Action: left\n",
      "Next State: 0\n",
      "Reward: 0.0\n",
      "  (Left)\n",
      "\u001B[41mS\u001B[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "------- Step: 4 --------\n",
      "Action: left\n",
      "Next State: 0\n",
      "Reward: 0.0\n",
      "  (Left)\n",
      "\u001B[41mS\u001B[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "------- Step: 5 --------\n",
      "Action: right\n",
      "Next State: 1\n",
      "Reward: 0.0\n",
      "  (Right)\n",
      "S\u001B[41mF\u001B[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "------- Step: 6 --------\n",
      "Action: down\n",
      "Next State: 5\n",
      "Reward: 0.0\n",
      "  (Down)\n",
      "SFFF\n",
      "F\u001B[41mH\u001B[0mFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "id": "cd9032570f515389",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "This completes one episode in the environment and we can run for a series of episodes as below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30888c2443ef18e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Multi-episode Random Walk\n",
    "Multiple episodes are important as agent can find the optimal policy by generating several episodes. "
   ]
  },
  {
   "cell_type": "code",
   "id": "464c056cc88d4bc5",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-02T14:56:20.274645Z",
     "start_time": "2024-06-02T14:56:20.267515Z"
    }
   },
   "source": [
    "# Number of times the agent moves\n",
    "num_timestep = 20\n",
    "\n",
    "# Number of episodes\n",
    "num_episodes = 10\n",
    "\n",
    "for e in range(num_episodes):\n",
    "    print(\"-----------------------------------\")\n",
    "    print(\"Episode {}/{}\".format(e, num_episodes))\n",
    "    print(\"-----------------------------------\")\n",
    "    # Reset the environment\n",
    "    env.reset(seed = SEED)\n",
    "    # We make 10 random walk in environment\n",
    "    for t in range(num_timestep):\n",
    "        print(\"timestep: {}\".format(t+1))\n",
    "        print(\"-----------------------------------------------------\")\n",
    "        # Let's take a random action now from the action space\n",
    "        # Random action means we are taking random policy at the moment.\n",
    "        random_action = env.action_space.sample()\n",
    "        \n",
    "        # # Take the action and get the new observation space\n",
    "        next_state, reward, done, info, transition_prob = env.step(random_action)\n",
    "        \n",
    "        print(\"Action: {}\".format(action_map[random_action]))\n",
    "        print(\"Next State: {}\".format(next_state))\n",
    "        print(\"Reward: {}\".format(reward))\n",
    "        print(\"\")\n",
    "        \n",
    "        print(env.render())\n",
    "        \n",
    "        # if the agent moves to hole state, then terminate\n",
    "        if done: \n",
    "            break"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "Episode 0/10\n",
      "-----------------------------------\n",
      "timestep: 1\n",
      "-----------------------------------------------------\n",
      "Action: up\n",
      "Next State: 0\n",
      "Reward: 0.0\n",
      "\n",
      "  (Up)\n",
      "\u001B[41mS\u001B[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 2\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: 1\n",
      "Reward: 0.0\n",
      "\n",
      "  (Right)\n",
      "S\u001B[41mF\u001B[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 3\n",
      "-----------------------------------------------------\n",
      "Action: up\n",
      "Next State: 1\n",
      "Reward: 0.0\n",
      "\n",
      "  (Up)\n",
      "S\u001B[41mF\u001B[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 4\n",
      "-----------------------------------------------------\n",
      "Action: down\n",
      "Next State: 5\n",
      "Reward: 0.0\n",
      "\n",
      "  (Down)\n",
      "SFFF\n",
      "F\u001B[41mH\u001B[0mFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "-----------------------------------\n",
      "Episode 1/10\n",
      "-----------------------------------\n",
      "timestep: 1\n",
      "-----------------------------------------------------\n",
      "Action: down\n",
      "Next State: 4\n",
      "Reward: 0.0\n",
      "\n",
      "  (Down)\n",
      "SFFF\n",
      "\u001B[41mF\u001B[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 2\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: 4\n",
      "Reward: 0.0\n",
      "\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001B[41mF\u001B[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 3\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: 5\n",
      "Reward: 0.0\n",
      "\n",
      "  (Right)\n",
      "SFFF\n",
      "F\u001B[41mH\u001B[0mFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "-----------------------------------\n",
      "Episode 2/10\n",
      "-----------------------------------\n",
      "timestep: 1\n",
      "-----------------------------------------------------\n",
      "Action: up\n",
      "Next State: 0\n",
      "Reward: 0.0\n",
      "\n",
      "  (Up)\n",
      "\u001B[41mS\u001B[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 2\n",
      "-----------------------------------------------------\n",
      "Action: down\n",
      "Next State: 4\n",
      "Reward: 0.0\n",
      "\n",
      "  (Down)\n",
      "SFFF\n",
      "\u001B[41mF\u001B[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 3\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: 4\n",
      "Reward: 0.0\n",
      "\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001B[41mF\u001B[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 4\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: 4\n",
      "Reward: 0.0\n",
      "\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001B[41mF\u001B[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 5\n",
      "-----------------------------------------------------\n",
      "Action: down\n",
      "Next State: 8\n",
      "Reward: 0.0\n",
      "\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001B[41mF\u001B[0mFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 6\n",
      "-----------------------------------------------------\n",
      "Action: up\n",
      "Next State: 4\n",
      "Reward: 0.0\n",
      "\n",
      "  (Up)\n",
      "SFFF\n",
      "\u001B[41mF\u001B[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 7\n",
      "-----------------------------------------------------\n",
      "Action: down\n",
      "Next State: 8\n",
      "Reward: 0.0\n",
      "\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001B[41mF\u001B[0mFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 8\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: 9\n",
      "Reward: 0.0\n",
      "\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001B[41mF\u001B[0mFH\n",
      "HFFG\n",
      "\n",
      "timestep: 9\n",
      "-----------------------------------------------------\n",
      "Action: down\n",
      "Next State: 13\n",
      "Reward: 0.0\n",
      "\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001B[41mF\u001B[0mFG\n",
      "\n",
      "timestep: 10\n",
      "-----------------------------------------------------\n",
      "Action: up\n",
      "Next State: 9\n",
      "Reward: 0.0\n",
      "\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001B[41mF\u001B[0mFH\n",
      "HFFG\n",
      "\n",
      "timestep: 11\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: 8\n",
      "Reward: 0.0\n",
      "\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001B[41mF\u001B[0mFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 12\n",
      "-----------------------------------------------------\n",
      "Action: down\n",
      "Next State: 12\n",
      "Reward: 0.0\n",
      "\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "\u001B[41mH\u001B[0mFFG\n",
      "\n",
      "-----------------------------------\n",
      "Episode 3/10\n",
      "-----------------------------------\n",
      "timestep: 1\n",
      "-----------------------------------------------------\n",
      "Action: down\n",
      "Next State: 4\n",
      "Reward: 0.0\n",
      "\n",
      "  (Down)\n",
      "SFFF\n",
      "\u001B[41mF\u001B[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 2\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: 4\n",
      "Reward: 0.0\n",
      "\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001B[41mF\u001B[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 3\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: 4\n",
      "Reward: 0.0\n",
      "\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001B[41mF\u001B[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 4\n",
      "-----------------------------------------------------\n",
      "Action: down\n",
      "Next State: 8\n",
      "Reward: 0.0\n",
      "\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001B[41mF\u001B[0mFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 5\n",
      "-----------------------------------------------------\n",
      "Action: up\n",
      "Next State: 4\n",
      "Reward: 0.0\n",
      "\n",
      "  (Up)\n",
      "SFFF\n",
      "\u001B[41mF\u001B[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 6\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: 5\n",
      "Reward: 0.0\n",
      "\n",
      "  (Right)\n",
      "SFFF\n",
      "F\u001B[41mH\u001B[0mFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "-----------------------------------\n",
      "Episode 4/10\n",
      "-----------------------------------\n",
      "timestep: 1\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: 0\n",
      "Reward: 0.0\n",
      "\n",
      "  (Left)\n",
      "\u001B[41mS\u001B[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 2\n",
      "-----------------------------------------------------\n",
      "Action: down\n",
      "Next State: 4\n",
      "Reward: 0.0\n",
      "\n",
      "  (Down)\n",
      "SFFF\n",
      "\u001B[41mF\u001B[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 3\n",
      "-----------------------------------------------------\n",
      "Action: down\n",
      "Next State: 8\n",
      "Reward: 0.0\n",
      "\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001B[41mF\u001B[0mFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 4\n",
      "-----------------------------------------------------\n",
      "Action: up\n",
      "Next State: 4\n",
      "Reward: 0.0\n",
      "\n",
      "  (Up)\n",
      "SFFF\n",
      "\u001B[41mF\u001B[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 5\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: 5\n",
      "Reward: 0.0\n",
      "\n",
      "  (Right)\n",
      "SFFF\n",
      "F\u001B[41mH\u001B[0mFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "-----------------------------------\n",
      "Episode 5/10\n",
      "-----------------------------------\n",
      "timestep: 1\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: 1\n",
      "Reward: 0.0\n",
      "\n",
      "  (Right)\n",
      "S\u001B[41mF\u001B[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 2\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: 0\n",
      "Reward: 0.0\n",
      "\n",
      "  (Left)\n",
      "\u001B[41mS\u001B[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 3\n",
      "-----------------------------------------------------\n",
      "Action: down\n",
      "Next State: 4\n",
      "Reward: 0.0\n",
      "\n",
      "  (Down)\n",
      "SFFF\n",
      "\u001B[41mF\u001B[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 4\n",
      "-----------------------------------------------------\n",
      "Action: up\n",
      "Next State: 0\n",
      "Reward: 0.0\n",
      "\n",
      "  (Up)\n",
      "\u001B[41mS\u001B[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 5\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: 0\n",
      "Reward: 0.0\n",
      "\n",
      "  (Left)\n",
      "\u001B[41mS\u001B[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 6\n",
      "-----------------------------------------------------\n",
      "Action: up\n",
      "Next State: 0\n",
      "Reward: 0.0\n",
      "\n",
      "  (Up)\n",
      "\u001B[41mS\u001B[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 7\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: 0\n",
      "Reward: 0.0\n",
      "\n",
      "  (Left)\n",
      "\u001B[41mS\u001B[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 8\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: 0\n",
      "Reward: 0.0\n",
      "\n",
      "  (Left)\n",
      "\u001B[41mS\u001B[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 9\n",
      "-----------------------------------------------------\n",
      "Action: up\n",
      "Next State: 0\n",
      "Reward: 0.0\n",
      "\n",
      "  (Up)\n",
      "\u001B[41mS\u001B[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 10\n",
      "-----------------------------------------------------\n",
      "Action: up\n",
      "Next State: 0\n",
      "Reward: 0.0\n",
      "\n",
      "  (Up)\n",
      "\u001B[41mS\u001B[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 11\n",
      "-----------------------------------------------------\n",
      "Action: up\n",
      "Next State: 0\n",
      "Reward: 0.0\n",
      "\n",
      "  (Up)\n",
      "\u001B[41mS\u001B[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 12\n",
      "-----------------------------------------------------\n",
      "Action: down\n",
      "Next State: 4\n",
      "Reward: 0.0\n",
      "\n",
      "  (Down)\n",
      "SFFF\n",
      "\u001B[41mF\u001B[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 13\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: 5\n",
      "Reward: 0.0\n",
      "\n",
      "  (Right)\n",
      "SFFF\n",
      "F\u001B[41mH\u001B[0mFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "-----------------------------------\n",
      "Episode 6/10\n",
      "-----------------------------------\n",
      "timestep: 1\n",
      "-----------------------------------------------------\n",
      "Action: up\n",
      "Next State: 0\n",
      "Reward: 0.0\n",
      "\n",
      "  (Up)\n",
      "\u001B[41mS\u001B[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 2\n",
      "-----------------------------------------------------\n",
      "Action: down\n",
      "Next State: 4\n",
      "Reward: 0.0\n",
      "\n",
      "  (Down)\n",
      "SFFF\n",
      "\u001B[41mF\u001B[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 3\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: 4\n",
      "Reward: 0.0\n",
      "\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001B[41mF\u001B[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 4\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: 5\n",
      "Reward: 0.0\n",
      "\n",
      "  (Right)\n",
      "SFFF\n",
      "F\u001B[41mH\u001B[0mFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "-----------------------------------\n",
      "Episode 7/10\n",
      "-----------------------------------\n",
      "timestep: 1\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: 1\n",
      "Reward: 0.0\n",
      "\n",
      "  (Right)\n",
      "S\u001B[41mF\u001B[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 2\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: 2\n",
      "Reward: 0.0\n",
      "\n",
      "  (Right)\n",
      "SF\u001B[41mF\u001B[0mF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 3\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: 1\n",
      "Reward: 0.0\n",
      "\n",
      "  (Left)\n",
      "S\u001B[41mF\u001B[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 4\n",
      "-----------------------------------------------------\n",
      "Action: up\n",
      "Next State: 1\n",
      "Reward: 0.0\n",
      "\n",
      "  (Up)\n",
      "S\u001B[41mF\u001B[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 5\n",
      "-----------------------------------------------------\n",
      "Action: down\n",
      "Next State: 5\n",
      "Reward: 0.0\n",
      "\n",
      "  (Down)\n",
      "SFFF\n",
      "F\u001B[41mH\u001B[0mFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "-----------------------------------\n",
      "Episode 8/10\n",
      "-----------------------------------\n",
      "timestep: 1\n",
      "-----------------------------------------------------\n",
      "Action: up\n",
      "Next State: 0\n",
      "Reward: 0.0\n",
      "\n",
      "  (Up)\n",
      "\u001B[41mS\u001B[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 2\n",
      "-----------------------------------------------------\n",
      "Action: up\n",
      "Next State: 0\n",
      "Reward: 0.0\n",
      "\n",
      "  (Up)\n",
      "\u001B[41mS\u001B[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 3\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: 0\n",
      "Reward: 0.0\n",
      "\n",
      "  (Left)\n",
      "\u001B[41mS\u001B[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 4\n",
      "-----------------------------------------------------\n",
      "Action: up\n",
      "Next State: 0\n",
      "Reward: 0.0\n",
      "\n",
      "  (Up)\n",
      "\u001B[41mS\u001B[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 5\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: 1\n",
      "Reward: 0.0\n",
      "\n",
      "  (Right)\n",
      "S\u001B[41mF\u001B[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 6\n",
      "-----------------------------------------------------\n",
      "Action: left\n",
      "Next State: 0\n",
      "Reward: 0.0\n",
      "\n",
      "  (Left)\n",
      "\u001B[41mS\u001B[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 7\n",
      "-----------------------------------------------------\n",
      "Action: up\n",
      "Next State: 0\n",
      "Reward: 0.0\n",
      "\n",
      "  (Up)\n",
      "\u001B[41mS\u001B[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 8\n",
      "-----------------------------------------------------\n",
      "Action: down\n",
      "Next State: 4\n",
      "Reward: 0.0\n",
      "\n",
      "  (Down)\n",
      "SFFF\n",
      "\u001B[41mF\u001B[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 9\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: 5\n",
      "Reward: 0.0\n",
      "\n",
      "  (Right)\n",
      "SFFF\n",
      "F\u001B[41mH\u001B[0mFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "-----------------------------------\n",
      "Episode 9/10\n",
      "-----------------------------------\n",
      "timestep: 1\n",
      "-----------------------------------------------------\n",
      "Action: down\n",
      "Next State: 4\n",
      "Reward: 0.0\n",
      "\n",
      "  (Down)\n",
      "SFFF\n",
      "\u001B[41mF\u001B[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 2\n",
      "-----------------------------------------------------\n",
      "Action: down\n",
      "Next State: 8\n",
      "Reward: 0.0\n",
      "\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001B[41mF\u001B[0mFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 3\n",
      "-----------------------------------------------------\n",
      "Action: up\n",
      "Next State: 4\n",
      "Reward: 0.0\n",
      "\n",
      "  (Up)\n",
      "SFFF\n",
      "\u001B[41mF\u001B[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 4\n",
      "-----------------------------------------------------\n",
      "Action: down\n",
      "Next State: 8\n",
      "Reward: 0.0\n",
      "\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001B[41mF\u001B[0mFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 5\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: 9\n",
      "Reward: 0.0\n",
      "\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001B[41mF\u001B[0mFH\n",
      "HFFG\n",
      "\n",
      "timestep: 6\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: 10\n",
      "Reward: 0.0\n",
      "\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FF\u001B[41mF\u001B[0mH\n",
      "HFFG\n",
      "\n",
      "timestep: 7\n",
      "-----------------------------------------------------\n",
      "Action: up\n",
      "Next State: 6\n",
      "Reward: 0.0\n",
      "\n",
      "  (Up)\n",
      "SFFF\n",
      "FH\u001B[41mF\u001B[0mH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 8\n",
      "-----------------------------------------------------\n",
      "Action: down\n",
      "Next State: 10\n",
      "Reward: 0.0\n",
      "\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FF\u001B[41mF\u001B[0mH\n",
      "HFFG\n",
      "\n",
      "timestep: 9\n",
      "-----------------------------------------------------\n",
      "Action: up\n",
      "Next State: 6\n",
      "Reward: 0.0\n",
      "\n",
      "  (Up)\n",
      "SFFF\n",
      "FH\u001B[41mF\u001B[0mH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "timestep: 10\n",
      "-----------------------------------------------------\n",
      "Action: right\n",
      "Next State: 7\n",
      "Reward: 0.0\n",
      "\n",
      "  (Right)\n",
      "SFFF\n",
      "FHF\u001B[41mH\u001B[0m\n",
      "FFFH\n",
      "HFFG\n",
      "\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "id": "cba6a8cc004a0231",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Through random walk, as explored above, our agent mostly landed on Hole. Even if not and have reached the goal successfully, there is no policy learned that guarantees the agent will reach the goal. This is where we need a reinforcement learning to learn optimal policy that guides our agent in each state and helps it to reach the goal state. This optimal policy is learnt through various RL algorithm."
   ]
  },
  {
   "cell_type": "code",
   "id": "2d7c1d927f852a20",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-02T14:56:20.277309Z",
     "start_time": "2024-06-02T14:56:20.275674Z"
    }
   },
   "source": [],
   "outputs": [],
   "execution_count": 10
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
