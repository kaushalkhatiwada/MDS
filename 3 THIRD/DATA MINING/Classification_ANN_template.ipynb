{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Neural Network\n"
      ],
      "metadata": {
        "id": "9f1kj3BZaE2h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Problem Statement\n",
        "The **handwritten digit recognition problem** is a **classification task** where the goal is to correctly identify digits (0-9) from images of handwritten numbers. In the context of **neural networks**, this problem is typically approached using **supervised learning**, where a model learns from labeled images.\n",
        "\n",
        "### Problem Definition:\n",
        "Given an input image \\( X \\), the neural network must output a predicted digit \\( y \\), where:\n",
        "\n",
        "$ f(X) = y, \\quad y \\in \\{0, 1, 2, ..., 9\\}$\n",
        "\n",
        "The function \\( f(X) \\) is learned by training a neural network on a dataset of images with corresponding digit labels.\n",
        "\n",
        "### Neural Network Approach:\n",
        "1. Input Representation\n",
        "   - The input is a grayscale or color image of a digit, often resized to a fixed dimension (e.g., 28×28 pixels in MNIST).  \n",
        "   - The pixel values are normalized (e.g., scaled between 0 and 1 or standardized).\n",
        "\n",
        "2. Model Architecture\n",
        "   - **Fully Connected Neural Network (MLP)**: A simple feedforward network with input, hidden, and output layers.  \n",
        "   - **Convolutional Neural Network (CNN)**: Uses convolutional layers to capture spatial features like edges, curves, and shapes, improving accuracy.  \n",
        "   - **Recurrent Neural Network (RNN)**: Sometimes used when dealing with sequential handwritten strokes.\n",
        "\n",
        "3. Training Process\n",
        "   - The network learns by minimizing a loss function, such as **MSE, cross-entropy loss**, using an optimizer like **SGD** or **Adam**.  \n",
        "   - **Backpropagation** updates weights to improve prediction accuracy.\n",
        "\n",
        "4. Output Layer and Classification**  \n",
        "   - The final layer has **10 neurons**, each representing a digit (0-9).\n",
        "\n",
        "### **Datasets Used:**\n",
        "- **MNIST**: 60,000 training and 10,000 testing images (28x28 grayscale).\n",
        "- **EMNIST**: Extended version of MNIST.\n",
        "- **SVHN**: Street View House Numbers dataset with real-world images.\n"
      ],
      "metadata": {
        "id": "ZGv8E7Izax1o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use the numpy for the matrix computations\n",
        "\n",
        "**Note**: This is not a production level code and is only for the education purpose.\n"
      ],
      "metadata": {
        "id": "yyxmjdQBbb7w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset source\n",
        "\n",
        "The **MNIST (Modified National Institute of Standards and Technology) dataset** is a widely used benchmark dataset for handwritten digit recognition. It contains grayscale images of digits (0-9) and is commonly used to train and evaluate machine learning and neural network models.\n",
        "\n",
        "1. Image Characteristics\n",
        "   - Each image is **28×28 pixels** in size.  \n",
        "   - The images are **grayscale** with pixel intensity values ranging from **0 (black) to 255 (white)**.  \n",
        "   - The digits are **centered** and **normalized** within the images.\n",
        "\n",
        "2. Dataset Composition\n",
        "   - **Training set**: 60,000 images  \n",
        "   - **Test set**: 10,000 images  \n",
        "   - Each image is labeled with a digit from **0 to 9**.\n",
        "\n",
        "3. Class Distribution\n",
        "   - The dataset is **balanced**, meaning each digit (0-9) appears roughly the same number of times in both training and testing sets.\n",
        "\n",
        "4. Format\n",
        "   - The dataset is available in **IDX file format** (compressed binary format).  \n",
        "   - Can be loaded using machine learning libraries like **TensorFlow, PyTorch, Keras**, or via `sklearn.datasets.fetch_openml('mnist_784')`.\n",
        "\n",
        "5. Preprocessing Requirements\n",
        "   - The images are often **normalized** to values between **0 and 1** (by dividing pixel values by 255).  \n",
        "   - Flattening is required for **fully connected networks** (reshaping to a 1D vector of 784 features).  \n",
        "   - Noisy or augmented versions may be used to improve robustness.\n",
        "\n",
        "6. Challenges\n",
        "   - Some digits are difficult to distinguish due to **variations in Handwriting styles**.  \n",
        "   - The dataset is relatively simple, making it **less representative of real-world handwritten text recognition**.\n",
        "\n",
        "7. Variants\n",
        "   - **EMNIST**: An extension that includes letters and additional handwritten data.  \n",
        "   - **Fashion-MNIST**: A similar dataset but with clothing images instead of digits.\n",
        "\n",
        "The MNIST dataset remains a **standard benchmark** for testing neural network architectures, particularly **Convolutional Neural Networks (CNNs)**, and serves as a stepping stone for more complex image classification tasks.\n",
        "\n",
        "\n",
        "http://yann.lecun.com/exdb/mnist/\n",
        "\n",
        "https://github.com/mbornet-hl/MNIST\n",
        "\n",
        "https://github.com/rupakraj/machine-learning/raw/refs/heads/main/datasets/mnist.pkl.gz"
      ],
      "metadata": {
        "id": "xtlPrktwge_g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/rupakraj/machine-learning/raw/refs/heads/main/datasets/mnist.pkl.gz"
      ],
      "metadata": {
        "id": "Q-xP1VCOhevx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import gzip\n",
        "\n",
        "\n",
        "def load_data():\n",
        "    mnistFile = gzip.open('mnist.pkl.gz', 'rb')\n",
        "    mnistFileUnpicker = pickle._Unpickler(mnistFile)\n",
        "    mnistFileUnpicker.encoding = 'latin1'\n",
        "    training_data, validation_data, test_data = mnistFileUnpicker.load()\n",
        "    mnistFile.close()\n",
        "    return (training_data, validation_data, test_data)"
      ],
      "metadata": {
        "id": "dwvLGQKnfl_f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vectorizing the result $[0,0,0,1,0,0,0,0,0,0]$\n"
      ],
      "metadata": {
        "id": "ScrPNdIJe9cb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def vectorized_result(j):\n",
        "    e = np.zeros((10, 1))\n",
        "    e[j] = 1.0\n",
        "    return e"
      ],
      "metadata": {
        "id": "lt4wDbvSf2PN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prepare the dataset for the training"
      ],
      "metadata": {
        "id": "tko7WwSTgqu4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data_wrapper():\n",
        "    trainData, valData, testData = load_data()\n",
        "\n",
        "    # prepare the dataset\n",
        "\n",
        "    return (trainingData, validationData, testingData)"
      ],
      "metadata": {
        "id": "hiLm8QnOfxV7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementation of Neural Network"
      ],
      "metadata": {
        "id": "ZQAnbrbmiP1d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Class: `Network`**  \n",
        "This class represents a simple **feedforward neural network** that can be trained using **stochastic gradient descent (SGD)**.\n"
      ],
      "metadata": {
        "id": "TMjFPlEDjVr6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Constructor:  `__init__(self, sizes)`**\n",
        "\n",
        "*Initialize network structure*\n",
        "   - Store the number of layers (`num_layers`) from the given list `sizes`.\n",
        "   - Store the list `sizes`, which defines the number of neurons in each layer.\n",
        "   \n",
        "*Initialize biases and weights*\n",
        "   - Initialize biases randomly for all layers except the input layer.\n",
        "   - Initialize weights randomly for connections between layers.\n",
        "\n",
        "```plaintext\n",
        "1. Input: sizes (list of neurons in each layer)\n",
        "2. Set num_layers = length of sizes\n",
        "3. Set sizes = sizes\n",
        "4. Initialize biases as random values for all layers except input layer\n",
        "5. Initialize weights as random values between consecutive layers\n",
        "6. Return the network object\n",
        "```"
      ],
      "metadata": {
        "id": "1N255RePjuaB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**`feedforward(self, a)`**:\n",
        "This function computes the **output of the network** for a given input.\n",
        "\n",
        "**Algorithm:**\n",
        "1. For each layer in the network:\n",
        "   - Compute the weighted sum of inputs and biases (`z = w * a + b`).\n",
        "   - Apply the activation function (`sigmoid(z)`) to compute activations for the next layer.\n",
        "2. Return the final activation as the output.\n",
        "\n",
        "**Pseudo-code:**\n",
        "```plaintext\n",
        "1. Input: a (input vector)\n",
        "2. For each layer (b, w) in (biases, weights):\n",
        "   3. Compute z = w * a + b\n",
        "   4. Apply activation function: a = sigmoid(z)\n",
        "5. Return a (final output)\n",
        "```"
      ],
      "metadata": {
        "id": "0Woa-w-9lzu-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**`SGD(self, training_data, epochs, mini_batch_size, eta, test_data=None)`**\n",
        "This function trains the network using **Stochastic Gradient Descent (SGD)**.\n",
        "\n",
        "**Algorithm:**\n",
        "1. If test data is provided, store its size (`n_test`).\n",
        "2. Store training dataset size (`n`).\n",
        "3. Loop for `epochs` times:\n",
        "   - Shuffle the training data randomly.\n",
        "   - Split the data into `mini-batches`.\n",
        "   - For each `mini-batch`, update network parameters using **backpropagation**.\n",
        "   - If test data is provided, evaluate the model after each epoch.\n",
        "   - Print progress information.\n",
        "\n",
        "**Pseudo-code:**\n",
        "```plaintext\n",
        "1. Input: training_data, epochs, mini_batch_size, eta (learning rate), test_data (optional)\n",
        "2. If test_data exists, set n_test = length of test_data\n",
        "3. Set n = length of training_data\n",
        "4. Repeat for j in range(epochs):\n",
        "   5. Shuffle training_data\n",
        "   6. Create mini_batches of size mini_batch_size\n",
        "   7. For each mini_batch:\n",
        "      8. Update network parameters using update_mini_batch(mini_batch, eta)\n",
        "   9. If test_data exists:\n",
        "      10. Evaluate performance and print results\n",
        "   11. Else, print epoch completion time\n",
        "```"
      ],
      "metadata": {
        "id": "e9SKzI4TmT9R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**`update_mini_batch(self, mini_batch, eta)`**\n",
        "This function updates network **weights and biases** using mini-batch **gradient descent**.\n",
        "\n",
        "**Algorithm:**\n",
        "1. Initialize gradient accumulators (`nabla_b`, `nabla_w`) as zero matrices.\n",
        "2. For each `(x, y)` in the mini-batch:\n",
        "   - Compute **gradients** using backpropagation.\n",
        "   - Accumulate gradients over the mini-batch.\n",
        "3. Update weights and biases using the averaged gradients.\n",
        "\n",
        "**Pseudo-code:**\n",
        "```plaintext\n",
        "1. Input: mini_batch (list of (x, y)), eta (learning rate)\n",
        "2. Initialize nabla_b and nabla_w as zero matrices\n",
        "3. For each (x, y) in mini_batch:\n",
        "   4. Compute gradients delta_nabla_b, delta_nabla_w using backprop(x, y)\n",
        "   5. Accumulate gradients: nabla_b += delta_nabla_b, nabla_w += delta_nabla_w\n",
        "6. Update weights: w = w - (eta / mini_batch_size) * nabla_w\n",
        "7. Update biases: b = b - (eta / mini_batch_size) * nabla_b\n",
        "8. Return updated weights and biases\n",
        "```\n"
      ],
      "metadata": {
        "id": "_j8TteW_msLz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**`backprop(self, x, y)`**\n",
        "This function implements **Backpropagation** to compute gradients for updating weights and biases.\n",
        "\n",
        "**Algorithm:**\n",
        "1. **Forward pass:**\n",
        "   - Compute activations for each layer.\n",
        "   - Store intermediate results (`activations` and `zs`).\n",
        "\n",
        "2. **Backward pass:**\n",
        "   - Compute output layer error (`delta`).\n",
        "   - Compute gradients for biases and weights at the output layer.\n",
        "   - Backpropagate the error to earlier layers using **chain rule**.\n",
        "   - Compute gradients for all layers.\n",
        "\n",
        "3. Return computed gradients.\n",
        "\n",
        "**Pseudo-code:**\n",
        "```plaintext\n",
        "1. Input: x (input), y (true label)\n",
        "2. Initialize nabla_b and nabla_w as zero matrices\n",
        "3. Forward pass:\n",
        "   4. Set activation = x\n",
        "   5. For each layer (b, w):\n",
        "      6. Compute z = w * activation + b\n",
        "      7. Store z in zs\n",
        "      8. Compute activation = sigmoid(z)\n",
        "      9. Store activation in activations\n",
        "10. Backward pass:\n",
        "   11. Compute output error: delta = cost_derivative(output_activation, y) * sigmoid_prime(last z)\n",
        "   12. Compute gradient for last layer weights and biases\n",
        "   13. For each previous layer:\n",
        "      14. Compute delta = backpropagated error\n",
        "      15. Compute gradients for biases and weights\n",
        "16. Return (nabla_b, nabla_w)\n",
        "```\n"
      ],
      "metadata": {
        "id": "4MVkw1a6mwlt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6. `evaluate(self, test_data)`**\n",
        "This function evaluates the network's performance on test data.\n",
        "\n",
        "**Algorithm:**\n",
        "1. Compute predictions using `feedforward(x)`.\n",
        "2. Compare predicted labels with actual labels.\n",
        "3. Count the number of correct predictions.\n",
        "\n",
        "**Pseudo-code:**\n",
        "```plaintext\n",
        "1. Input: test_data (list of (x, y))\n",
        "2. For each (x, y) in test_data:\n",
        "   3. Compute predicted digit using argmax(feedforward(x))\n",
        "   4. Compare with y (true label)\n",
        "5. Return total correct predictions\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "eGp-b4xjiU87"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**7. `cost_derivative(self, output_activations, y)`**\n",
        "Computes the derivative of the cost function for backpropagation.\n",
        "\n",
        "**Algorithm:**\n",
        "1. Compute the difference between predicted output (`output_activations`) and actual target (`y`).\n",
        "\n",
        "**Pseudo-code:**\n",
        "```plaintext\n",
        "1. Input: output_activations, y\n",
        "2. Compute cost derivative: output_activations - y\n",
        "3. Return the computed derivative\n",
        "```"
      ],
      "metadata": {
        "id": "ySBXy-2MnZhF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import time"
      ],
      "metadata": {
        "id": "VwLPp9LWfoM4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Network(object):\n",
        "    def __init__(self, sizes):\n",
        "        pass\n",
        "\n",
        "    def SGD(self, training_data, epochs, mini_batch_size, eta, test_data=None):\n",
        "        if test_data: n_test = len(test_data)\n",
        "        n = len(training_data)\n",
        "        for j in range(epochs):\n",
        "            time1 = time.time()\n",
        "\n",
        "            # shuffle\n",
        "            # prepare the mini batches\n",
        "\n",
        "            for mini_batch in mini_batches:\n",
        "                self.update_mini_batch(mini_batch, eta)\n",
        "\n",
        "            time2 = time.time()\n",
        "\n",
        "            if test_data:\n",
        "                print(\"Epoch {0}: {1} / {2}, took {3:.2f} seconds\".format(\n",
        "                    j, self.evaluate(test_data), n_test, time2-time1))\n",
        "            else:\n",
        "                print(\"Epoch {0} complete in {1:.2f} seconds\".format(j, time2-time1))\n",
        "\n",
        "\n",
        "    def update_mini_batch(self, mini_batch, eta):\n",
        "        # nabla_b =\n",
        "        # nabla_w =\n",
        "\n",
        "        for x, y in mini_batch:\n",
        "            # backpropagate and update nabla_b and w\n",
        "\n",
        "\n",
        "        # update the weight and bias\n",
        "\n",
        "\n",
        "    def backprop(self, x, y):\n",
        "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
        "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
        "\n",
        "        # feedforward\n",
        "        activation = x\n",
        "        activations = [x]\n",
        "        zs = []\n",
        "        for b, w in zip(self.biases, self.weights):\n",
        "            # calculate activations\n",
        "\n",
        "        # backward pass\n",
        "        delta = self.cost_derivative(activations[-1], y) * \\\n",
        "            sigmoid_prime(zs[-1])\n",
        "        nabla_b[-1] = delta\n",
        "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
        "\n",
        "        for l in range(2, self.num_layers):\n",
        "            # error correctdion and compute nabla_a and w\n",
        "        return (nabla_b, nabla_w)\n",
        "\n",
        "\n",
        "    def feedforward(self, a):\n",
        "        # inference\n",
        "        pass\n",
        "\n",
        "\n",
        "    def evaluate(self, test_data):\n",
        "        # evaluate: only return number of correct samples\n",
        "\n",
        "    def cost_derivative(self, output_activations, y):\n",
        "        return (output_activations-y)"
      ],
      "metadata": {
        "id": "H5hFijQ6cI0b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nn = Network([784, 30, 10])"
      ],
      "metadata": {
        "id": "5bFSIeDndXK6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Neuran Activation and derivatives**\n",
        "\n",
        "**`sigmoid_prime(z)`**\n",
        "This function calculates the derivative of the **sigmoid activation function**, which is used in **backpropagation** to update weights and biases.\n",
        "\n",
        "\n",
        "**Algorithm:**\n",
        "1. Compute the sigmoid function:  \n",
        "   $ \\sigma(z) = \\frac{1}{1 + e^{-z}} $\n",
        "2. Compute the derivative using the formula:  \n",
        "   $ \\sigma'(z) = \\sigma(z) \\cdot (1 - \\sigma(z)) $\n",
        "3. Return the computed derivative.\n",
        "\n",
        "**Pseudo-code:**\n",
        "```plaintext\n",
        "1. Input: z (scalar or matrix)\n",
        "2. Compute sigmoid(z) = 1 / (1 + exp(-z))\n",
        "3. Compute derivative: sigmoid(z) * (1 - sigmoid(z))\n",
        "4. Return the computed value\n",
        "```\n",
        "\n",
        "This derivative is crucial in **gradient-based learning**, as it helps propagate the error during **backpropagation** by determining how much each weight should be adjusted."
      ],
      "metadata": {
        "id": "Zpsf8_0sowuL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(z):\n",
        "    return 1.0/(1.0+np.exp(-z))\n",
        "\n",
        "def sigmoid_prime(z):\n",
        "    return sigmoid(z)*(1-sigmoid(z))"
      ],
      "metadata": {
        "id": "DZkr1YosdGzv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cost_derivative(self, output_activations, y):\n",
        "    return (output_activations-y)\n",
        "\n",
        "nn.cost_derivative = cost_derivative"
      ],
      "metadata": {
        "id": "DOeEKIHBeJWl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_data, validation_data, test_data = load_data_wrapper()"
      ],
      "metadata": {
        "id": "5pOcxLYzhStz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "recognizer = Network([784, 50, 10])"
      ],
      "metadata": {
        "id": "D4QG1Okphpyg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "recognizer.SGD(training_data, 1, 500, 3.0, test_data=test_data)"
      ],
      "metadata": {
        "id": "xamCBhGRhuBY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fun Task 1: Train the neural network for adding two digits"
      ],
      "metadata": {
        "id": "mE2475f6vGOa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "def generate_unique_triples(n=500):\n",
        "    unique_triples = set()\n",
        "    while len(unique_triples) < n:\n",
        "        a = random.randint(1, 500)\n",
        "        b = random.randint(1, 500)\n",
        "        c = a + b\n",
        "        unique_triples.add((a, b, c))\n",
        "    return list(unique_triples)\n",
        "\n",
        "# Generate 500 unique numbers\n",
        "unique_numbers = generate_unique_triples()\n",
        "print(unique_numbers[:10])  # Print first 10 triples as an example"
      ],
      "metadata": {
        "id": "UcK0V5ChvKV5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task: Work with real data: try to use  Disease Diagnosis (Medical Data)\n",
        "\n",
        "- Dataset: UCI Heart Disease Dataset\n",
        "\n",
        "    https://www.kaggle.com/datasets/redwankarimsony/heart-disease-data/data\n",
        "\n",
        "- Features: Age, cholesterol levels, blood pressure, heart rate, etc.\n",
        "- Classes: Disease present vs. Disease absent\n"
      ],
      "metadata": {
        "id": "2fGyZR4mxfWQ"
      }
    }
  ]
}