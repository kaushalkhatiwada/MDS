{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3214ae33",
   "metadata": {},
   "source": [
    "### About the project\n",
    "The idea is to do basic ETL demo for a BigData Project. \n",
    "\n",
    "The Process Flows like this.\n",
    "\n",
    "- Raw CSV Data ---> PySpark ---> Basic Transformation ---> Load to MongoDB ---> Read From MongoDB ---> Load to HBase ---> Do Data Analysis\n",
    "\n",
    "The system setup is like this:\n",
    "- PySpark (locally setup, used for raw data processing)\n",
    "- MongoDB (locally setup, used to store the process data into the database)\n",
    "- HBase (setup in docker, user for data analysis)\n",
    "\n",
    "About the dataset:\n",
    "- TLC Trip Record Data\n",
    "    - Yellow and green taxi trip records include fields capturing pick-up and drop-off dates/times, pick-up and drop-off locations, trip distances, itemized fares, rate types, payment types, and driver-reported passenger counts. The data used in the attached datasets were collected and provided to the NYC Taxi and Limousine Commission (TLC) by technology providers authorized under the Taxicab & Livery Passenger Enhancement Programs (TPEP/LPEP). The trip data was not created by the TLC, and TLC makes no representations as to the accuracy of these data.\n",
    "\n",
    "    - For-Hire Vehicle (“FHV”) trip records include fields capturing the dispatching base license number and the pick-up date, time, and taxi zone location ID (shape file below). These records are generated from the FHV Trip Record submissions made by bases. Note: The TLC publishes base trip record data as submitted by the bases, and we cannot guarantee or confirm their accuracy or completeness. Therefore, this may not represent the total amount of trips dispatched by all TLC-licensed bases. The TLC performs routine reviews of the records and takes enforcement actions when necessary to ensure, to the extent possible, complete and accurate information.\n",
    "\n",
    "    - The dataset was in parquet format, which is then converted to CSV via pyspark.\n",
    "\n",
    "    - The dataset contains 3475226 records.\n",
    "\n",
    "Here's a table for the NYC Taxi Trip data description:\n",
    "\n",
    "| Field Name | Description |\n",
    "|------------|-------------|\n",
    "| **VendorID** | A code indicating the TPEP provider that provided the record.<br>• 1 = Creative Mobile Technologies, LLC<br>• 2 = Curb Mobility, LLC<br>• 6 = Myle Technologies Inc<br>• 7 = Helix tpep |\n",
    "| **tpep_pickup_datetime** | The date and time when the meter was engaged. |\n",
    "| **tpep_dropoff_datetime** | The date and time when the meter was disengaged. |\n",
    "| **passenger_count** | The number of passengers in the vehicle. |\n",
    "| **trip_distance** | The elapsed trip distance in miles reported by the taximeter. |\n",
    "| **RatecodeID** | The final rate code in effect at the end of the trip.<br>• 1 = Standard rate<br>• 2 = JFK<br>• 3 = Newark<br>• 4 = Nassau or Westchester<br>• 5 = Negotiated fare<br>• 6 = Group ride<br>• 99 = Null/unknown |\n",
    "| **store_and_fwd_flag** | This flag indicates whether the trip record was held in vehicle memory before sending to the vendor.<br>• Y = store and forward trip<br>• N = not a store and forward trip |\n",
    "| **PULocationID** | TLC Taxi Zone in which the taximeter was engaged. |\n",
    "| **DOLocationID** | TLC Taxi Zone in which the taximeter was disengaged. |\n",
    "| **payment_type** | A numeric code signifying how the passenger paid for the trip.<br>• 0 = Flex Fare trip<br>• 1 = Credit card<br>• 2 = Cash<br>• 3 = No charge<br>• 4 = Dispute<br>• 5 = Unknown<br>• 6 = Voided trip |\n",
    "| **fare_amount** | The time-and-distance fare calculated by the meter. |\n",
    "| **extra** | Miscellaneous extras and surcharges. |\n",
    "| **mta_tax** | Tax that is automatically triggered based on the metered rate in use. |\n",
    "| **tip_amount** | Tip amount – This field is automatically populated for credit card tips. Cash tips are not included. |\n",
    "| **tolls_amount** | Total amount of all tolls paid in trip. |\n",
    "| **improvement_surcharge** | Improvement surcharge assessed trips at the flag drop. The improvement surcharge began being levied in 2015. |\n",
    "| **total_amount** | The total amount charged to passengers. Does not include cash tips. |\n",
    "| **congestion_surcharge** | Total amount collected in trip for NYS congestion surcharge. |\n",
    "| **airport_fee** | For pick up only at LaGuardia and John F. Kennedy Airports. |\n",
    "| **cbd_congestion_fee** | Per-trip charge for MTA's Congestion Relief Zone starting Jan. 5, 2025. |\n",
    "\n",
    "\n",
    "Note that: Spark out of the box doesn't have support for HBase, so I had to use `happybase` library to connect to HBase and do the data analysis, because of the CPU restriction and time dependencies, the dataset used is random 100000 records of original dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69872995",
   "metadata": {},
   "source": [
    "## Part I: Data Ingestion to MongoDB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206a7d57",
   "metadata": {},
   "source": [
    "#### Load Necessary Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "846578bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n",
      "25/04/15 22:35:10 WARN Utils: Your hostname, DESKTOP-U7R862J resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "25/04/15 22:35:10 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/suman/.ivy2/cache\n",
      "The jars for the packages stored in: /home/suman/.ivy2/jars\n",
      "org.mongodb.spark#mongo-spark-connector_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-ee934838-392a-482d-8c3f-392062220cdb;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.mongodb.spark#mongo-spark-connector_2.12;3.0.1 in central\n",
      "\tfound org.mongodb#mongodb-driver-sync;4.0.5 in central\n",
      "\tfound org.mongodb#bson;4.0.5 in central\n",
      "\tfound org.mongodb#mongodb-driver-core;4.0.5 in central\n",
      ":: resolution report :: resolve 288ms :: artifacts dl 11ms\n",
      "\t:: modules in use:\n",
      "\torg.mongodb#bson;4.0.5 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-core;4.0.5 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-sync;4.0.5 from central in [default]\n",
      "\torg.mongodb.spark#mongo-spark-connector_2.12;3.0.1 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   4   |   0   |   0   |   0   ||   4   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-ee934838-392a-482d-8c3f-392062220cdb\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 4 already retrieved (0kB/8ms)\n",
      "25/04/15 22:35:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_timestamp, hour, dayofweek, month, year, date_format, rand\n",
    "import pymongo\n",
    "import happybase\n",
    "import uuid\n",
    "from datetime import datetime\n",
    "import time\n",
    "import json\n",
    "import happybase\n",
    "import pymongo\n",
    "import uuid\n",
    "import time\n",
    "import concurrent.futures\n",
    "from datetime import datetime\n",
    "import multiprocessing\n",
    "from threading import Lock\n",
    "import queue\n",
    "import itertools\n",
    "import threading\n",
    "\n",
    "# Initialize Spark with proper configurations\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"NYC Taxi Data Pipeline\") \\\n",
    "    .config(\"spark.mongodb.output.uri\", \"mongodb://localhost:27017/bigdata_demo.raw_taxi_data\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.mongodb.spark:mongo-spark-connector_2.12:3.0.1\") \\\n",
    "    .config(\"spark.mongodb.input.uri\", \"mongodb://localhost:27017/bigdata_demo.raw_taxi_data\") \\\n",
    "    .getOrCreate()\n",
    "    \n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1dd2cd",
   "metadata": {},
   "source": [
    "#### Load the the Yellow Trip CSV file into Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47d578ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|Airport_fee|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "|       2| 2025-01-17 18:08:04|  2025-01-17 18:19:55|           NULL|          0.0|      NULL|              NULL|         161|         186|           0|      13.22|  0.0|    0.5|       0.0|         0.0|                  1.0|       17.97|                NULL|       NULL|\n",
      "|       2| 2025-01-17 18:16:20|  2025-01-17 18:33:22|           NULL|         0.01|      NULL|              NULL|         234|         143|           0|      21.09|  0.0|    0.5|       0.0|         0.0|                  1.0|       25.84|                NULL|       NULL|\n",
      "|       1| 2025-01-17 18:01:26|  2025-01-17 18:22:49|           NULL|          6.7|      NULL|              NULL|         231|         141|           0|       25.7|  0.0|    0.5|       0.0|         0.0|                  1.0|       30.45|                NULL|       NULL|\n",
      "|       2| 2025-01-17 18:51:18|  2025-01-17 19:11:39|           NULL|          0.0|      NULL|              NULL|         234|         239|           0|      -4.75|  0.0|    0.5|       0.0|         0.0|                  1.0|        6.37|                NULL|       NULL|\n",
      "|       2| 2025-01-17 18:14:51|  2025-01-17 18:25:51|           NULL|          0.0|      NULL|              NULL|         193|         179|           0|      10.04|  0.0|    0.5|       0.0|         0.0|                  1.0|       11.54|                NULL|       NULL|\n",
      "|       2| 2025-01-17 18:06:08|  2025-01-17 18:11:01|           NULL|          0.0|      NULL|              NULL|         164|         164|           0|       8.11|  0.0|    0.5|       0.0|         0.0|                  1.0|       12.86|                NULL|       NULL|\n",
      "|       1| 2025-01-17 18:52:22|  2025-01-17 18:58:24|           NULL|          0.8|      NULL|              NULL|         114|         113|           0|       6.03|  0.0|    0.5|       0.0|         0.0|                  1.0|       10.78|                NULL|       NULL|\n",
      "|       2| 2025-01-17 18:26:00|  2025-01-17 18:54:38|           NULL|          0.0|      NULL|              NULL|          74|         209|           0|      32.62|  0.0|    0.5|       0.0|         0.0|                  1.0|       37.37|                NULL|       NULL|\n",
      "|       2| 2025-01-17 18:39:09|  2025-01-17 18:39:25|           NULL|          0.0|      NULL|              NULL|          88|          88|           0|      -4.75|  0.0|    0.5|       0.0|         0.0|                  1.0|         3.0|                NULL|       NULL|\n",
      "|       2| 2025-01-17 18:47:11|  2025-01-17 19:06:21|           NULL|          0.0|      NULL|              NULL|         261|         114|           0|      16.88|  0.0|    0.5|       0.0|         0.0|                  1.0|       21.63|                NULL|       NULL|\n",
      "|       2| 2025-01-17 18:38:52|  2025-01-17 18:55:25|           NULL|          0.0|      NULL|              NULL|         262|          74|           0|       -4.0|  0.0|    0.5|       0.0|         0.0|                  1.0|        2.37|                NULL|       NULL|\n",
      "|       2| 2025-01-17 18:13:53|  2025-01-17 18:39:08|           NULL|          0.0|      NULL|              NULL|         141|         144|           0|      -4.75|  0.0|    0.5|       0.0|         0.0|                  1.0|        6.39|                NULL|       NULL|\n",
      "|       2| 2025-01-17 18:56:20|  2025-01-17 19:15:13|           NULL|          0.0|      NULL|              NULL|         116|         140|           0|       24.8|  0.0|    0.5|       0.0|         0.0|                  1.0|        28.8|                NULL|       NULL|\n",
      "|       2| 2025-01-17 18:20:10|  2025-01-17 18:44:13|           NULL|         0.01|      NULL|              NULL|         246|         116|           0|       27.3|  0.0|    0.5|       0.0|         0.0|                  1.0|       32.05|                NULL|       NULL|\n",
      "|       2| 2025-01-17 18:05:25|  2025-01-17 18:18:38|           NULL|         0.01|      NULL|              NULL|         186|         137|           0|      -4.75|  0.0|    0.5|       0.0|         0.0|                  1.0|        3.19|                NULL|       NULL|\n",
      "|       2| 2025-01-17 18:12:43|  2025-01-17 18:29:15|           NULL|          0.0|      NULL|              NULL|         237|         148|           0|      27.31|  0.0|    0.5|       0.0|         0.0|                  1.0|       32.06|                NULL|       NULL|\n",
      "|       2| 2025-01-17 18:49:53|  2025-01-17 19:03:13|           NULL|          0.0|      NULL|              NULL|         234|          79|           0|      13.99|  0.0|    0.5|       0.0|         0.0|                  1.0|       18.74|                NULL|       NULL|\n",
      "|       2| 2025-01-17 18:20:34|  2025-01-17 18:38:37|           NULL|          0.0|      NULL|              NULL|          43|         186|           0|      19.14|  0.0|    0.5|       0.0|         0.0|                  1.0|       23.89|                NULL|       NULL|\n",
      "|       2| 2025-01-17 18:37:50|  2025-01-17 18:52:43|           NULL|          0.0|      NULL|              NULL|         107|         141|           0|      16.37|  0.0|    0.5|       0.0|         0.0|                  1.0|       21.12|                NULL|       NULL|\n",
      "|       2| 2025-01-17 18:15:36|  2025-01-17 18:34:02|           NULL|          0.0|      NULL|              NULL|         141|         107|           0|      16.58|  0.0|    0.5|       0.0|         0.0|                  1.0|       21.33|                NULL|       NULL|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "taxi_df  = spark.read.csv(\"yellow_tripdata_2025-01.csv\", inferSchema=True, header=True)\n",
    "taxi_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247c24cc",
   "metadata": {},
   "source": [
    "#### Check the schema: it should automatically infer the schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4327a6fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Schema:\n",
      "root\n",
      " |-- VendorID: integer (nullable = true)\n",
      " |-- tpep_pickup_datetime: timestamp (nullable = true)\n",
      " |-- tpep_dropoff_datetime: timestamp (nullable = true)\n",
      " |-- passenger_count: integer (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- RatecodeID: integer (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- payment_type: integer (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- congestion_surcharge: double (nullable = true)\n",
      " |-- Airport_fee: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display the schema\n",
    "print(\"Dataset Schema:\")\n",
    "taxi_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b647bcc0",
   "metadata": {},
   "source": [
    "#### Basic Data Manupulation: Renaming Column Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2deee631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing data cleaning and transformations...\n"
     ]
    }
   ],
   "source": [
    "print(\"Performing data cleaning and transformations...\")\n",
    "cleaned_df = taxi_df \\\n",
    "    .withColumn(\"pickup_datetime\", to_timestamp(col(\"tpep_pickup_datetime\"))) \\\n",
    "    .withColumn(\"dropoff_datetime\", to_timestamp(col(\"tpep_dropoff_datetime\"))) \\\n",
    "    .withColumn(\"pickup_hour\", hour(col(\"pickup_datetime\"))) \\\n",
    "    .withColumn(\"pickup_day\", dayofweek(col(\"pickup_datetime\"))) \\\n",
    "    .withColumn(\"pickup_month\", month(col(\"pickup_datetime\"))) \\\n",
    "    .withColumn(\"pickup_year\", year(col(\"pickup_datetime\"))) \\\n",
    "    .withColumn(\"trip_duration_minutes\", (col(\"dropoff_datetime\").cast(\"long\") - col(\"pickup_datetime\").cast(\"long\")) / 60) \\\n",
    "    .withColumn(\"date\", date_format(col(\"pickup_datetime\"), \"yyyy-MM-dd\")) \\\n",
    "    .drop(\"tpep_pickup_datetime\", \"tpep_dropoff_datetime\")  # Drop original timestamp columns\n",
    "    \n",
    "    \n",
    "# I have selected random 100000 samples to handle the large dataset on convservative system (low ram and time is slow processing all 3.7 mil records)\n",
    "cleaned_df = cleaned_df.orderBy(rand()).limit(100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7571ac30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of transformed data:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+-------------------+-------------------+-----------+----------+------------+-----------+---------------------+----------+\n",
      "|VendorID|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|Airport_fee|    pickup_datetime|   dropoff_datetime|pickup_hour|pickup_day|pickup_month|pickup_year|trip_duration_minutes|      date|\n",
      "+--------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+-------------------+-------------------+-----------+----------+------------+-----------+---------------------+----------+\n",
      "|       2|              1|         4.78|         1|                 N|         164|          24|           1|       24.0|  1.0|    0.5|      5.95|         0.0|                  1.0|        35.7|                 2.5|        0.0|2025-01-18 21:24:44|2025-01-18 21:45:48|         21|         7|           1|       2025|   21.066666666666666|2025-01-18|\n",
      "|       2|              1|         1.14|         1|                 N|         229|         170|           1|       10.0|  1.0|    0.5|       3.0|         0.0|                  1.0|        18.0|                 2.5|        0.0|2025-01-04 21:11:11|2025-01-04 21:20:21|         21|         7|           1|       2025|    9.166666666666666|2025-01-04|\n",
      "|       2|              2|         1.99|         1|                 N|         231|          13|           1|       12.8|  1.0|    0.5|      3.71|         0.0|                  1.0|       22.26|                 2.5|        0.0|2025-01-16 22:38:30|2025-01-16 22:48:20|         22|         5|           1|       2025|    9.833333333333334|2025-01-16|\n",
      "|       2|              1|          0.7|         1|                 N|         234|         137|           1|        7.2|  0.0|    0.5|      2.39|         0.0|                  1.0|       14.34|                 2.5|        0.0|2025-01-07 08:21:27|2025-01-07 08:27:29|          8|         3|           1|       2025|    6.033333333333333|2025-01-07|\n",
      "|       2|              2|         1.98|         1|                 N|         236|         239|           1|       12.1|  1.0|    0.5|      3.42|         0.0|                  1.0|       20.52|                 2.5|        0.0|2025-01-28 23:20:06|2025-01-28 23:29:05|         23|         3|           1|       2025|    8.983333333333333|2025-01-28|\n",
      "+--------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+-------------------+-------------------+-----------+----------+------------+-----------+---------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Sample of transformed data:\")\n",
    "cleaned_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64211d6f",
   "metadata": {},
   "source": [
    "#### Load the cleaned data in MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "209977e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing data to MongoDB...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully loaded into MongoDB collection: raw_taxi_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(\"Writing data to MongoDB...\")\n",
    "cleaned_df.write.format(\"mongo\").mode(\"overwrite\").save()\n",
    "print(\"Data successfully loaded into MongoDB collection: raw_taxi_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef66ef1b",
   "metadata": {},
   "source": [
    "## Part II: Load the data from local database, i.e. MongoDB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666b3a47",
   "metadata": {},
   "source": [
    "#### Load via Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ee4fbf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from MongoDB...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Load data from MongoDB\n",
    "print(\"Loading data from MongoDB...\")\n",
    "mongo_df = spark.read.format(\"mongo\").load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df68be4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample data from MongoDB:\n",
      "+-----------+------------+------------+----------+--------+--------------------+--------------------+----------+-------------------+-----+-----------+---------------------+-------+---------------+------------+-------------------+----------+-----------+------------+-----------+------------------+----------+------------+------------+-------------+---------------------+\n",
      "|Airport_fee|DOLocationID|PULocationID|RatecodeID|VendorID|                 _id|congestion_surcharge|      date|   dropoff_datetime|extra|fare_amount|improvement_surcharge|mta_tax|passenger_count|payment_type|    pickup_datetime|pickup_day|pickup_hour|pickup_month|pickup_year|store_and_fwd_flag|tip_amount|tolls_amount|total_amount|trip_distance|trip_duration_minutes|\n",
      "+-----------+------------+------------+----------+--------+--------------------+--------------------+----------+-------------------+-----+-----------+---------------------+-------+---------------+------------+-------------------+----------+-----------+------------+-----------+------------------+----------+------------+------------+-------------+---------------------+\n",
      "|        0.0|          24|         164|         1|       2|{67fe8e6c9f06d84a...|                 2.5|2025-01-18|2025-01-18 21:45:48|  1.0|       24.0|                  1.0|    0.5|              1|           1|2025-01-18 21:24:44|         7|         21|           1|       2025|                 N|      5.95|         0.0|        35.7|         4.78|   21.066666666666666|\n",
      "|        0.0|         170|         229|         1|       2|{67fe8e6c9f06d84a...|                 2.5|2025-01-04|2025-01-04 21:20:21|  1.0|       10.0|                  1.0|    0.5|              1|           1|2025-01-04 21:11:11|         7|         21|           1|       2025|                 N|       3.0|         0.0|        18.0|         1.14|    9.166666666666666|\n",
      "|        0.0|          13|         231|         1|       2|{67fe8e6c9f06d84a...|                 2.5|2025-01-16|2025-01-16 22:48:20|  1.0|       12.8|                  1.0|    0.5|              2|           1|2025-01-16 22:38:30|         5|         22|           1|       2025|                 N|      3.71|         0.0|       22.26|         1.99|    9.833333333333334|\n",
      "|        0.0|         137|         234|         1|       2|{67fe8e6c9f06d84a...|                 2.5|2025-01-07|2025-01-07 08:27:29|  0.0|        7.2|                  1.0|    0.5|              1|           1|2025-01-07 08:21:27|         3|          8|           1|       2025|                 N|      2.39|         0.0|       14.34|          0.7|    6.033333333333333|\n",
      "|        0.0|         239|         236|         1|       2|{67fe8e6c9f06d84a...|                 2.5|2025-01-28|2025-01-28 23:29:05|  1.0|       12.1|                  1.0|    0.5|              2|           1|2025-01-28 23:20:06|         3|         23|           1|       2025|                 N|      3.42|         0.0|       20.52|         1.98|    8.983333333333333|\n",
      "+-----------+------------+------------+----------+--------+--------------------+--------------------+----------+-------------------+-----+-----------+---------------------+-------+---------------+------------+-------------------+----------+-----------+------------+-----------+------------------+----------+------------+------------+-------------+---------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show sample data from MongoDB\n",
    "print(\"Sample data from MongoDB:\")\n",
    "mongo_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317e0b60",
   "metadata": {},
   "source": [
    "#### Setup MongoDB connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eacf0f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_to_mongodb():\n",
    "    \"\"\"Connect to MongoDB\"\"\"\n",
    "    try:\n",
    "        # Update with your MongoDB connection details if different\n",
    "        client = pymongo.MongoClient(\"mongodb://localhost:27017/\")\n",
    "        # Assuming you have a database called bigdata_demo with raw_taxi_data collection\n",
    "        db = client[\"bigdata_demo\"]\n",
    "        collection = db[\"raw_taxi_data\"]\n",
    "        \n",
    "        print(f\"Successfully connected to MongoDB\")\n",
    "        print(f\"Collection count: {collection.count_documents({})}\")\n",
    "        return collection\n",
    "    except Exception as e:\n",
    "        print(f\"Error connecting to MongoDB: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4879625",
   "metadata": {},
   "source": [
    "#### Setup HBase connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1909e80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_to_hbase():\n",
    "    \"\"\"Connect to HBase running in Docker\"\"\"\n",
    "    try:\n",
    "        # Connect to HBase Thrift server running on default port 9090\n",
    "        connection = happybase.Connection('localhost', port=9090, timeout=300000, autoconnect=True)\n",
    "        print(\"Successfully connected to HBase\")\n",
    "        return connection\n",
    "    except Exception as e:\n",
    "        print(f\"Error connecting to HBase: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d6d7f7",
   "metadata": {},
   "source": [
    "#### Create Tables on Hbase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5841005e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_hbase_table(connection):\n",
    "    \"\"\"Create HBase table for taxi data\"\"\"\n",
    "    try:\n",
    "        # Define the table schema\n",
    "        families = {\n",
    "            'trips': dict(max_versions=1),     # Trip details\n",
    "            'payment': dict(max_versions=1),   # Payment information \n",
    "            'location': dict(max_versions=1)   # Location information\n",
    "        }\n",
    "        \n",
    "        if b'taxi_trips' in connection.tables():\n",
    "            print(\"Dropping existing taxi_trips table for demo...\")\n",
    "            connection.delete_table('taxi_trips', disable=True)\n",
    "        \n",
    "        # Create the table\n",
    "        print(\"Creating taxi_trips table in HBase...\")\n",
    "        connection.create_table('taxi_trips', families)\n",
    "        print(\"Table created successfully!\")\n",
    "        \n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating HBase table: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f54381",
   "metadata": {},
   "source": [
    "#### Create equivalent tables in Hbase as per records in MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e05a1624",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def mongodb_to_hbase_record(mongo_doc):\n",
    "    \"\"\"Convert a MongoDB document to HBase record format\"\"\"\n",
    "    \n",
    "    date_str = mongo_doc.get('date', datetime.now().strftime('%Y-%m-%d'))\n",
    "    date_formatted = date_str.replace('-', '')\n",
    "    row_key = f\"{date_formatted}-{str(uuid.uuid4())}\"\n",
    "    \n",
    "    # Prepare the HBase record\n",
    "    hbase_record = {\n",
    "        'row_key': row_key,\n",
    "        'trips': {},\n",
    "        'payment': {},\n",
    "        'location': {}\n",
    "    }\n",
    "    \n",
    "    # Map MongoDB fields to HBase column families\n",
    "    # Trips related fields\n",
    "    trip_fields = ['pickup_datetime', 'dropoff_datetime', 'pickup_hour', 'pickup_day',\n",
    "                  'pickup_month', 'pickup_year', 'trip_duration_minutes', 'passenger_count']\n",
    "    \n",
    "    # Payment related fields\n",
    "    payment_fields = ['fare_amount', 'extra', 'mta_tax', 'tip_amount', 'tolls_amount',\n",
    "                     'improvement_surcharge', 'total_amount', 'congestion_surcharge', 'payment_type']\n",
    "    \n",
    "    # Location related fields\n",
    "    location_fields = ['PULocationID', 'DOLocationID', 'trip_distance']\n",
    "    \n",
    "    # Fill the HBase record from MongoDB document\n",
    "    for field in trip_fields:\n",
    "        if field in mongo_doc:\n",
    "            hbase_record['trips'][field] = str(mongo_doc[field])\n",
    "    \n",
    "    for field in payment_fields:\n",
    "        if field in mongo_doc:\n",
    "            hbase_record['payment'][field] = str(mongo_doc[field])\n",
    "    \n",
    "    for field in location_fields:\n",
    "        if field in mongo_doc:\n",
    "            hbase_record['location'][field] = str(mongo_doc[field])\n",
    "    \n",
    "    return hbase_record\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a7e563",
   "metadata": {},
   "source": [
    "#### Insert the batched data in Hbase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e2eda053",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "processed_count = 0\n",
    "processed_lock = Lock()\n",
    "progress_update_interval = 5\n",
    "last_progress_time = time.time()\n",
    "\n",
    "def insert_batch_to_hbase(batch, host='localhost', port=9090):\n",
    "    \"\"\"\n",
    "    Insert a batch of records into HBase\n",
    "    Each worker thread/process will have its own connection\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create a new connection for this thread/process\n",
    "        connection = happybase.Connection(host=host, port=port, timeout=30000)\n",
    "        table = connection.table('taxi_trips')\n",
    "        \n",
    "        for record in batch:\n",
    "            # Prepare data in HBase format\n",
    "            data = {}\n",
    "            \n",
    "            # Add trip details\n",
    "            for column, value in record['trips'].items():\n",
    "                data[f'trips:{column}'.encode()] = str(value).encode()\n",
    "                \n",
    "            # Add payment details\n",
    "            for column, value in record['payment'].items():\n",
    "                data[f'payment:{column}'.encode()] = str(value).encode()\n",
    "                \n",
    "            # Add location details\n",
    "            for column, value in record['location'].items():\n",
    "                data[f'location:{column}'.encode()] = str(value).encode()\n",
    "            \n",
    "            # Insert record\n",
    "            table.put(record['row_key'].encode(), data)\n",
    "        \n",
    "        # Update progress counter\n",
    "        global processed_count, processed_lock, last_progress_time\n",
    "        with processed_lock:\n",
    "            processed_count += len(batch)\n",
    "            current_time = time.time()\n",
    "            if current_time - last_progress_time >= progress_update_interval:\n",
    "                print(f\"Progress: {processed_count} records processed\")\n",
    "                last_progress_time = current_time\n",
    "        \n",
    "        # Close the connection\n",
    "        connection.close()\n",
    "        return len(batch)\n",
    "    except Exception as e:\n",
    "        print(f\"Error in worker thread/process: {e}\")\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b778892",
   "metadata": {},
   "source": [
    "##### Since data is big and there is no out of the box Spark support for HBase, I am using aysnchronous batched processing to make the transfer faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed8ce7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def chunker(iterable, chunk_size):\n",
    "    \"\"\"Yield chunks from an iterable\"\"\"\n",
    "    it = iter(iterable)\n",
    "    while True:\n",
    "        chunk = list(itertools.islice(it, chunk_size))\n",
    "        if not chunk:\n",
    "            break\n",
    "        yield chunk\n",
    "\n",
    "def transfer_data_async(mongo_collection, hbase_connection, batch_size=100, num_workers=None):\n",
    "    \"\"\"Transfer data from MongoDB to HBase using multiple threads/processes\"\"\"\n",
    "    try:\n",
    "        \n",
    "        \n",
    "        if num_workers is None:\n",
    "            num_workers = multiprocessing.cpu_count()\n",
    "        \n",
    "        print(f\"Using {num_workers} worker threads for data transfer\")\n",
    "        \n",
    "        \n",
    "        total_docs = mongo_collection.count_documents({})\n",
    "        print(f\"Transferring {total_docs} documents from MongoDB to HBase...\")\n",
    "        \n",
    "        \n",
    "        global processed_count\n",
    "        processed_count = 0\n",
    "        \n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        \n",
    "        cursor = mongo_collection.find({})\n",
    "        \n",
    "        \n",
    "        print(\"Converting MongoDB documents to HBase records...\")\n",
    "        \n",
    "        \n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "        \n",
    "            futures = []\n",
    "            \n",
    "        \n",
    "            for chunk in chunker(cursor, 10000):\n",
    "                # Convert documents to HBase records\n",
    "                hbase_records = [mongodb_to_hbase_record(doc) for doc in chunk]\n",
    "                \n",
    "                \n",
    "                for i in range(0, len(hbase_records), batch_size):\n",
    "                    batch = hbase_records[i:i + batch_size]\n",
    "                    futures.append(executor.submit(insert_batch_to_hbase, batch))\n",
    "            \n",
    "            total_processed = 0\n",
    "            for future in concurrent.futures.as_completed(futures):\n",
    "                try:\n",
    "                    batch_count = future.result()\n",
    "                    total_processed += batch_count\n",
    "                except Exception as exc:\n",
    "                    print(f\"Batch processing generated an exception: {exc}\")\n",
    "        \n",
    "        \n",
    "        total_time = time.time() - start_time\n",
    "        print(f\"Transfer complete! {total_processed} records transferred to HBase in {total_time:.2f} seconds\")\n",
    "        if total_time > 0:\n",
    "            print(f\"Transfer rate: {total_processed/total_time:.2f} records/second\")\n",
    "        \n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error transferring data: {e}\")\n",
    "        return False\n",
    "\n",
    "def transfer_data_with_queue(mongo_collection, hbase_connection, batch_size=100, num_workers=None):\n",
    "    \"\"\"\n",
    "    Transfer data using a producer-consumer pattern with a queue\n",
    "    This is an alternative implementation that can be more memory efficient\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if num_workers is None:\n",
    "            num_workers = multiprocessing.cpu_count()\n",
    "        \n",
    "        print(f\"Using producer-consumer pattern with {num_workers} workers\")\n",
    "        \n",
    "        \n",
    "        total_docs = mongo_collection.count_documents({})\n",
    "        print(f\"Transferring {total_docs} documents from MongoDB to HBase...\")\n",
    "        \n",
    "        \n",
    "        global processed_count\n",
    "        processed_count = 0     \n",
    "        \n",
    "        batch_queue = queue.Queue(maxsize=num_workers * 2)  # Buffer some batches\n",
    "               \n",
    "        done_producing = threading.Event()\n",
    "        \n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        \n",
    "        def consumer():\n",
    "            while not (done_producing.is_set() and batch_queue.empty()):\n",
    "                try:\n",
    "                    batch = batch_queue.get(timeout=1)\n",
    "                    insert_batch_to_hbase(batch)\n",
    "                    batch_queue.task_done()\n",
    "                except queue.Empty:\n",
    "                    continue\n",
    "                except Exception as e:\n",
    "                    print(f\"Consumer error: {e}\")\n",
    "        \n",
    "        \n",
    "        consumers = []\n",
    "        for _ in range(num_workers):\n",
    "            t = threading.Thread(target=consumer)\n",
    "            t.daemon = True\n",
    "            t.start()\n",
    "            consumers.append(t)\n",
    "        \n",
    "        \n",
    "        try:\n",
    "            cursor = mongo_collection.find({})\n",
    "            current_batch = []\n",
    "            \n",
    "            for doc in cursor:\n",
    "                \n",
    "                record = mongodb_to_hbase_record(doc)\n",
    "                current_batch.append(record)\n",
    "                                \n",
    "                if len(current_batch) >= batch_size:\n",
    "                    batch_queue.put(current_batch)\n",
    "                    current_batch = []\n",
    "            \n",
    "            \n",
    "            if current_batch:\n",
    "                batch_queue.put(current_batch)\n",
    "            \n",
    "        finally:\n",
    "            \n",
    "            done_producing.set()\n",
    "                \n",
    "        batch_queue.join()        \n",
    "        for t in consumers:\n",
    "            t.join(timeout=1)\n",
    "        \n",
    "        total_time = time.time() - start_time\n",
    "        print(f\"Transfer complete! {processed_count} records transferred to HBase in {total_time:.2f} seconds\")\n",
    "        if total_time > 0:\n",
    "            print(f\"Transfer rate: {processed_count/total_time:.2f} records/second\")\n",
    "        \n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error transferring data: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22685c77",
   "metadata": {},
   "source": [
    "#### Load the Data from MongoDB to HBase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "65ea8605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully connected to HBase\n",
      "Dropping existing taxi_trips table for demo...\n",
      "Creating taxi_trips table in HBase...\n",
      "Table created successfully!\n",
      "Using 12 worker threads for data transfer\n",
      "Transferring 100000 documents from MongoDB to HBase...\n",
      "Converting MongoDB documents to HBase records...\n",
      "Progress: 1000 records processed\n",
      "Waiting for 100 batch operations to complete...\n",
      "Progress: 13000 records processed\n",
      "Progress: 25000 records processed\n",
      "Progress: 37000 records processed\n",
      "Progress: 49000 records processed\n",
      "Progress: 61000 records processed\n",
      "Progress: 73000 records processed\n",
      "Progress: 85000 records processed\n",
      "Transfer complete! 100000 records transferred to HBase in 60.92 seconds\n",
      "Transfer rate: 1641.56 records/second\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mongo_client = pymongo.MongoClient(\"mongodb://localhost:27017/\")\n",
    "mongo_db = mongo_client[\"bigdata_demo\"]\n",
    "mongo_collection = mongo_db[\"raw_taxi_data\"]\n",
    "\n",
    "hbase_connection = connect_to_hbase()\n",
    "create_hbase_table(hbase_connection)\n",
    "transfer_data_async(mongo_collection, hbase_connection, batch_size=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2eaa65",
   "metadata": {},
   "source": [
    "## Part III: Data analysis in HBase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b4ae50a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_sample_data():\n",
    "    \"\"\"Query and display sample data from the HBase table\"\"\"\n",
    "    try:\n",
    "        connection = connect_to_hbase()\n",
    "        table = connection.table('taxi_trips')\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\" \"*30 + \"SAMPLE RECORDS FROM HBASE\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Scan the first 5 records\n",
    "        records = []\n",
    "        for key, data in table.scan(limit=5):\n",
    "            # Decode binary data to string\n",
    "            decoded_data = {col_name.decode(): value.decode() \n",
    "                           for col_name, value in data.items()}\n",
    "            \n",
    "            # Group by column family for readability\n",
    "            grouped_data = {}\n",
    "            for col_key, value in decoded_data.items():\n",
    "                family, qualifier = col_key.split(':')\n",
    "                if family not in grouped_data:\n",
    "                    grouped_data[family] = {}\n",
    "                grouped_data[family][qualifier] = value\n",
    "            \n",
    "            # Add to records list\n",
    "            records.append({\n",
    "                'key': key.decode(),\n",
    "                'data': grouped_data\n",
    "            })\n",
    "        \n",
    "        # Display the records in a nicely formatted way\n",
    "        for i, record in enumerate(records, 1):\n",
    "            print(f\"\\n{'='*40} RECORD {i} {'='*40}\")\n",
    "            print(f\"KEY: {record['key']}\")\n",
    "            \n",
    "            for family, columns in record['data'].items():\n",
    "                print(f\"\\n  {family.upper()} FAMILY:\")\n",
    "                print(\"  \" + \"-\"*50)\n",
    "                \n",
    "                # Get max column name length for alignment\n",
    "                max_col_len = max([len(col) for col in columns.keys()]) if columns else 0\n",
    "                \n",
    "                # Print columns with aligned values\n",
    "                for col, val in columns.items():\n",
    "                    # Truncate very long values\n",
    "                    display_val = val\n",
    "                    if len(val) > 50:\n",
    "                        display_val = val[:47] + \"...\"\n",
    "                    \n",
    "                    print(f\"  {col.ljust(max_col_len)} : {display_val}\")\n",
    "        \n",
    "        # Count total records\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\" \"*30 + \"RECORD COUNT SUMMARY\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Show progress while counting\n",
    "        row_count = 0\n",
    "        chunk_size = 1000\n",
    "        print(\"Counting records\", end=\"\", flush=True)\n",
    "        \n",
    "        for _ in table.scan(batch_size=chunk_size, columns=[b'trips:pickup_datetime']):\n",
    "            row_count += 1\n",
    "            if row_count % chunk_size == 0:\n",
    "                print(\".\", end=\"\", flush=True)\n",
    "        \n",
    "        print(f\"\\n\\nTotal records in HBase table: {row_count:,}\")\n",
    "        print(\"=\"*80 + \"\\n\")\n",
    "        \n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ Error querying data: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4f90c7c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully connected to HBase\n",
      "\n",
      "================================================================================\n",
      "                              SAMPLE RECORDS FROM HBASE\n",
      "================================================================================\n",
      "\n",
      "======================================== RECORD 1 ========================================\n",
      "KEY: 20250101-001ccea6-3ffb-4d5b-aa53-f80069f32c3c\n",
      "\n",
      "  LOCATION FAMILY:\n",
      "  --------------------------------------------------\n",
      "  DOLocationID  : 114\n",
      "  PULocationID  : 211\n",
      "  trip_distance : 0.72\n",
      "\n",
      "  PAYMENT FAMILY:\n",
      "  --------------------------------------------------\n",
      "  congestion_surcharge  : 2.5\n",
      "  extra                 : 1.0\n",
      "  fare_amount           : 5.8\n",
      "  improvement_surcharge : 1.0\n",
      "  mta_tax               : 0.5\n",
      "  payment_type          : 2\n",
      "  tip_amount            : 0.0\n",
      "  tolls_amount          : 0.0\n",
      "  total_amount          : 10.8\n",
      "\n",
      "  TRIPS FAMILY:\n",
      "  --------------------------------------------------\n",
      "  dropoff_datetime      : 2024-12-31 20:47:20\n",
      "  passenger_count       : 3\n",
      "  pickup_datetime       : 2024-12-31 20:43:44\n",
      "  pickup_day            : 4\n",
      "  pickup_hour           : 2\n",
      "  pickup_month          : 1\n",
      "  pickup_year           : 2025\n",
      "  trip_duration_minutes : 3.6\n",
      "\n",
      "======================================== RECORD 2 ========================================\n",
      "KEY: 20250101-00330f03-2bc7-41fb-b40b-3751ae44edf4\n",
      "\n",
      "  LOCATION FAMILY:\n",
      "  --------------------------------------------------\n",
      "  DOLocationID  : 249\n",
      "  PULocationID  : 132\n",
      "  trip_distance : 18.16\n",
      "\n",
      "  PAYMENT FAMILY:\n",
      "  --------------------------------------------------\n",
      "  congestion_surcharge  : 2.5\n",
      "  extra                 : 0.0\n",
      "  fare_amount           : 70.0\n",
      "  improvement_surcharge : 1.0\n",
      "  mta_tax               : 0.5\n",
      "  payment_type          : 1\n",
      "  tip_amount            : 10.0\n",
      "  tolls_amount          : 0.0\n",
      "  total_amount          : 84.0\n",
      "\n",
      "  TRIPS FAMILY:\n",
      "  --------------------------------------------------\n",
      "  dropoff_datetime      : 2025-01-01 10:10:02\n",
      "  passenger_count       : 1\n",
      "  pickup_datetime       : 2025-01-01 09:27:20\n",
      "  pickup_day            : 4\n",
      "  pickup_hour           : 15\n",
      "  pickup_month          : 1\n",
      "  pickup_year           : 2025\n",
      "  trip_duration_minutes : 42.7\n",
      "\n",
      "======================================== RECORD 3 ========================================\n",
      "KEY: 20250101-00a7a3be-8365-43e6-9682-8706835e025f\n",
      "\n",
      "  LOCATION FAMILY:\n",
      "  --------------------------------------------------\n",
      "  DOLocationID  : 117\n",
      "  PULocationID  : 132\n",
      "  trip_distance : 10.62\n",
      "\n",
      "  PAYMENT FAMILY:\n",
      "  --------------------------------------------------\n",
      "  congestion_surcharge  : 0.0\n",
      "  extra                 : 0.0\n",
      "  fare_amount           : 43.6\n",
      "  improvement_surcharge : 1.0\n",
      "  mta_tax               : 0.5\n",
      "  payment_type          : 2\n",
      "  tip_amount            : 0.0\n",
      "  tolls_amount          : 0.0\n",
      "  total_amount          : 46.85\n",
      "\n",
      "  TRIPS FAMILY:\n",
      "  --------------------------------------------------\n",
      "  dropoff_datetime      : 2025-01-01 03:36:35\n",
      "  passenger_count       : 1\n",
      "  pickup_datetime       : 2025-01-01 03:13:36\n",
      "  pickup_day            : 4\n",
      "  pickup_hour           : 8\n",
      "  pickup_month          : 1\n",
      "  pickup_year           : 2025\n",
      "  trip_duration_minutes : 22.983333333333334\n",
      "\n",
      "======================================== RECORD 4 ========================================\n",
      "KEY: 20250101-00b84b34-5dd9-4bb8-a26e-eb5113dbda9b\n",
      "\n",
      "  LOCATION FAMILY:\n",
      "  --------------------------------------------------\n",
      "  DOLocationID  : 141\n",
      "  PULocationID  : 107\n",
      "  trip_distance : 2.98\n",
      "\n",
      "  PAYMENT FAMILY:\n",
      "  --------------------------------------------------\n",
      "  congestion_surcharge  : 2.5\n",
      "  extra                 : 0.0\n",
      "  fare_amount           : 15.6\n",
      "  improvement_surcharge : 1.0\n",
      "  mta_tax               : 0.5\n",
      "  payment_type          : 1\n",
      "  tip_amount            : 2.94\n",
      "  tolls_amount          : 0.0\n",
      "  total_amount          : 22.54\n",
      "\n",
      "  TRIPS FAMILY:\n",
      "  --------------------------------------------------\n",
      "  dropoff_datetime      : 2025-01-01 14:14:43\n",
      "  passenger_count       : 1\n",
      "  pickup_datetime       : 2025-01-01 14:02:51\n",
      "  pickup_day            : 4\n",
      "  pickup_hour           : 19\n",
      "  pickup_month          : 1\n",
      "  pickup_year           : 2025\n",
      "  trip_duration_minutes : 11.866666666666667\n",
      "\n",
      "======================================== RECORD 5 ========================================\n",
      "KEY: 20250101-00fe7b4e-6c73-4339-a965-13a8b4bfe203\n",
      "\n",
      "  LOCATION FAMILY:\n",
      "  --------------------------------------------------\n",
      "  DOLocationID  : 48\n",
      "  PULocationID  : 236\n",
      "  trip_distance : 2.73\n",
      "\n",
      "  PAYMENT FAMILY:\n",
      "  --------------------------------------------------\n",
      "  congestion_surcharge  : 2.5\n",
      "  extra                 : 1.0\n",
      "  fare_amount           : 14.9\n",
      "  improvement_surcharge : 1.0\n",
      "  mta_tax               : 0.5\n",
      "  payment_type          : 1\n",
      "  tip_amount            : 3.98\n",
      "  tolls_amount          : 0.0\n",
      "  total_amount          : 23.88\n",
      "\n",
      "  TRIPS FAMILY:\n",
      "  --------------------------------------------------\n",
      "  dropoff_datetime      : 2024-12-31 21:11:04\n",
      "  passenger_count       : 1\n",
      "  pickup_datetime       : 2024-12-31 20:59:11\n",
      "  pickup_day            : 4\n",
      "  pickup_hour           : 2\n",
      "  pickup_month          : 1\n",
      "  pickup_year           : 2025\n",
      "  trip_duration_minutes : 11.883333333333333\n",
      "\n",
      "================================================================================\n",
      "                              RECORD COUNT SUMMARY\n",
      "================================================================================\n",
      "Counting records....................................................................................................\n",
      "\n",
      "Total records in HBase table: 100,000\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_sample_data()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
