{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846578bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n",
      "25/04/15 21:29:01 WARN Utils: Your hostname, DESKTOP-U7R862J resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "25/04/15 21:29:01 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/suman/.ivy2/cache\n",
      "The jars for the packages stored in: /home/suman/.ivy2/jars\n",
      "org.mongodb.spark#mongo-spark-connector_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-f88bd66f-f108-4745-a9f2-4577ba5f2729;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.mongodb.spark#mongo-spark-connector_2.12;3.0.1 in central\n",
      "\tfound org.mongodb#mongodb-driver-sync;4.0.5 in central\n",
      "\tfound org.mongodb#bson;4.0.5 in central\n",
      "\tfound org.mongodb#mongodb-driver-core;4.0.5 in central\n",
      ":: resolution report :: resolve 305ms :: artifacts dl 14ms\n",
      "\t:: modules in use:\n",
      "\torg.mongodb#bson;4.0.5 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-core;4.0.5 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-sync;4.0.5 from central in [default]\n",
      "\torg.mongodb.spark#mongo-spark-connector_2.12;3.0.1 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   4   |   0   |   0   |   0   ||   4   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-f88bd66f-f108-4745-a9f2-4577ba5f2729\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 4 already retrieved (0kB/7ms)\n",
      "25/04/15 21:29:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_timestamp, hour, dayofweek, month, year, date_format, rand\n",
    "import pymongo\n",
    "import happybase\n",
    "import uuid\n",
    "from datetime import datetime\n",
    "import time\n",
    "import json\n",
    "\n",
    "\n",
    "\n",
    "# Initialize Spark with proper configurations\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"NYC Taxi Data Pipeline\") \\\n",
    "    .config(\"spark.mongodb.output.uri\", \"mongodb://localhost:27017/bigdata_demo.raw_taxi_data\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.mongodb.spark:mongo-spark-connector_2.12:3.0.1\") \\\n",
    "    .config(\"spark.mongodb.input.uri\", \"mongodb://localhost:27017/bigdata_demo.raw_taxi_data\") \\\n",
    "    .getOrCreate()\n",
    "    \n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47d578ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_df  = spark.read.csv(\"yellow_tripdata_2025-01.csv\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4327a6fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Schema:\n",
      "root\n",
      " |-- VendorID: string (nullable = true)\n",
      " |-- tpep_pickup_datetime: string (nullable = true)\n",
      " |-- tpep_dropoff_datetime: string (nullable = true)\n",
      " |-- passenger_count: string (nullable = true)\n",
      " |-- trip_distance: string (nullable = true)\n",
      " |-- RatecodeID: string (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- PULocationID: string (nullable = true)\n",
      " |-- DOLocationID: string (nullable = true)\n",
      " |-- payment_type: string (nullable = true)\n",
      " |-- fare_amount: string (nullable = true)\n",
      " |-- extra: string (nullable = true)\n",
      " |-- mta_tax: string (nullable = true)\n",
      " |-- tip_amount: string (nullable = true)\n",
      " |-- tolls_amount: string (nullable = true)\n",
      " |-- improvement_surcharge: string (nullable = true)\n",
      " |-- total_amount: string (nullable = true)\n",
      " |-- congestion_surcharge: string (nullable = true)\n",
      " |-- Airport_fee: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display the schema\n",
    "print(\"Dataset Schema:\")\n",
    "taxi_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2deee631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing data cleaning and transformations...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(\"Performing data cleaning and transformations...\")\n",
    "cleaned_df = taxi_df \\\n",
    "    .withColumn(\"pickup_datetime\", to_timestamp(col(\"tpep_pickup_datetime\"))) \\\n",
    "    .withColumn(\"dropoff_datetime\", to_timestamp(col(\"tpep_dropoff_datetime\"))) \\\n",
    "    .withColumn(\"pickup_hour\", hour(col(\"pickup_datetime\"))) \\\n",
    "    .withColumn(\"pickup_day\", dayofweek(col(\"pickup_datetime\"))) \\\n",
    "    .withColumn(\"pickup_month\", month(col(\"pickup_datetime\"))) \\\n",
    "    .withColumn(\"pickup_year\", year(col(\"pickup_datetime\"))) \\\n",
    "    .withColumn(\"trip_duration_minutes\", (col(\"dropoff_datetime\").cast(\"long\") - col(\"pickup_datetime\").cast(\"long\")) / 60) \\\n",
    "    .withColumn(\"date\", date_format(col(\"pickup_datetime\"), \"yyyy-MM-dd\")) \\\n",
    "    .drop(\"tpep_pickup_datetime\", \"tpep_dropoff_datetime\")  # Drop original timestamp columns\n",
    "    \n",
    "    \n",
    "# select random 100000 samples to handle the large dataset on convservative system\n",
    "cleaned_df = cleaned_df.orderBy(rand()).limit(100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7571ac30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of transformed data:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+-------------------+-------------------+-----------+----------+------------+-----------+---------------------+----------+\n",
      "|VendorID|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|Airport_fee|    pickup_datetime|   dropoff_datetime|pickup_hour|pickup_day|pickup_month|pickup_year|trip_duration_minutes|      date|\n",
      "+--------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+-------------------+-------------------+-----------+----------+------------+-----------+---------------------+----------+\n",
      "|       2|              1|          6.5|         1|                 N|         249|         179|           1|       33.8|  1.0|    0.5|      7.91|         0.0|                  1.0|       47.46|                 2.5|        0.0|2025-01-26 00:21:15|2025-01-26 00:53:28|          0|         1|           1|       2025|    32.21666666666667|2025-01-26|\n",
      "|       2|           NULL|         7.24|      NULL|              NULL|         236|          88|           0|      25.73|  0.0|    0.5|       0.0|         0.0|                  1.0|       30.48|                NULL|       NULL|2025-01-09 09:16:13|2025-01-09 09:35:10|          9|         5|           1|       2025|                18.95|2025-01-09|\n",
      "|       2|           NULL|         2.82|      NULL|              NULL|         113|          13|           0|      13.73|  0.0|    0.5|       0.0|         0.0|                  1.0|       18.48|                NULL|       NULL|2025-01-23 06:55:53|2025-01-23 07:11:12|          6|         5|           1|       2025|   15.316666666666666|2025-01-23|\n",
      "|       2|              1|         1.26|         1|                 N|         141|          75|           1|        8.6|  0.0|    0.5|      2.52|         0.0|                  1.0|       15.12|                 2.5|        0.0|2025-01-15 08:52:41|2025-01-15 08:59:13|          8|         4|           1|       2025|    6.533333333333333|2025-01-15|\n",
      "|       2|              1|         1.03|         1|                 N|         170|         107|           2|        7.2|  0.0|    0.5|       0.0|         0.0|                  1.0|       11.95|                 2.5|        0.0|2025-01-14 15:48:17|2025-01-14 15:53:22|         15|         3|           1|       2025|    5.083333333333333|2025-01-14|\n",
      "+--------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+-------------------+-------------------+-----------+----------+------------+-----------+---------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Writing data to MongoDB...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully loaded into MongoDB collection: raw_taxi_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Show sample data\n",
    "print(\"Sample of transformed data:\")\n",
    "cleaned_df.show(5)\n",
    "\n",
    "# Write to MongoDB as raw data\n",
    "print(\"Writing data to MongoDB...\")\n",
    "cleaned_df.write.format(\"mongo\").mode(\"overwrite\").save()\n",
    "\n",
    "print(\"Data successfully loaded into MongoDB collection: raw_taxi_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee4fbf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from MongoDB...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Load data from MongoDB\n",
    "print(\"Loading data from MongoDB...\")\n",
    "mongo_df = spark.read.format(\"mongo\").load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df68be4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample data from MongoDB:\n",
      "+-----------+------------+------------+----------+--------+--------------------+--------------------+----------+-------------------+-----+-----------+---------------------+-------+---------------+------------+-------------------+----------+-----------+------------+-----------+------------------+----------+------------+------------+-------------+---------------------+\n",
      "|Airport_fee|DOLocationID|PULocationID|RatecodeID|VendorID|                 _id|congestion_surcharge|      date|   dropoff_datetime|extra|fare_amount|improvement_surcharge|mta_tax|passenger_count|payment_type|    pickup_datetime|pickup_day|pickup_hour|pickup_month|pickup_year|store_and_fwd_flag|tip_amount|tolls_amount|total_amount|trip_distance|trip_duration_minutes|\n",
      "+-----------+------------+------------+----------+--------+--------------------+--------------------+----------+-------------------+-----+-----------+---------------------+-------+---------------+------------+-------------------+----------+-----------+------------+-----------+------------------+----------+------------+------------+-------------+---------------------+\n",
      "|        0.0|         179|         249|         1|       2|{67fe7ee305da526e...|                 2.5|2025-01-26|2025-01-26 00:53:28|  1.0|       33.8|                  1.0|    0.5|              1|           1|2025-01-26 00:21:15|         1|          0|           1|       2025|                 N|      7.91|         0.0|       47.46|          6.5|    32.21666666666667|\n",
      "|       NULL|          88|         236|      NULL|       2|{67fe7ee305da526e...|                NULL|2025-01-09|2025-01-09 09:35:10|  0.0|      25.73|                  1.0|    0.5|           NULL|           0|2025-01-09 09:16:13|         5|          9|           1|       2025|              NULL|       0.0|         0.0|       30.48|         7.24|                18.95|\n",
      "|       NULL|          13|         113|      NULL|       2|{67fe7ee305da526e...|                NULL|2025-01-23|2025-01-23 07:11:12|  0.0|      13.73|                  1.0|    0.5|           NULL|           0|2025-01-23 06:55:53|         5|          6|           1|       2025|              NULL|       0.0|         0.0|       18.48|         2.82|   15.316666666666666|\n",
      "|        0.0|          75|         141|         1|       2|{67fe7ee305da526e...|                 2.5|2025-01-15|2025-01-15 08:59:13|  0.0|        8.6|                  1.0|    0.5|              1|           1|2025-01-15 08:52:41|         4|          8|           1|       2025|                 N|      2.52|         0.0|       15.12|         1.26|    6.533333333333333|\n",
      "|        0.0|         107|         170|         1|       2|{67fe7ee305da526e...|                 2.5|2025-01-14|2025-01-14 15:53:22|  0.0|        7.2|                  1.0|    0.5|              1|           2|2025-01-14 15:48:17|         3|         15|           1|       2025|                 N|       0.0|         0.0|       11.95|         1.03|    5.083333333333333|\n",
      "+-----------+------------+------------+----------+--------+--------------------+--------------------+----------+-------------------+-----+-----------+---------------------+-------+---------------+------------+-------------------+----------+-----------+------------+-----------+------------------+----------+------------+------------+-------------+---------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show sample data from MongoDB\n",
    "print(\"Sample data from MongoDB:\")\n",
    "mongo_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eacf0f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# MongoDB to HBase Data Transfer\n",
    "\n",
    "def connect_to_mongodb():\n",
    "    \"\"\"Connect to MongoDB\"\"\"\n",
    "    try:\n",
    "        # Update with your MongoDB connection details if different\n",
    "        client = pymongo.MongoClient(\"mongodb://localhost:27017/\")\n",
    "        # Assuming you have a database called bigdata_demo with raw_taxi_data collection\n",
    "        db = client[\"bigdata_demo\"]\n",
    "        collection = db[\"raw_taxi_data\"]\n",
    "        \n",
    "        print(f\"Successfully connected to MongoDB\")\n",
    "        print(f\"Collection count: {collection.count_documents({})}\")\n",
    "        return collection\n",
    "    except Exception as e:\n",
    "        print(f\"Error connecting to MongoDB: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed8ce7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully connected to HBase\n",
      "Dropping existing taxi_trips table for demo...\n",
      "Creating taxi_trips table in HBase...\n",
      "Table created successfully!\n",
      "Using 12 worker threads for data transfer\n",
      "Transferring 100000 documents from MongoDB to HBase...\n",
      "Converting MongoDB documents to HBase records...\n",
      "Progress: 1000 records processed\n",
      "Waiting for 100 batch operations to complete...\n",
      "Progress: 13000 records processed\n",
      "Progress: 25000 records processed\n",
      "Progress: 37000 records processed\n",
      "Progress: 49000 records processed\n",
      "Progress: 61000 records processed\n",
      "Progress: 73000 records processed\n",
      "Progress: 85000 records processed\n",
      "Transfer complete! 100000 records transferred to HBase in 62.65 seconds\n",
      "Transfer rate: 1596.06 records/second\n"
     ]
    }
   ],
   "source": [
    "import happybase\n",
    "import pymongo\n",
    "import uuid\n",
    "import time\n",
    "import concurrent.futures\n",
    "from datetime import datetime\n",
    "import multiprocessing\n",
    "from threading import Lock\n",
    "import queue\n",
    "import itertools\n",
    "# Global variables for thread-safe counters\n",
    "processed_count = 0\n",
    "processed_lock = Lock()\n",
    "progress_update_interval = 5  # seconds\n",
    "last_progress_time = time.time()\n",
    "\n",
    "def connect_to_hbase():\n",
    "    \"\"\"Connect to HBase running in Docker\"\"\"\n",
    "    try:\n",
    "        # Connect to HBase Thrift server running on default port 9090\n",
    "        connection = happybase.Connection('localhost', port=9090, timeout=300000, autoconnect=True)\n",
    "        print(\"Successfully connected to HBase\")\n",
    "        return connection\n",
    "    except Exception as e:\n",
    "        print(f\"Error connecting to HBase: {e}\")\n",
    "        return None\n",
    "\n",
    "def create_hbase_table(connection):\n",
    "    \"\"\"Create HBase table for taxi data\"\"\"\n",
    "    try:\n",
    "        # Define the table schema\n",
    "        families = {\n",
    "            'trips': dict(max_versions=1),     # Trip details\n",
    "            'payment': dict(max_versions=1),   # Payment information \n",
    "            'location': dict(max_versions=1)   # Location information\n",
    "        }\n",
    "        \n",
    "        # Check if table exists, if so delete it for demo purposes\n",
    "        if b'taxi_trips' in connection.tables():\n",
    "            print(\"Dropping existing taxi_trips table for demo...\")\n",
    "            connection.delete_table('taxi_trips', disable=True)\n",
    "        \n",
    "        # Create the table\n",
    "        print(\"Creating taxi_trips table in HBase...\")\n",
    "        connection.create_table('taxi_trips', families)\n",
    "        print(\"Table created successfully!\")\n",
    "        \n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating HBase table: {e}\")\n",
    "        return False\n",
    "\n",
    "def mongodb_to_hbase_record(mongo_doc):\n",
    "    \"\"\"Convert a MongoDB document to HBase record format\"\"\"\n",
    "    # Create a row key using date from the MongoDB document and a UUID\n",
    "    # Assuming the document has a 'date' field\n",
    "    date_str = mongo_doc.get('date', datetime.now().strftime('%Y-%m-%d'))\n",
    "    date_formatted = date_str.replace('-', '')\n",
    "    row_key = f\"{date_formatted}-{str(uuid.uuid4())}\"\n",
    "    \n",
    "    # Prepare the HBase record\n",
    "    hbase_record = {\n",
    "        'row_key': row_key,\n",
    "        'trips': {},\n",
    "        'payment': {},\n",
    "        'location': {}\n",
    "    }\n",
    "    \n",
    "    # Map MongoDB fields to HBase column families\n",
    "    # Trips related fields\n",
    "    trip_fields = ['pickup_datetime', 'dropoff_datetime', 'pickup_hour', 'pickup_day',\n",
    "                  'pickup_month', 'pickup_year', 'trip_duration_minutes', 'passenger_count']\n",
    "    \n",
    "    # Payment related fields\n",
    "    payment_fields = ['fare_amount', 'extra', 'mta_tax', 'tip_amount', 'tolls_amount',\n",
    "                     'improvement_surcharge', 'total_amount', 'congestion_surcharge', 'payment_type']\n",
    "    \n",
    "    # Location related fields\n",
    "    location_fields = ['PULocationID', 'DOLocationID', 'trip_distance']\n",
    "    \n",
    "    # Fill the HBase record from MongoDB document\n",
    "    for field in trip_fields:\n",
    "        if field in mongo_doc:\n",
    "            hbase_record['trips'][field] = str(mongo_doc[field])\n",
    "    \n",
    "    for field in payment_fields:\n",
    "        if field in mongo_doc:\n",
    "            hbase_record['payment'][field] = str(mongo_doc[field])\n",
    "    \n",
    "    for field in location_fields:\n",
    "        if field in mongo_doc:\n",
    "            hbase_record['location'][field] = str(mongo_doc[field])\n",
    "    \n",
    "    return hbase_record\n",
    "\n",
    "def insert_batch_to_hbase(batch, host='localhost', port=9090):\n",
    "    \"\"\"\n",
    "    Insert a batch of records into HBase\n",
    "    Each worker thread/process will have its own connection\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create a new connection for this thread/process\n",
    "        connection = happybase.Connection(host=host, port=port, timeout=30000)\n",
    "        table = connection.table('taxi_trips')\n",
    "        \n",
    "        for record in batch:\n",
    "            # Prepare data in HBase format\n",
    "            data = {}\n",
    "            \n",
    "            # Add trip details\n",
    "            for column, value in record['trips'].items():\n",
    "                data[f'trips:{column}'.encode()] = str(value).encode()\n",
    "                \n",
    "            # Add payment details\n",
    "            for column, value in record['payment'].items():\n",
    "                data[f'payment:{column}'.encode()] = str(value).encode()\n",
    "                \n",
    "            # Add location details\n",
    "            for column, value in record['location'].items():\n",
    "                data[f'location:{column}'.encode()] = str(value).encode()\n",
    "            \n",
    "            # Insert record\n",
    "            table.put(record['row_key'].encode(), data)\n",
    "        \n",
    "        # Update progress counter\n",
    "        global processed_count, processed_lock, last_progress_time\n",
    "        with processed_lock:\n",
    "            processed_count += len(batch)\n",
    "            current_time = time.time()\n",
    "            if current_time - last_progress_time >= progress_update_interval:\n",
    "                print(f\"Progress: {processed_count} records processed\")\n",
    "                last_progress_time = current_time\n",
    "        \n",
    "        # Close the connection\n",
    "        connection.close()\n",
    "        return len(batch)\n",
    "    except Exception as e:\n",
    "        print(f\"Error in worker thread/process: {e}\")\n",
    "        return 0\n",
    "\n",
    "def chunker(iterable, chunk_size):\n",
    "    \"\"\"Yield chunks from an iterable\"\"\"\n",
    "    it = iter(iterable)\n",
    "    while True:\n",
    "        chunk = list(itertools.islice(it, chunk_size))\n",
    "        if not chunk:\n",
    "            break\n",
    "        yield chunk\n",
    "\n",
    "def transfer_data_async(mongo_collection, hbase_connection, batch_size=100, num_workers=None):\n",
    "    \"\"\"Transfer data from MongoDB to HBase using multiple threads/processes\"\"\"\n",
    "    try:\n",
    "        import itertools\n",
    "        \n",
    "        # Determine number of workers (default to CPU count)\n",
    "        if num_workers is None:\n",
    "            num_workers = multiprocessing.cpu_count()\n",
    "        \n",
    "        print(f\"Using {num_workers} worker threads for data transfer\")\n",
    "        \n",
    "        # Get total document count\n",
    "        total_docs = mongo_collection.count_documents({})\n",
    "        print(f\"Transferring {total_docs} documents from MongoDB to HBase...\")\n",
    "        \n",
    "        # Reset global counter\n",
    "        global processed_count\n",
    "        processed_count = 0\n",
    "        \n",
    "        # Start time for performance tracking\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Process all documents in batches\n",
    "        cursor = mongo_collection.find({})\n",
    "        \n",
    "        # Convert MongoDB documents to HBase format\n",
    "        print(\"Converting MongoDB documents to HBase records...\")\n",
    "        \n",
    "        # Create a thread pool for processing batches\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "            # Submit batches for processing\n",
    "            futures = []\n",
    "            \n",
    "            # Process in chunks to avoid memory issues\n",
    "            for chunk in chunker(cursor, 10000):\n",
    "                # Convert documents to HBase records\n",
    "                hbase_records = [mongodb_to_hbase_record(doc) for doc in chunk]\n",
    "                \n",
    "                # Process in batches\n",
    "                for i in range(0, len(hbase_records), batch_size):\n",
    "                    batch = hbase_records[i:i + batch_size]\n",
    "                    futures.append(executor.submit(insert_batch_to_hbase, batch))\n",
    "            \n",
    "            # Wait for all futures to complete\n",
    "            print(f\"Waiting for {len(futures)} batch operations to complete...\")\n",
    "            total_processed = 0\n",
    "            for future in concurrent.futures.as_completed(futures):\n",
    "                try:\n",
    "                    batch_count = future.result()\n",
    "                    total_processed += batch_count\n",
    "                except Exception as exc:\n",
    "                    print(f\"Batch processing generated an exception: {exc}\")\n",
    "        \n",
    "        # Final statistics\n",
    "        total_time = time.time() - start_time\n",
    "        print(f\"Transfer complete! {total_processed} records transferred to HBase in {total_time:.2f} seconds\")\n",
    "        if total_time > 0:\n",
    "            print(f\"Transfer rate: {total_processed/total_time:.2f} records/second\")\n",
    "        \n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error transferring data: {e}\")\n",
    "        return False\n",
    "\n",
    "def transfer_data_with_queue(mongo_collection, hbase_connection, batch_size=100, num_workers=None):\n",
    "    \"\"\"\n",
    "    Transfer data using a producer-consumer pattern with a queue\n",
    "    This is an alternative implementation that can be more memory efficient\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import threading\n",
    "        \n",
    "        # Determine number of workers (default to CPU count)\n",
    "        if num_workers is None:\n",
    "            num_workers = multiprocessing.cpu_count()\n",
    "        \n",
    "        print(f\"Using producer-consumer pattern with {num_workers} workers\")\n",
    "        \n",
    "        # Get total document count\n",
    "        total_docs = mongo_collection.count_documents({})\n",
    "        print(f\"Transferring {total_docs} documents from MongoDB to HBase...\")\n",
    "        \n",
    "        # Reset global counter\n",
    "        global processed_count\n",
    "        processed_count = 0\n",
    "        \n",
    "        # Create a queue for batches\n",
    "        batch_queue = queue.Queue(maxsize=num_workers * 2)  # Buffer some batches\n",
    "        \n",
    "        # Flag to signal consumers to exit\n",
    "        done_producing = threading.Event()\n",
    "        \n",
    "        # Start time for performance tracking\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Consumer function\n",
    "        def consumer():\n",
    "            while not (done_producing.is_set() and batch_queue.empty()):\n",
    "                try:\n",
    "                    batch = batch_queue.get(timeout=1)\n",
    "                    insert_batch_to_hbase(batch)\n",
    "                    batch_queue.task_done()\n",
    "                except queue.Empty:\n",
    "                    continue\n",
    "                except Exception as e:\n",
    "                    print(f\"Consumer error: {e}\")\n",
    "        \n",
    "        # Start consumer threads\n",
    "        consumers = []\n",
    "        for _ in range(num_workers):\n",
    "            t = threading.Thread(target=consumer)\n",
    "            t.daemon = True\n",
    "            t.start()\n",
    "            consumers.append(t)\n",
    "        \n",
    "        # Producer: get documents from MongoDB and put batches in queue\n",
    "        try:\n",
    "            cursor = mongo_collection.find({})\n",
    "            current_batch = []\n",
    "            \n",
    "            for doc in cursor:\n",
    "                # Convert document to HBase record\n",
    "                record = mongodb_to_hbase_record(doc)\n",
    "                current_batch.append(record)\n",
    "                \n",
    "                # When batch is full, add to queue\n",
    "                if len(current_batch) >= batch_size:\n",
    "                    batch_queue.put(current_batch)\n",
    "                    current_batch = []\n",
    "            \n",
    "            # Add any remaining records\n",
    "            if current_batch:\n",
    "                batch_queue.put(current_batch)\n",
    "            \n",
    "        finally:\n",
    "            # Signal that we're done producing\n",
    "            done_producing.set()\n",
    "        \n",
    "        # Wait for all tasks to be processed\n",
    "        batch_queue.join()\n",
    "        \n",
    "        # Wait for consumer threads to exit\n",
    "        for t in consumers:\n",
    "            t.join(timeout=1)\n",
    "        \n",
    "        # Final statistics\n",
    "        total_time = time.time() - start_time\n",
    "        print(f\"Transfer complete! {processed_count} records transferred to HBase in {total_time:.2f} seconds\")\n",
    "        if total_time > 0:\n",
    "            print(f\"Transfer rate: {processed_count/total_time:.2f} records/second\")\n",
    "        \n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error transferring data: {e}\")\n",
    "        return False\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    import pymongo\n",
    "    \n",
    "    # Connect to MongoDB\n",
    "    mongo_client = pymongo.MongoClient(\"mongodb://localhost:27017/\")\n",
    "    mongo_db = mongo_client[\"bigdata_demo\"]\n",
    "    mongo_collection = mongo_db[\"raw_taxi_data\"]\n",
    "    \n",
    "    # Connect to HBase\n",
    "    hbase_connection = connect_to_hbase()\n",
    "    \n",
    "    # Create table\n",
    "    create_hbase_table(hbase_connection)\n",
    "    \n",
    "    # Choose the transfer method:\n",
    "    # 1. Thread pool executor (better for smaller datasets)\n",
    "    transfer_data_async(mongo_collection, hbase_connection, batch_size=1000)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
