vertex.size=30,
edge.size=5,
edge.color='red')
degree(g1)
closeness(g1)
betweenness(g1,directed = T)
str(USArrests)
head(USArrests)
library(dplyr)
USArrests.1 <- USArrests[,-3] %>%
scale
USArrests.1
# Fittinfg pca
pca.1 <- procomp(USArrests.1)
# Fittinfg pca
pca.1 <- prcomp(USArrests.1)
summary(pca.1)
install.packages("psych")
library(psych)
# Rotation PCA with variance maximization
fa.2 <- psych::principal(USArrests.1,
nfactors = 3, rotate ="varimax")
summary(fa.2)
# Rotation PCA with variance maximization
fa.2 <- psych::principal(USArrests.1,
nfactors = 3, rotate = varimax)
# Rotation PCA with variance maximization
fa.2 <- psych::principal(USArrests.1,
nfactors = 3, rotate = "varimax")
summary(fa.2)
fa.2
fa.2
# Multi dimensional scaling
# Fiting MDS
USArrests.1 <- scale(USArrests[,-3])
# Distance calculation
state.disimilarity <- dist(USArrests.1)
#MDS fit
mds.1 <- cmdscale(state.disimilarity)
mds.1
summary(mds.1)
#MDS plot
plor(mds.1,pch=19)
#MDS plot
plot(mds.1,pch=19)
abline(h=0,v=0,lty=2)
text(mds.1,pos=4,labels= rownames(USArrests.1),col="tomato")
# to minize stress use Sammon't strees
library(MASS)
mds.2 <- MASS::sammon(state.disimilarity, trace=FALSE)
plot(mds.2$points,pch=19)
abline(h=0,v=0,lty=2)
text(mds.2$points,pos=4,labels= rownames(USArrests.1))
#Compare PCA and MDS
arrows(
x0 = mds.2$points[,1], y0=mds.2$points[,2],
x1 = pca.1$x[,1],y1=pca.1$x[,3],
col = 'red',pch=19,cex=0.5
)
############################ S26 ###################
set.seed(4)
km.out <- kmeans(x,3,nstart = 1)
km.out$tot.withinss
str(iris)
table(iris$Species)
install.packages("ClusterR")
library(ClusterR)
install.packages("cluster")
library(cluster)
data(iris)
View(iris)
iris_1 <- iris[,-5]
set.seed(240)
kmeans.res <- kmeans(iris_1,center=3,nstart=20)
kmeans.res
#Confusion matrix
cm <- table(iris$Species,
kmeans.res$cluster)
cm
#Accuracy
(accuracy<- sum(diag(cm))/sum(cm))
(mce <- 1-accuracy)
# Hierarchical cluster analysis (HCA)
hirar.1 <- hclust(state.disimilarity, method = 'single')
plot(hirar.1,
labels = rownames(USArrests.1),
ylab = "Distance")
summary(hirar.1)
hirar.1
plot(hirar.2,
labels = rownames(USArrests.1),
ylab = "Distance")
hirar.2 <- hclust(state.disimilarity, method = 'complete')
plot(hirar.2,
labels = rownames(USArrests.1),
ylab = "Distance")
# Using airquality Dataset
aq <- airquality
chisq.test(aq$Wind, p=aq$Month)
# Perform goodness-of-fit test on Wind variable by Month variable
# to check if it follows normal distribution or not
library(stats)
chisq.test(aq$Wind, p=aq$Month)
chisq.test(x=aq$Wind, p=aq$Month)
aq$Wind
shapiro.test(aq$Wind)
shapiro.test(aq_table)
aq <- airquality
# Get mean and standard deviation of Wind variable by Month variable
#using the appropriate “apply” family of function,
#show both the results in a single table and interpret them carefully
aq_means<- tapply(aq$Wind,aq$Month,mean) #Mean
aq_sd <- tapply(aq$Wind,aq$Month,sd)     #Standard Deviation
aq_table <- data.frame(Month = names(aq_means),
Mean_of_Wind = as.numeric(aq_means),
SD_of_Wind = as.numeric(aq_sd))
aq_table
# Perform goodness-of-fit test on Wind variable by Month variable
# to check if it follows normal distribution or not
library(stats)
shapiro.test(aq_table)
shapiro.test(aq$Wind)
hist(aq$Wind)
aq$Wind
shapiro.test(aq$Wind)   # Kolmogorov-Smirnov Test to test of normality
?shapiro.test
?leveneTest
leveneTest(Wind ~Month),data=aq)
?stats
# Perform goodness-of-fit test on Wind variable by Month variable
# to check if it follows normal distribution or not
library(stats)
leveneTest(Wind ~Month),data=aq)
leveneTest((Wind~Month),data=aq)
# Perform goodness-of-fit test on Wind variable by Month variable
# to check if the variances of mpg are equal or not on a variable categories
library(car)
leveneTest((Wind~Month),data=aq)
aq$Month
# Perform goodness-of-fit test on Wind variable by Month variable
# to check if the variances of mpg are equal or not on a variable categories
with(aq,shapiro.test(Wind[Month==5]))
with(aq,shapiro.test(Wind[Month==5]))
with(aq,shapiro.test(Wind[Month==6]))
with(aq,shapiro.test(Wind[Month==7]))
with(aq,shapiro.test(Wind[Month==8]))
with(aq,shapiro.test(Wind[Month==9]))
library(car)
leveneTest(Wind~Month, data=aq)
shapiro.test(aq$Wind)   # Shapiro-Wilk Normality Test
shapiro.test(aq$Wind)   # Shapiro-Wilk Normality Test
leveneTest(aq$Wind~aq$Month, data=aq)
result = leveneTest(aq$Wind~aq$Month, data=aq)
ggcorrplot(cor(crime))
## A
# Create an “crime” dataset containing all the variables of USArrests
crime <- USArrests
## B
# Create correlation matrix plot of the crime data and interpret each scatterplot carefully
cor(crime)
#visualize correlation matrix
library(ggcorrplot)
ggcorrplot(cor(crime))
## C
# Split the crime dataset into training and testing data with 70% and 30% cases
set.seed(13)
index <- sample(2,nrow(crime),replace = T,prob = c(0.7,0.3)) #Random sampling into two independent c=variable
train.crime <- crime[index==1,] #Training set
test.crime <- crime[index==2,]  #Test set
rnorm(1:200)
sample(1:200)
install.packages("arules")
install.packages("arulesViz")
result = leveneTest(aq$Wind~aq$Month, data=aq)
result = leveneTest(Wind ~ Month, data=aq)
result = leveneTest(Month ~ Wind, data=aq)
result = leveneTest(aq$Month ~ aq$Wind, data=aq)
result = leveneTest(aq$Month ~ aq$Wind, center=mean)
leveneTest(aq$Wind ~ aq$Wind)
leveneTest(aq$Wind ,aq$Wind)
leveneTest(aq$Wind ,aq$Wind,center=mean)
## C
# Perform goodness-of-fit test on Wind variable by Month variable
# to check if the variances of mpg are equal or not on a variable categories
aq$Month
factor(aq$Month)
leveneTest(aq$Wind ~ factor(aq$Month), data=aq)
leveneTest(aq$Wind ~ factor(aq$Month), data=aq,center=mean)
## D
# Discuss which one-way ANOVA must be used to compare “Wind” variable by “Month” variable categories based on the results obtained above
summary(aov(Wind ~ Month, data = aq))
## D
# Discuss which one-way ANOVA must be used to compare “Wind” variable by “Month” variable categories based on the results obtained above
###### ASSUMPTIONS #####
with(aq, shapiro.test(mpg[Month == 5]))
## D
# Discuss which one-way ANOVA must be used to compare “Wind” variable by “Month” variable categories based on the results obtained above
###### ASSUMPTIONS #####
with(aq, shapiro.test(Wind[Month == 5]))
factor(aq$Month)
## D
# Discuss which one-way ANOVA must be used to compare “Wind” variable by “Month” variable categories based on the results obtained above
###### ASSUMPTIONS #####
with(aq, shapiro.test(Wind[Month == 5]))
with(aq, shapiro.test(Wind[Month == 6]))
with(aq, shapiro.test(Wind[Month == 7]))
with(aq, shapiro.test(Wind[Month == 8]))
with(aq, shapiro.test(Wind[Month == 9]))
summary(aov(Wind ~ Month, data = aq))
# S28
# Create a list of baskets
market_basket <- list(
c("bread","milk"),
c("bread","diapers","beer","Eggs"),
c("milk","diapers","beer","cola"),
c("bread","milk","diapers","beer"),
c("bread","milk","diapers","cola")
)
names(market_basket) <- paste("T",c(1:5), sep = "")
names(market_basket)
library(arules)
# Transformation
trans<- as(market_basket,"transactions")
dim(trans)
trans
#Item Labels
itemLabels(trans)
#Summary
summary(trans)
#Plot
image(trans)
#Inspect
inspect(trans)
# Apriori Algorithm
# Min 0.3 and confidence 0.5
rules<- apriori(
trans,
parameter = list(supp=0.3, conf=0.5,
maxlen=10,
target="rules")
)
# Apriori Algorithm
# Min 0.3 and confidence 0.5
rules<- apriori(
trans,
parameter = list(supp=0.3,
conf=0.5,
maxlen=10,
target="rules")
)
rules
rules
summary(rules)
summary(rules)
inspect(rules)
#Remove empty rules
rules<- apriori(
trans,
parameter = list(supp=0.3,
conf=0.5,
maxlen=10,
minlen=2,
target="rules")
)
inspect(rules)
# rhs = beer
beer_rules<- apriori(
trans,
parameter = list(supp=0.3,
conf=0.5,
maxlen=10,
minlen=2,
appearance = list(default="lhs",rhs="beer")
target="rules")
appearance = list(default="lhs",rhs="beer")
parameter = list(supp=0.3,
)
parameter = list(supp=0.3,
appearance = list(default="lhs",rhs="beer"
)
()
)
# rhs = beer
beer_rules<- apriori(
trans,
parameter = list(supp=0.3,
conf=0.5,
maxlen=10,
minlen=2,)
appearance = list(default="lhs",rhs="beer")
minlen=2,
# rhs = beer
beer_rules<- apriori(
trans,
parameter = list(supp=0.3,
conf=0.5,
maxlen=10,
minlen=2),
appearance = list(default="lhs",rhs="beer")
)
inspect(beer_rules)
#lhs = beer
beer_rules_lhs<- apriori(
trans,
parameter = list(supp=0.3,
conf=0.5,
maxlen=10,
minlen=2),
appearance = list(lhs="beer",default="rhs")
)
inspect(beer_rules_lhs)
#### Product recommendation rule
rules_conf <- sort(rules,by="confidence",decreasing = TRUE)
inspect(head(rules_conf))
################ PLOTTING
library(arulesViz)
plot(rules)
plot(rules, measure = "confidence")
plot(rules, method="two-key plot")
### Interactive plot
library(plotly)
plot(rules, engine = "plotly")
subrules <- head(rules,n=10,by="confidence")
plot(subrules,method="graph",engine = "htmlwidget")
# parrallel coordinates
plot(subrules,method="paracord")
# parrallel coordinates
plot(subrules,method="paracoord")
plot(subrules,method="graph",engine = "htmlwidget")
# parallel coordinates
plot(subrules,method="paracoord")
# Roll Number: 1, 5, 9, 13, 17, 21, 25, 29, 33
#(Generate 200 random data with one continuous dependent variable and five independent variables, random seed = your roll number.
set.seed(13)
sample(200)
A<-sample(200)
A
B<-sample(200)
B
E<-sample(200)
data <- data.frame(A, B,C, D, E)
?knnreg
?annreg
summary(aov(Wind ~ Month, data = aq))
## B
# Create correlation matrix plot of the crime data and interpret each scatterplot carefully
cor(crime)
## A
# Create an “crime” dataset containing all the variables of USArrests
crime <- USArrests
## B
# Create correlation matrix plot of the crime data and interpret each scatterplot carefully
cor(crime)
#visualize correlation matrix
library(ggcorrplot)
ggcorrplot(cor(crime))
#visualize correlation matrix
library(corrplot)
install.packages("corrplot")
#visualize correlation matrix
library(corrplot)
corrplot(cor(crime))
corrplot(cor(crime),method = "number")
set.seed(13)
index <- sample(2,nrow(crime),replace = T,prob = c(0.7,0.3)) #Random sampling into two independent c=variable
train.crime <- crime[index==1,] #Training set
test.crime <- crime[index==2,]  #Test set
crime
summary(lm(Murder ~ .,data=crime))
with(mtcars,shapiro.test(mpg[gear==3]))
with(mtcars,shapiro.test(mpg[gear==4]))
summary(aov(mpg ~gear,data=mtcars))
lm1 <- lm(mtcars$mpg ~mtcars$wt)  #regression model
lm1
summary(lm1)  # coefficient of determiniant(multiple R- squarted) is > 50%, so good regression model assumption 3
summary(lm(Murder ~ .,data=crime))
## A
#Divide the mtcars data into train and test datasets with 80:20 random splits
index <- sample(2,nrow(mtcars),replace = T,prob = c(0.8,0.2)) #Random sampling into two independent c=variable
train.mtcars <- kkdata[index==1,] #Training set
test.mtcars <- kkdata[index==2,]  #Test set
suprv <- glm(am ~., data=train.mtcars, family=binomial)
suprv <- glm(am ~., data=train.mtcars, family=binomial)
summary(suprv)
#Define data
boston = MASS::Boston
str(boston)
#Data partition
set.seed(123)
ind <- sample(2,nrow(boston),replace = T,prob=c(0.8,0.2))
ind
train.data <- boston[ind==1,]
test.data <- boston[ind==2,]
tes.data
test.data
colnames(test.data)
#Training data scaling(must for KNN)
train_x = train.data[,-14]
train_x
test_x <- scale(test.data[,-14])[,]
test_x
library(neuralnet)
plot.data<- read.csv("covnep_252days.csv")
SN<- seq(from=1, to =252)
plot.data<- cbind(plot.data,SN)
##NN Model
n <- neuralnet(totalDeaths ~ SN,
data=plot.data ,
hidden =1, linear.output=F)
plot(n)
n1 <- neuralnet(totalDeaths ~ SN,
data=plot.data ,
hidden =c(3,2),   # hidden layer 3 and 2 neurons
linear.output=F)
plot(n1)
library(rpart)
library(caret)
dt = rpart(medv ~., data=boston)
set.sed(13)
library(rpart)
library(caret)
dt = rpart(medv ~., data=boston)
set.seed(13)
train.dt <- train(medv ~.,
data = boston,
method = "rpart2")
train.dt
plot(train.dt)
library(readr)
ctg <- read_csv("Cardiotocographic.csv")
#######################################
#Decicion tree
getwd()
setwd("C:/Users/kaush/OneDrive - Tribhuvan University/MDS/FIRST/R/Resources")
library(readr)
ctg <- read_csv("Cardiotocographic.csv")
str(ctg)
View(ctg)
#NSP as dependent variable
#Change NSP as factor variable (3 variable)
ctg$NSPF <- factor(ctg$NSP)
str(ctg$NSPF)
set.seed(1234)
ind <- sample(2,nrow(ctg),replace=T,prob=c(0.8,0.2))
train <- ctg[ind==1,]
test <- ctg[ind==2,]
#Fit decision tree
install.packages("party")
library(party)
tree <- ctree(NSPF~LP+AC+FM,data=train)
library(party)
tree <- ctree(NSPF~LP+AC+FM,data=train)
str(ctg$NSPF)
View(ctg)
tree <- ctree(NSPF~LB+AC+FM,data=train)
tree <- ctree(NSPF~LB+AC+FM,data=train)
#Then fit bivariate and multivariate linear regression model, check
# VIF [Variance Inflation Factor],
# BLUE [Best, Linear, Unbiased, Estimate]
# LINE [Linear, Independence, Normal, Equal variance].
summary(lm(K~A))
#Then fit bivariate and multivariate linear regression model, check
# VIF [Variance Inflation Factor],
# BLUE [Best, Linear, Unbiased, Estimate]
# LINE [Linear, Independence, Normal, Equal variance].
summary(lm(K~A))
#Then fit bivariate and multivariate linear regression model, check
# VIF [Variance Inflation Factor],
# BLUE [Best, Linear, Unbiased, Estimate]
# LINE [Linear, Independence, Normal, Equal variance].
summary(lm(K~A),data=train.data)
K <- A+B+C+D+E
plot(A)
data <- data.frame(K,A, B,C, D, E)
data
ind <- sample(2, nrow(data),
replace = T, prob = c(0.7, 0.3))
train.data <- data[ind==1,]
test.data <- data[ind==2,]
#Then fit bivariate and multivariate linear regression model, check
# VIF [Variance Inflation Factor],
# BLUE [Best, Linear, Unbiased, Estimate]
# LINE [Linear, Independence, Normal, Equal variance].
summary(lm(K~A),data=train.data)
K <- A+B+C+D+E
set.seed(13)
A<-rnorm(200)
B<-rnorm(200)
C<-rnorm(200)
D<-rnorm(200)
E<-rnorm(200)
K <- A+B+C+D+E
plot(A)
data <- data.frame(K,A, B,C, D, E)
data
ind <- sample(2, nrow(data),
replace = T, prob = c(0.7, 0.3))
train.data <- data[ind==1,]
test.data <- data[ind==2,]
#Then fit bivariate and multivariate linear regression model, check
# VIF [Variance Inflation Factor],
# BLUE [Best, Linear, Unbiased, Estimate]
# LINE [Linear, Independence, Normal, Equal variance].
summary(lm(K~A),data=train.data)
table(mtcars$gear)
prop.test(x=c(15,12,5),n=c(32,32,32),correct = F)
pairwise.prop.test(x=c(15,12,5),n=c(32,32,32),correct=F)  #post hoc test
#################
plot(mtcars$wt,mtcars$mpg)
plot(mtcars$disp,mtcars$mpg)
cov(mtcars$wt,mtcars$mpg)
cor(mtcars$wt,mtcars$mpg)
