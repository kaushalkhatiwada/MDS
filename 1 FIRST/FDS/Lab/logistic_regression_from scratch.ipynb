{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data preprocessing \n",
    "def scale(d):\n",
    "    mean=d.mean()\n",
    "    sd=d.std()\n",
    "\n",
    "    return (d-mean)/sd          #Z-score=(x-MEAN)/SD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using an iterator to process data in smaller chunks (batches) and update model parameters accordingly.\n",
    "\n",
    "def data_iter(batch_size, features, labels):\n",
    "    num_examples = len(features)\n",
    "    indices = list(range(num_examples))\n",
    "\n",
    "    np.random.shuffle(indices)\n",
    "    for i in range(0,num_examples,batch_size):\n",
    "        batch_indices = np.array(\n",
    "            indices[i:min(i+batch_size,num_examples)]\n",
    "        )\n",
    "        yield features[batch_indices],labels[batch_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "data=load_diabetes()\n",
    "\n",
    "X=data.data\n",
    "y=data.target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self,X,y):\n",
    "        self.param={}\n",
    "        self.m,self.n=X.shape\n",
    "        self.param['W'] = np.random.randn(self.n,1) * 0.001\n",
    "        self.param['b'] = np.zeros(1)\n",
    "\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.result = pd.DataFrame()\n",
    "\n",
    "    def train(self,alpha=0.001,epochs=10):\n",
    "        for epoch in range(epochs):\n",
    "            print(\"Epoch:\", epoch,end=\"\")\n",
    "            z=np.dot(self.X,self.param['W']) + self.param['b']\n",
    "\n",
    "            #print (y_pred)\n",
    "            self.y_pred=self.sigmoid(z)\n",
    "            self.result[0] = self.y\n",
    "\n",
    "            #update the parameter\n",
    "            self.param['W'] = self.param['W'] - alpha * 1/self.m * np.dot(self.X.T,self.y_pred - np.reshape(self.y,(self.m,1)))\n",
    "            self.param['b'] = self.param['b'] - alpha * 1/self.m * np.sum(self.y_pred - np.reshape(self.y,(self.m,1)))\n",
    "\n",
    "            self.y_pred = self.sigmoid(np.dot(self.X,self.param['W'] + self.param['b']))\n",
    "            loss=self.loss(self.y,self.y_pred)\n",
    "\n",
    "            self.result[1]=self.y_pred\n",
    "            print(\", loss = \",loss)\n",
    "        \n",
    "        print(\",Final Loss = \",loss)\n",
    "        print(\"  W: {},b:{}\".format(self.param['W'],self.param['b']))\n",
    "\n",
    "    def loss(self,y,y_pred):\n",
    "        #printing log(1-y_pred)\n",
    "        y_zero_loss=y.T.dot(np.log(y_pred))\n",
    "        y_one_loss=(1-y).T.dot(np.log(1-y_pred))\n",
    "\n",
    "        return -np.sum(y_zero_loss+ y_one_loss)/len(y)\n",
    "    \n",
    "    def sigmoid(self,z):\n",
    "        return 1.0/(1+np.exp(-z))\n",
    "    \n",
    "    def predict(self,X):\n",
    "        return self.sigmoid(np.dot(X,self.param['W'])+self.param['b'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, loss =  -inf\n",
      "Epoch: 1, loss =  -inf\n",
      "Epoch: 2, loss =  -inf\n",
      "Epoch: 3, loss =  -inf\n",
      "Epoch: 4, loss =  -inf\n",
      "Epoch: 5, loss =  -inf\n",
      "Epoch: 6, loss =  -inf\n",
      "Epoch: 7, loss =  nan\n",
      "Epoch: 8, loss =  nan\n",
      "Epoch: 9, loss =  nan\n",
      "Epoch: 10, loss =  nan\n",
      "Epoch: 11, loss =  nan\n",
      "Epoch: 12, loss =  nan\n",
      "Epoch: 13, loss =  nan\n",
      "Epoch: 14, loss =  nan\n",
      "Epoch: 15, loss =  nan\n",
      "Epoch: 16, loss =  nan\n",
      "Epoch: 17, loss =  nan\n",
      "Epoch: 18, loss =  nan\n",
      "Epoch: 19, loss =  nan\n",
      "Epoch: 20, loss =  nan\n",
      "Epoch: 21, loss =  nan\n",
      "Epoch: 22, loss =  nan\n",
      "Epoch: 23, loss =  nan\n",
      "Epoch: 24, loss =  nan\n",
      "Epoch: 25, loss =  nan\n",
      "Epoch: 26, loss =  nan\n",
      "Epoch: 27, loss =  nan\n",
      "Epoch: 28, loss =  nan\n",
      "Epoch: 29, loss =  nan\n",
      "Epoch: 30, loss =  nan\n",
      "Epoch: 31, loss =  nan\n",
      "Epoch: 32, loss =  nan\n",
      "Epoch: 33, loss =  nan\n",
      "Epoch: 34, loss =  nan\n",
      "Epoch: 35, loss =  nan\n",
      "Epoch: 36, loss =  nan\n",
      "Epoch: 37, loss =  nan\n",
      "Epoch: 38, loss =  nan\n",
      "Epoch: 39, loss =  nan\n",
      "Epoch: 40, loss =  nan\n",
      "Epoch: 41, loss =  nan\n",
      "Epoch: 42, loss =  nan\n",
      "Epoch: 43, loss =  nan\n",
      "Epoch: 44, loss =  nan\n",
      "Epoch: 45, loss =  nan\n",
      "Epoch: 46, loss =  nan\n",
      "Epoch: 47, loss =  nan\n",
      "Epoch: 48, loss =  nan\n",
      "Epoch: 49, loss =  nan\n",
      "Epoch: 50, loss =  nan\n",
      "Epoch: 51, loss =  nan\n",
      "Epoch: 52, loss =  nan\n",
      "Epoch: 53, loss =  nan\n",
      "Epoch: 54, loss =  nan\n",
      "Epoch: 55, loss =  nan\n",
      "Epoch: 56, loss =  nan\n",
      "Epoch: 57, loss =  nan\n",
      "Epoch: 58, loss =  nan\n",
      "Epoch: 59, loss =  nan\n",
      "Epoch: 60, loss =  nan\n",
      "Epoch: 61, loss =  nan\n",
      "Epoch: 62, loss =  nan\n",
      "Epoch: 63, loss =  nan\n",
      "Epoch: 64, loss =  nan\n",
      "Epoch: 65, loss =  nan\n",
      "Epoch: 66, loss =  nan\n",
      "Epoch: 67, loss =  nan\n",
      "Epoch: 68, loss =  nan\n",
      "Epoch: 69, loss =  nan\n",
      "Epoch: 70, loss =  nan\n",
      "Epoch: 71, loss =  nan\n",
      "Epoch: 72, loss =  nan\n",
      "Epoch: 73, loss =  nan\n",
      "Epoch: 74, loss =  nan\n",
      "Epoch: 75, loss =  nan\n",
      "Epoch: 76, loss =  nan\n",
      "Epoch: 77, loss =  nan\n",
      "Epoch: 78, loss =  nan\n",
      "Epoch: 79, loss =  nan\n",
      "Epoch: 80, loss =  nan\n",
      "Epoch: 81, loss =  nan\n",
      "Epoch: 82, loss =  nan\n",
      "Epoch: 83, loss =  nan\n",
      "Epoch: 84, loss =  nan\n",
      "Epoch: 85, loss =  nan\n",
      "Epoch: 86, loss =  nan\n",
      "Epoch: 87, loss =  nan\n",
      "Epoch: 88, loss =  nan\n",
      "Epoch: 89, loss =  nan\n",
      "Epoch: 90, loss =  nan\n",
      "Epoch: 91, loss =  nan\n",
      "Epoch: 92, loss =  nan\n",
      "Epoch: 93, loss =  nan\n",
      "Epoch: 94, loss =  nan\n",
      "Epoch: 95, loss =  nan\n",
      "Epoch: 96, loss =  nan\n",
      "Epoch: 97, loss =  nan\n",
      "Epoch: 98, loss =  nan\n",
      "Epoch: 99, loss =  nan\n",
      "Epoch: 100, loss =  nan\n",
      "Epoch: 101, loss =  nan\n",
      "Epoch: 102, loss =  nan\n",
      "Epoch: 103, loss =  nan\n",
      "Epoch: 104, loss =  nan\n",
      "Epoch: 105, loss =  nan\n",
      "Epoch: 106, loss =  nan\n",
      "Epoch: 107, loss =  nan\n",
      "Epoch: 108, loss =  nan\n",
      "Epoch: 109, loss =  nan\n",
      "Epoch: 110, loss =  nan\n",
      "Epoch: 111, loss =  nan\n",
      "Epoch: 112, loss =  nan\n",
      "Epoch: 113, loss =  nan\n",
      "Epoch: 114, loss =  nan\n",
      "Epoch: 115, loss =  nan\n",
      "Epoch: 116, loss =  nan\n",
      "Epoch: 117, loss =  nan\n",
      "Epoch: 118, loss =  nan\n",
      "Epoch: 119, loss =  nan\n",
      "Epoch: 120, loss =  nan\n",
      "Epoch: 121, loss =  nan\n",
      "Epoch: 122, loss =  nan\n",
      "Epoch: 123, loss =  nan\n",
      "Epoch: 124, loss =  nan\n",
      "Epoch: 125, loss =  nan\n",
      "Epoch: 126, loss =  nan\n",
      "Epoch: 127, loss =  nan\n",
      "Epoch: 128, loss =  nan\n",
      "Epoch: 129, loss =  nan\n",
      "Epoch: 130, loss =  nan\n",
      "Epoch: 131, loss =  nan\n",
      "Epoch: 132, loss =  nan\n",
      "Epoch: 133, loss =  nan\n",
      "Epoch: 134, loss =  nan\n",
      "Epoch: 135, loss =  nan\n",
      "Epoch: 136, loss =  nan\n",
      "Epoch: 137, loss =  nan\n",
      "Epoch: 138, loss =  nan\n",
      "Epoch: 139, loss =  nan\n",
      "Epoch: 140, loss =  nan\n",
      "Epoch: 141, loss =  nan\n",
      "Epoch: 142, loss =  nan\n",
      "Epoch: 143, loss =  nan\n",
      "Epoch: 144, loss =  nan\n",
      "Epoch: 145, loss =  nan\n",
      "Epoch: 146, loss =  nan\n",
      "Epoch: 147, loss =  nan\n",
      "Epoch: 148, loss =  nan\n",
      "Epoch: 149, loss =  nan\n",
      "Epoch: 150, loss =  nan\n",
      "Epoch: 151, loss =  nan\n",
      "Epoch: 152, loss =  nan\n",
      "Epoch: 153, loss =  nan\n",
      "Epoch: 154, loss =  nan\n",
      "Epoch: 155, loss =  nan\n",
      "Epoch: 156, loss =  nan\n",
      "Epoch: 157, loss =  nan\n",
      "Epoch: 158, loss =  nan\n",
      "Epoch: 159, loss =  nan\n",
      "Epoch: 160, loss =  nan\n",
      "Epoch: 161, loss =  nan\n",
      "Epoch: 162, loss =  nan\n",
      "Epoch: 163, loss =  nan\n",
      "Epoch: 164, loss =  nan\n",
      "Epoch: 165, loss =  nan\n",
      "Epoch: 166, loss =  nan\n",
      "Epoch: 167, loss =  nan\n",
      "Epoch: 168, loss =  nan\n",
      "Epoch: 169, loss =  nan\n",
      "Epoch: 170, loss =  nan\n",
      "Epoch: 171, loss =  nan\n",
      "Epoch: 172, loss =  nan\n",
      "Epoch: 173, loss =  nan\n",
      "Epoch: 174, loss =  nan\n",
      "Epoch: 175, loss =  nan\n",
      "Epoch: 176, loss =  nan\n",
      "Epoch: 177, loss =  nan\n",
      "Epoch: 178, loss =  nan\n",
      "Epoch: 179, loss =  nan\n",
      "Epoch: 180, loss =  nan\n",
      "Epoch: 181, loss =  nan\n",
      "Epoch: 182, loss =  nan\n",
      "Epoch: 183, loss =  nan\n",
      "Epoch: 184, loss =  nan\n",
      "Epoch: 185, loss =  nan\n",
      "Epoch: 186, loss =  nan\n",
      "Epoch: 187, loss =  nan\n",
      "Epoch: 188, loss =  nan\n",
      "Epoch: 189, loss =  nan\n",
      "Epoch: 190, loss =  nan\n",
      "Epoch: 191, loss =  nan\n",
      "Epoch: 192, loss =  nan\n",
      "Epoch: 193, loss =  nan\n",
      "Epoch: 194, loss =  nan\n",
      "Epoch: 195, loss =  nan\n",
      "Epoch: 196, loss =  nan\n",
      "Epoch: 197, loss =  nan\n",
      "Epoch: 198, loss =  nan\n",
      "Epoch: 199, loss =  nan\n",
      "Epoch: 200, loss =  nan\n",
      "Epoch: 201, loss =  nan\n",
      "Epoch: 202, loss =  nan\n",
      "Epoch: 203, loss =  nan\n",
      "Epoch: 204, loss =  nan\n",
      "Epoch: 205, loss =  nan\n",
      "Epoch: 206, loss =  nan\n",
      "Epoch: 207, loss =  nan\n",
      "Epoch: 208, loss =  nan\n",
      "Epoch: 209, loss =  nan\n",
      "Epoch: 210, loss =  nan\n",
      "Epoch: 211, loss =  nan\n",
      "Epoch: 212, loss =  nan\n",
      "Epoch: 213, loss =  nan\n",
      "Epoch: 214, loss =  nan\n",
      "Epoch: 215, loss =  nan\n",
      "Epoch: 216, loss =  nan\n",
      "Epoch: 217, loss =  nan\n",
      "Epoch: 218, loss =  nan\n",
      "Epoch: 219, loss =  nan\n",
      "Epoch: 220, loss =  nan\n",
      "Epoch: 221, loss =  nan\n",
      "Epoch: 222, loss =  nan\n",
      "Epoch: 223, loss =  nan\n",
      "Epoch: 224, loss =  nan\n",
      "Epoch: 225, loss =  nan\n",
      "Epoch: 226, loss =  nan\n",
      "Epoch: 227, loss =  nan\n",
      "Epoch: 228, loss =  nan\n",
      "Epoch: 229, loss =  nan\n",
      "Epoch: 230, loss =  nan\n",
      "Epoch: 231, loss =  nan\n",
      "Epoch: 232, loss =  nan\n",
      "Epoch: 233, loss =  nan\n",
      "Epoch: 234, loss =  nan\n",
      "Epoch: 235, loss =  nan\n",
      "Epoch: 236, loss =  nan\n",
      "Epoch: 237, loss =  nan\n",
      "Epoch: 238, loss =  nan\n",
      "Epoch: 239, loss =  nan\n",
      "Epoch: 240, loss =  nan\n",
      "Epoch: 241, loss =  nan\n",
      "Epoch: 242, loss =  nan\n",
      "Epoch: 243, loss =  nan\n",
      "Epoch: 244, loss =  nan\n",
      "Epoch: 245, loss =  nan\n",
      "Epoch: 246, loss =  nan\n",
      "Epoch: 247, loss =  nan\n",
      "Epoch: 248, loss =  nan\n",
      "Epoch: 249, loss =  nan\n",
      "Epoch: 250, loss =  nan\n",
      "Epoch: 251, loss =  nan\n",
      "Epoch: 252, loss =  nan\n",
      "Epoch: 253, loss =  nan\n",
      "Epoch: 254, loss =  nan\n",
      "Epoch: 255, loss =  nan\n",
      "Epoch: 256, loss =  nan\n",
      "Epoch: 257, loss =  nan\n",
      "Epoch: 258, loss =  nan\n",
      "Epoch: 259, loss =  nan\n",
      "Epoch: 260, loss =  nan\n",
      "Epoch: 261, loss =  nan\n",
      "Epoch: 262, loss =  nan\n",
      "Epoch: 263, loss =  nan\n",
      "Epoch: 264, loss =  nan\n",
      "Epoch: 265, loss =  nan\n",
      "Epoch: 266, loss =  nan\n",
      "Epoch: 267, loss =  nan\n",
      "Epoch: 268, loss =  nan\n",
      "Epoch: 269, loss =  nan\n",
      "Epoch: 270, loss =  nan\n",
      "Epoch: 271, loss =  nan\n",
      "Epoch: 272, loss =  nan\n",
      "Epoch: 273, loss =  nan\n",
      "Epoch: 274, loss =  nan\n",
      "Epoch: 275, loss =  nan\n",
      "Epoch: 276, loss =  nan\n",
      "Epoch: 277, loss =  nan\n",
      "Epoch: 278, loss =  nan\n",
      "Epoch: 279, loss =  nan\n",
      "Epoch: 280, loss =  nan\n",
      "Epoch: 281, loss =  nan\n",
      "Epoch: 282, loss =  nan\n",
      "Epoch: 283, loss =  nan\n",
      "Epoch: 284, loss =  nan\n",
      "Epoch: 285, loss =  nan\n",
      "Epoch: 286, loss =  nan\n",
      "Epoch: 287, loss =  nan\n",
      "Epoch: 288, loss =  nan\n",
      "Epoch: 289, loss =  nan\n",
      "Epoch: 290, loss =  nan\n",
      "Epoch: 291, loss =  nan\n",
      "Epoch: 292, loss =  nan\n",
      "Epoch: 293, loss =  nan\n",
      "Epoch: 294, loss =  nan\n",
      "Epoch: 295, loss =  nan\n",
      "Epoch: 296, loss =  nan\n",
      "Epoch: 297, loss =  nan\n",
      "Epoch: 298, loss =  nan\n",
      "Epoch: 299, loss =  nan\n",
      ",Final Loss =  nan\n",
      "  W: [[ 206.45904851]\n",
      " [  47.31800373]\n",
      " [ 644.41411404]\n",
      " [ 485.11520933]\n",
      " [ 232.97888039]\n",
      " [ 191.25480072]\n",
      " [-433.80921886]\n",
      " [ 472.9972972 ]\n",
      " [ 621.81382533]\n",
      " [ 420.28696969]],b:[45340.54524887]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kaush\\AppData\\Local\\Temp\\ipykernel_19568\\1021783568.py:37: RuntimeWarning: divide by zero encountered in log\n",
      "  y_one_loss=(1-y).T.dot(np.log(1-y_pred))\n",
      "C:\\Users\\kaush\\AppData\\Local\\Temp\\ipykernel_19568\\1021783568.py:42: RuntimeWarning: overflow encountered in exp\n",
      "  return 1.0/(1+np.exp(-z))\n",
      "C:\\Users\\kaush\\AppData\\Local\\Temp\\ipykernel_19568\\1021783568.py:36: RuntimeWarning: divide by zero encountered in log\n",
      "  y_zero_loss=y.T.dot(np.log(y_pred))\n",
      "C:\\Users\\kaush\\AppData\\Local\\Temp\\ipykernel_19568\\1021783568.py:39: RuntimeWarning: invalid value encountered in add\n",
      "  return -np.sum(y_zero_loss+ y_one_loss)/len(y)\n"
     ]
    }
   ],
   "source": [
    "alpha=1\n",
    "epochs = 300\n",
    "log_model = LogisticRegression(X,y)\n",
    "log_model.train(alpha,epochs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
